[
  {
    "objectID": "CH3.html",
    "href": "CH3.html",
    "title": "3  环境数据整理与可视化",
    "section": "",
    "text": "3.1 环境数据整理",
    "crumbs": [
      "基础知识",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>环境数据整理与可视化</span>"
    ]
  },
  {
    "objectID": "CH3.html#环境数据整理",
    "href": "CH3.html#环境数据整理",
    "title": "3  环境数据整理与可视化",
    "section": "",
    "text": "3.1.1 tidyr工具包主要函数\ntidyr工具包的主要功能是清洗数据集，确保每一列对应一个变量(特征或属性)，每一行对应一个观测(样例或数据点)，每一个单元格对应一个值或元素，即形成整洁(tidy)的数据表。\n\n3.1.1.1 长宽表转换\npivot_longer()函数的作用在于将分散在不同列的同一个变量的值合并为一列。\n\n例 3.1 如 表 3.1 所示的数据表(随机生成的伪数据)，是最常见的非tidy数据表，因为PM10的浓度数据分散在四个列变量中。为此，需要将四个列变量转换为一列名为Season的字符变量，将所有的数值转换为一列为PM10的数值变量，这样清理以后的数据表才是整洁数据表，如 表 3.2 所示。\n\n\nlibrary(tidyr)\nset.seed(2023)\npm10 = tibble(ID = 1:6,\n              spring = round(rnorm(6, 50, 10) * 10),\n              summer = round(rnorm(6, 25, 5) * 10),\n              autumn = round(rnorm(6, 45, 10) * 10),\n              winter = round(rnorm(6, 75, 20) * 10))\n\n\n\n\n\n表 3.1: 一个宽表示例\n\n\n\n\n\n\nID\nspring\nsummer\nautumn\nwinter\n\n\n\n\n1\n492\n204\n506\n929\n\n\n2\n402\n300\n516\n864\n\n\n3\n312\n230\n390\n668\n\n\n4\n481\n227\n520\n691\n\n\n5\n437\n266\n510\n994\n\n\n6\n609\n229\n495\n799\n\n\n\n\n\n\n\n\n利用pivot_long()函数将宽表转换为tidy格式的长表：\n\npm10_long = pivot_longer(pm10, 2:5,\n                         names_to = \"Season\", \n                         values_to = \"PM10\")\n\n\n\n\n\n表 3.2: 转换后的tidy格式长表\n\n\n\n\n\n\nID\nSeason\nPM10\n\n\n\n\n1\nspring\n492\n\n\n1\nsummer\n204\n\n\n1\nautumn\n506\n\n\n1\nwinter\n929\n\n\n2\nspring\n402\n\n\n2\nsummer\n300\n\n\n2\nautumn\n516\n\n\n2\nwinter\n864\n\n\n3\nspring\n312\n\n\n3\nsummer\n230\n\n\n3\nautumn\n390\n\n\n3\nwinter\n668\n\n\n4\nspring\n481\n\n\n4\nsummer\n227\n\n\n4\nautumn\n520\n\n\n4\nwinter\n691\n\n\n5\nspring\n437\n\n\n5\nsummer\n266\n\n\n5\nautumn\n510\n\n\n5\nwinter\n994\n\n\n6\nspring\n609\n\n\n6\nsummer\n229\n\n\n6\nautumn\n495\n\n\n6\nwinter\n799\n\n\n\n\n\n\n\n\npivot_wider()函数与pivot_longer()函数功能相反，是将长数据表转换为宽数据表。 以下代码将pm10_long转换为宽表pm10_wide，结果与 表 3.1 完全一致：\n\npm10_wide = pivot_wider(pm10_long,\n                        id_cols = ID,\n                        names_from = \"Season\", \n                        values_from = \"PM10\") \n\n\n\n3.1.1.2 列值合并与分割\n对于 表 3.3 (a) 所示的数据表中century和year两列，可以通过unite()函数将其合并为一列，合并后的结果如 表 3.3 (b) 所示。\n\ntb = tibble(country  = LETTERS[1:4],\n            century = rep(c(\"19\", \"20\"), 2),\n            year = rep(c(\"99\", \"00\"), 2))\ntb1 = unite(tb, \"year\", 2:3, sep = \"\")\n\n\n\n\n表 3.3: 列值合并操作\n\n\n\n\n\n\n\n(a) 合并前数据表\n\n\n\n\n\ncountry\ncentury\nyear\n\n\n\n\nA\n19\n99\n\n\nB\n20\n00\n\n\nC\n19\n99\n\n\nD\n20\n00\n\n\n\n\n\n\n\n\n\n\n\n(b) 合并后数据表\n\n\n\n\n\ncountry\nyear\n\n\n\n\nA\n1999\n\n\nB\n2000\n\n\nC\n1999\n\n\nD\n2000\n\n\n\n\n\n\n\n\n\n\n\nseparate_wider_*()函数(*为dilim、position或regex)能够将一列中的字符分割为多列。如 表 3.4 (a) 所示的数据表中，如果需要将x列的数据从下划线处分割：\n\ntb = tibble(id = 1:4, x = c(\"o3_1h\", \"o3_8h\", \"co_1h\", \"co_8h\"))\n# 以下三个函数操作结果相同\ntb1 = tb %&gt;% separate_wider_delim(x, delim = \"_\", names = c(\"pollutant\", \"hour\"))\n# tb1 = tb %&gt;% separate_wider_position(x, c(\"pollutant\" = 2, 1, \"hour\" = 2))\n# tb1 = tb %&gt;% separate_wider_regex(x, c(\"pollutant\" = \".*\", \"_\", \"hour\" = \".*\"))\n\n分割后的数据表见 表 3.4 (b) 。\n\n\n\n表 3.4: 列值分割操作\n\n\n\n\n\n\n\n(a) 分割前数据表\n\n\n\n\n\nid\nx\n\n\n\n\n1\no3_1h\n\n\n2\no3_8h\n\n\n3\nco_1h\n\n\n4\nco_8h\n\n\n\n\n\n\n\n\n\n\n\n(b) 分割后数据表\n\n\n\n\n\nid\npollutant\nhour\n\n\n\n\n1\no3\n1h\n\n\n2\no3\n8h\n\n\n3\nco\n1h\n\n\n4\nco\n8h\n\n\n\n\n\n\n\n\n\n\n\n如果将x变量中的“co_8h”改为“co_24h”，separate_wider_position()函数就会报错，因为该值的宽度与前面三个值的宽度不同。separate_wider_regex()可以应对更复杂的字符串分割问题，但需要用户掌握正则表达式的语法。学习正则表达式可参阅菜鸟教程网站正则表达式教程。\n当数据集中的列值需要分割为多行而不是多列时，可以利用函数separate_longer_dilim()和separate_longer_position()。如 表 3.5 所示的两个数据表，变量x的值需要分割为多行。\n\ntb1 = tibble(id = 1:4, x = c(\"x\", \"x y\", \"x y z\", NA))\ntb2 = tibble(id = 1:4, x = c(\"ab\", \"def\", \"bf\",\"\"))\n\n\n\n\n表 3.5: 列值需要分割为多列的数据表\n\n\n\n\n\n\n\n(a) 包含空格\n\n\n\n\n\nid\nx\n\n\n\n\n1\nx\n\n\n2\nx y\n\n\n3\nx y z\n\n\n4\nNA\n\n\n\n\n\n\n\n\n\n\n\n(b) 不含空格\n\n\n\n\n\nid\nx\n\n\n\n\n1\nab\n\n\n2\ndef\n\n\n3\nbf\n\n\n4\n\n\n\n\n\n\n\n\n\n\n\n\n分别利用separate_longer_delim()和separate_longer_position()对 表 3.5 中的两个表进行分割，结果如 表 3.6 所示。\n\n\n\n表 3.6: 列值分割为多列后的数据表\n\n\n\n\n\n\n\n(a) 包含空格\n\n\n\n\n\nid\nx\n\n\n\n\n1\nx\n\n\n2\nx\n\n\n2\ny\n\n\n3\nx\n\n\n3\ny\n\n\n3\nz\n\n\n4\nNA\n\n\n\n\n\n\n\n\n\n\n\n(b) 不含空格\n\n\n\n\n\nid\nx\n\n\n\n\n1\na\n\n\n1\nb\n\n\n2\nd\n\n\n2\ne\n\n\n2\nf\n\n\n3\nb\n\n\n3\nf\n\n\n4\nNA\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.1.1.3 其他操作\nexpand()函数可以根据数据表提供的变量生成所有可能的组合。如表 表 3.7 (a)所示的数据表，并没有显示所有因子与水平的组合。利用expand()进行处理后，结果见 表 3.7 (b)所示。\n\n\n\n表 3.7: expand()函数的操作\n\n\n\n\n\n\n\n(a) 操作前的数据表\n\n\n\n\n\ntrt\nlvl\n\n\n\n\nA\n1\n\n\nA\n2\n\n\nB\n1\n\n\nC\n2\n\n\n\n\n\n\n\n\n\n\n\n(b) 操作后的数据表\n\n\n\n\n\ntrt\nlvl\n\n\n\n\nA\n1\n\n\nA\n2\n\n\nB\n1\n\n\nB\n2\n\n\nC\n1\n\n\nC\n2\n\n\n\n\n\n\n\n\n\n\n\n\ntb\n\n# A tibble: 4 × 2\n  trt     lvl\n  &lt;chr&gt; &lt;dbl&gt;\n1 A         1\n2 A         2\n3 B         1\n4 C         2\n\n\ncomplete()函数将隐式的缺失值显式化。如 表 3.8 (a) 所示的数据表，存在隐式的缺失值。通过complete()可以将其显式地呈现出来，结果如 表 3.8 (b) 所示。\n\n\n\n表 3.8: complete()函数的操作\n\n\n\n\n\n\n\n(a) 操作前的数据表\n\n\n\n\n\ngrp\nsam_id\nsam_name\nconc\n\n\n\n\n1\n1\nA\n5\n\n\n2\n2\nA\nNA\n\n\n1\n2\nB\n8\n\n\n2\n3\nB\n10\n\n\n\n\n\n\n\n\n\n\n\n(b) 操作后的数据表\n\n\n\n\n\ngrp\nsam_id\nsam_name\nconc\n\n\n\n\n1\n1\nA\n5\n\n\n1\n2\nB\n8\n\n\n1\n3\nNA\nNA\n\n\n2\n1\nNA\nNA\n\n\n2\n2\nA\nNA\n\n\n2\n3\nB\n10\n\n\n\n\n\n\n\n\n\n\n\ndrop_na()函数提供了最简单的处理缺失值的方式：删除包含缺失值的行。而fill()函数则提供了利用前值和/或后值填补缺失值的方法。以下代码分别利用drop_na()和fill()删除和填补airquality数据集中的NA值：\n\n# 删除NA值前\nairquality[, 1:2] %&gt;% \n  summary()\n\n     Ozone           Solar.R     \n Min.   :  1.00   Min.   :  7.0  \n 1st Qu.: 18.00   1st Qu.:115.8  \n Median : 31.50   Median :205.0  \n Mean   : 42.13   Mean   :185.9  \n 3rd Qu.: 63.25   3rd Qu.:258.8  \n Max.   :168.00   Max.   :334.0  \n NA's   :37       NA's   :7      \n\n# 删除NA值\nairquality[, 1:2] %&gt;% \n  drop_na(Ozone, Solar.R) %&gt;% \n  summary()\n\n     Ozone          Solar.R     \n Min.   :  1.0   Min.   :  7.0  \n 1st Qu.: 18.0   1st Qu.:113.5  \n Median : 31.0   Median :207.0  \n Mean   : 42.1   Mean   :184.8  \n 3rd Qu.: 62.0   3rd Qu.:255.5  \n Max.   :168.0   Max.   :334.0  \n\n# 填补NA值\nairquality[, 1:2] %&gt;% \n  fill(Ozone, Solar.R, .direction = \"up\") %&gt;% \n  summary()\n\n     Ozone          Solar.R     \n Min.   :  1.0   Min.   :  7.0  \n 1st Qu.: 20.0   1st Qu.:118.0  \n Median : 31.0   Median :213.0  \n Mean   : 46.8   Mean   :188.1  \n 3rd Qu.: 71.0   3rd Qu.:259.0  \n Max.   :168.0   Max.   :334.0  \n\n\nR用NA表示缺失值，但其他数据处理软件可能采用不同的缺失值符号，replace_na()函数提供了修改缺失值符号的功能。下面的代码将airquality数据集中Ozone的缺失值修改为-99999。在修改缺失值符号时需要注意变量的数据类型。\n\nairquality %&gt;% \n  replace_na(list(Ozone = -99999)) %&gt;% \n  summary()\n\n     Ozone           Solar.R           Wind             Temp      \n Min.   :-99999   Min.   :  7.0   Min.   : 1.700   Min.   :56.00  \n 1st Qu.:     4   1st Qu.:115.8   1st Qu.: 7.400   1st Qu.:72.00  \n Median :    21   Median :205.0   Median : 9.700   Median :79.00  \n Mean   :-24151   Mean   :185.9   Mean   : 9.958   Mean   :77.88  \n 3rd Qu.:    46   3rd Qu.:258.8   3rd Qu.:11.500   3rd Qu.:85.00  \n Max.   :   168   Max.   :334.0   Max.   :20.700   Max.   :97.00  \n                  NA's   :7                                       \n     Month            Day      \n Min.   :5.000   Min.   : 1.0  \n 1st Qu.:6.000   1st Qu.: 8.0  \n Median :7.000   Median :16.0  \n Mean   :6.993   Mean   :15.8  \n 3rd Qu.:8.000   3rd Qu.:23.0  \n Max.   :9.000   Max.   :31.0  \n                               \n\n\n\n\n\n3.1.2 dplyr工具包主要函数\ndplyr为数据框类型数据的整理提供了统一且高效的操作函数，提供了包括行列选择、行列合并、排序、列操作(创建、修改和删除)、分组汇总统计等。\n\n3.1.2.1 选择列\nselect()函数可以根据位置、列名和条件选择等多种方式从数据中选择列。下面的代码演示select()函数对airquality数据集进行列选择：\n\nlibrary(dplyr)\n\n# 按列位置选择列\nairquality %&gt;% \n  dplyr::select(1:3) %&gt;%   \n  head()\n\n  Ozone Solar.R Wind\n1    41     190  7.4\n2    36     118  8.0\n3    12     149 12.6\n4    18     313 11.5\n5    NA      NA 14.3\n6    28      NA 14.9\n\n# 按列位置选择列\nairquality %&gt;% \n   dplyr::select(1, 3:5) %&gt;%  \n  head()\n\n  Ozone Wind Temp Month\n1    41  7.4   67     5\n2    36  8.0   72     5\n3    12 12.6   74     5\n4    18 11.5   62     5\n5    NA 14.3   56     5\n6    28 14.9   66     5\n\n# 按列位置列表选择列\nairquality %&gt;% \n   dplyr::select(c(1, 3, 5)) %&gt;%   \n  head()\n\n  Ozone Wind Month\n1    41  7.4     5\n2    36  8.0     5\n3    12 12.6     5\n4    18 11.5     5\n5    NA 14.3     5\n6    28 14.9     5\n\n# 按列名范围选择列\nairquality %&gt;% \n   dplyr::select(Month, Day) %&gt;%  \n  head(5)\n\n  Month Day\n1     5   1\n2     5   2\n3     5   3\n4     5   4\n5     5   5\n\n# 按列名范围选择列\nairquality %&gt;% \n   dplyr::select(Temp:Day) %&gt;%   \n  head()\n\n  Temp Month Day\n1   67     5   1\n2   72     5   2\n3   74     5   3\n4   62     5   4\n5   56     5   5\n6   66     5   6\n\n# 按列名列表范围选择列\nairquality %&gt;% \n   dplyr::select(c(Ozone, Temp, Month)) %&gt;%   \n  head()\n\n  Ozone Temp Month\n1    41   67     5\n2    36   72     5\n3    12   74     5\n4    18   62     5\n5    NA   56     5\n6    28   66     5\n\n# 按列名包含字符选择列\nairquality %&gt;% \n   dplyr::select(contains(\"o\")) %&gt;%   \n  head()\n\n  Ozone Solar.R Month\n1    41     190     5\n2    36     118     5\n3    12     149     5\n4    18     313     5\n5    NA      NA     5\n6    28      NA     5\n\n\ntidyselect包提供以下功能函数，可以提高列选择操作的效率：\n\ncontains()、starts_with()、ends_with()函数: 通过指定列名包含字符、开始字符和结束字符的条件来选择列；\nmatchs()函数: 通过匹配指定正则表达式的条件来选择列；\nnum_range()函数: 通过指定范围来选择列，用于以字符前缀+数字序号的列名(如x1、x2、x3等)；\nall_of()和any_of()函数: 从指定的列名列表中选择列，前者会进行列名匹配检查(如果数据集中没有列表中的列名，会报错)，而后者不进行列名匹配检查；\neverything()函数: 指定所有的列；\nlast_col()函数: 指定最后一列；\nwhere()函数: 根据函数返回的逻辑值来选择列。\n\n\n\n3.1.2.2 行选择\nslice()和filter()是两个最常用的行选择函数。下面的代码演示这两个函数对airquality数据集进行行选择：\n\nairquality %&gt;% \n  slice(1:5)  # 按行位置选择行\n\n  Ozone Solar.R Wind Temp Month Day\n1    41     190  7.4   67     5   1\n2    36     118  8.0   72     5   2\n3    12     149 12.6   74     5   3\n4    18     313 11.5   62     5   4\n5    NA      NA 14.3   56     5   5\n\nairquality %&gt;% \n  slice(1, 3, 5, 10)  # 按行位置选择行\n\n  Ozone Solar.R Wind Temp Month Day\n1    41     190  7.4   67     5   1\n2    12     149 12.6   74     5   3\n3    NA      NA 14.3   56     5   5\n4    NA     194  8.6   69     5  10\n\nairquality %&gt;% \n  dplyr::filter(Month == 7, Day %in% c(1:5))  # 按指定条件选择行\n\n  Ozone Solar.R Wind Temp Month Day\n1   135     269  4.1   84     7   1\n2    49     248  9.2   85     7   2\n3    32     236  9.2   81     7   3\n4    NA     101 10.9   84     7   4\n5    64     175  4.6   83     7   5\n\nairquality %&gt;% \n  dplyr::filter(Month &gt; 7, Day &lt;= 3)  # 按指定条件选择行\n\n  Ozone Solar.R Wind Temp Month Day\n1    39      83  6.9   81     8   1\n2     9      24 13.8   81     8   2\n3    16      77  7.4   82     8   3\n4    96     167  6.9   91     9   1\n5    78     197  5.1   92     9   2\n6    73     183  2.8   93     9   3\n\n\nfilter()函数中的条件表达式可充分运用关系运算符、逻辑运算符以及类似between()、is.na()、near()这样的条件判断函数等。\nslice_head()和slice_tail()函数分别用于选择指定头部和尾部的行数，类似于Base-R中的head()和tail()两个函数，slice_min()和slice_max()函数分别选择指定列的最小值和最大值所在的行(通过参数n可指定根据最小值和最大值依次排序的行数)，slice_sample()函数用于随机抽取指定行数的行。\n\n\n3.1.2.3 列操作\nmutate()函数可创建新的列、修改现有的列(列名与已有列名相同)和删除指定的列(设置列值为NULL)。下面的代码演示mutate()对airqulity数据集进行各种列的操作：\n\nairquality %&gt;% \n  # 选择前20行\n  slice(1:10) %&gt;% \n  # 根据Temp(华氏温度)计算摄氏温度并创建Temp_c\n  mutate(Temp_C = round((Temp - 32)/1.8, 2)) %&gt;% \n  # 计算Ozone与Temp的比值并创建新列Ratio\n  mutate(Ratio = round(Ozone/Temp_C, 2)) %&gt;% \n  # 创建新列ID\n  mutate(ID = 1:nrow(.)) %&gt;% \n  # 将ID和Ratio列设置为第一和第二列\n  relocate(ID, Ratio, everything()) %&gt;% \n  # 删除Temp列\n  mutate(Temp = NULL) %&gt;% \n  # 修改现有列：将Month和Day列有序因子化\n  mutate(Month = ordered(Month),\n         Day = ordered(Day))\n\n   ID Ratio Ozone Solar.R Wind Month Day Temp_C\n1   1  2.11    41     190  7.4     5   1  19.44\n2   2  1.62    36     118  8.0     5   2  22.22\n3   3  0.51    12     149 12.6     5   3  23.33\n4   4  1.08    18     313 11.5     5   4  16.67\n5   5    NA    NA      NA 14.3     5   5  13.33\n6   6  1.48    28      NA 14.9     5   6  18.89\n7   7  1.25    23     299  8.6     5   7  18.33\n8   8  1.27    19      99 13.8     5   8  15.00\n9   9  0.50     8      19 20.1     5   9  16.11\n10 10    NA    NA     194  8.6     5  10  20.56\n\n\n实际上，以上多个mutate()的操作可以合并为一个mutate()函数的操作，但是要注意操作的顺序，避免冲突，例如创建Temp_C列必须在创建Ratio列之前，删除Temp列必须在创建Temp_C列之后：\n\nairquality %&gt;% \n  slice(1:10) %&gt;% \n  mutate(Temp_C = round((Temp - 32)/1.8, 2),\n         Ratio = round(Ozone/Temp_C, 2),\n         ID = 1:nrow(.),\n         Temp = NULL,\n         Month = ordered(Month),\n         Day = ordered(Day)) %&gt;% \n  relocate(ID, Ratio, everything())\n\n   ID Ratio Ozone Solar.R Wind Month Day Temp_C\n1   1  2.11    41     190  7.4     5   1  19.44\n2   2  1.62    36     118  8.0     5   2  22.22\n3   3  0.51    12     149 12.6     5   3  23.33\n4   4  1.08    18     313 11.5     5   4  16.67\n5   5    NA    NA      NA 14.3     5   5  13.33\n6   6  1.48    28      NA 14.9     5   6  18.89\n7   7  1.25    23     299  8.6     5   7  18.33\n8   8  1.27    19      99 13.8     5   8  15.00\n9   9  0.50     8      19 20.1     5   9  16.11\n10 10    NA    NA     194  8.6     5  10  20.56\n\n\n\n\n3.1.2.4 数据集拼接\ninner_join()、left_join()、right_join()和full_join()等函数可以将两个数据按特定方式进行合并。下面的代码演示这四个函数对两个数据集的拼接操作：\n\nset.seed(2023)\nx = tibble(\n  ID = 1:5,\n  x1 = round(rnorm(5) * 10, 2),\n  x2 = x1^2 - 1,\n  trt = sample(letters[1:10], 5, replace = T)\n)\n\nset.seed(2023)\ny = tibble(\n  ID = 1:8,\n  y1 = round(rnorm(8, 5, 1), 2),\n  trt = sample(letters[1:10], 8, replace = T)\n)\n\n# 内拼接\ninner_join(x, y, by = \"trt\")\n\n# A tibble: 3 × 6\n   ID.x    x1    x2 trt    ID.y    y1\n  &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt;\n1     4 -1.86  2.46 e         5  4.37\n2     5 -6.33 39.1  h         1  4.92\n3     5 -6.33 39.1  h         6  6.09\n\n# 左拼接\nleft_join(x, y, by = \"trt\")\n\n# A tibble: 6 × 6\n   ID.x     x1      x2 trt    ID.y    y1\n  &lt;int&gt;  &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt;\n1     1  -0.84  -0.294 a        NA NA   \n2     2  -9.83  95.6   a        NA NA   \n3     3 -18.8  351.    a        NA NA   \n4     4  -1.86   2.46  e         5  4.37\n5     5  -6.33  39.1   h         1  4.92\n6     5  -6.33  39.1   h         6  6.09\n\n# 右拼接\nright_join(x, y, by = \"ID\")\n\n# A tibble: 8 × 6\n     ID     x1      x2 trt.x    y1 trt.y\n  &lt;int&gt;  &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt;\n1     1  -0.84  -0.294 a      4.92 h    \n2     2  -9.83  95.6   a      4.02 b    \n3     3 -18.8  351.    a      3.12 c    \n4     4  -1.86   2.46  e      4.81 d    \n5     5  -6.33  39.1   h      4.37 e    \n6     6  NA     NA     &lt;NA&gt;   6.09 h    \n7     7  NA     NA     &lt;NA&gt;   4.09 i    \n8     8  NA     NA     &lt;NA&gt;   6    g    \n\n# 全拼接\nfull_join(x, y)\n\nJoining with `by = join_by(ID, trt)`\n\n\n# A tibble: 13 × 5\n      ID     x1      x2 trt      y1\n   &lt;int&gt;  &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;\n 1     1  -0.84  -0.294 a     NA   \n 2     2  -9.83  95.6   a     NA   \n 3     3 -18.8  351.    a     NA   \n 4     4  -1.86   2.46  e     NA   \n 5     5  -6.33  39.1   h     NA   \n 6     1  NA     NA     h      4.92\n 7     2  NA     NA     b      4.02\n 8     3  NA     NA     c      3.12\n 9     4  NA     NA     d      4.81\n10     5  NA     NA     e      4.37\n11     6  NA     NA     h      6.09\n12     7  NA     NA     i      4.09\n13     8  NA     NA     g      6   \n\n\nsemi_join()和anti_join()函数根据与第二个数据集匹配的列值来选择第一个数据集中的行，可以视为一种行选择操作。例如以下的操作：\n\nsemi_join(x, y, by = \"trt\")  # 选择x中与y中trt列匹配的行\n\n# A tibble: 2 × 4\n     ID    x1    x2 trt  \n  &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;\n1     4 -1.86  2.46 e    \n2     5 -6.33 39.1  h    \n\nanti_join(x, y, by = \"trt\")  # 选择x中与y中trt列不匹配的行\n\n# A tibble: 3 × 4\n     ID     x1      x2 trt  \n  &lt;int&gt;  &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;\n1     1  -0.84  -0.294 a    \n2     2  -9.83  95.6   a    \n3     3 -18.8  351.    a    \n\n\nbind_cols()函数对多个等函数的数据集进行列合并，bind_rows()函数对多个数据集进行行合并。以下代码演示了这两个函数的操作：\n\ndf1 = tibble(x = 1:3, y = letters[2:4])\ndf2 = tibble(x = 1:3, z = 6:8)\n\nbind_cols(df1, df2)\n\nNew names:\n• `x` -&gt; `x...1`\n• `x` -&gt; `x...3`\n\n\n# A tibble: 3 × 4\n  x...1 y     x...3     z\n  &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt;\n1     1 b         1     6\n2     2 c         2     7\n3     3 d         3     8\n\nbind_rows(df1, df2)\n\n# A tibble: 6 × 3\n      x y         z\n  &lt;int&gt; &lt;chr&gt; &lt;int&gt;\n1     1 b        NA\n2     2 c        NA\n3     3 d        NA\n4     1 &lt;NA&gt;      6\n5     2 &lt;NA&gt;      7\n6     3 &lt;NA&gt;      8\n\n\n\n\n3.1.2.5 分组汇总统计\nsummarise()函数与group_by()协作可实现对一个或多个列变量执行分组汇总统计。以下代码根据Month变量分组，分别统计Ozone、Solar.R、Temp、Wind变量的月均值：\n\nairquality %&gt;% \n  group_by(Month) %&gt;% \n  summarise(ozone_mean = mean(Ozone, na.rm = T),\n            Solar.R_mean = mean(Solar.R, na.rm = T),\n            Temp_mean = mean(Temp, na.rm = T),\n            Wind_mean = mean(Wind, na.rm = T))\n\n# A tibble: 5 × 5\n  Month ozone_mean Solar.R_mean Temp_mean Wind_mean\n  &lt;int&gt;      &lt;dbl&gt;        &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1     5       23.6         181.      65.5     11.6 \n2     6       29.4         190.      79.1     10.3 \n3     7       59.1         216.      83.9      8.94\n4     8       60.0         172.      84.0      8.79\n5     9       31.4         167.      76.9     10.2 \n\n\n如果采用across()函数，则更简洁：\n\nairquality %&gt;% \n  group_by(Month) %&gt;% \n  summarise(\n    across(.cols = 1:4,   # 用位置或列名选择列\n           .fns = mean, na.rm = T,  # 函数或函数列表，可以加上函数的参数设置\n           .names = \"{.col}_mean\"))  # 设置列名，{.col}表示引用原列名\n\n# A tibble: 5 × 5\n  Month Ozone_mean Solar.R_mean Wind_mean Temp_mean\n  &lt;int&gt;      &lt;dbl&gt;        &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1     5       23.6         181.     11.6       65.5\n2     6       29.4         190.     10.3       79.1\n3     7       59.1         216.      8.94      83.9\n4     8       60.0         172.      8.79      84.0\n5     9       31.4         167.     10.2       76.9\n\n\nacross()函数还可以对选定的列应用多个函数：\n\nfn = list(\n  mean = ~mean(.x, na.rm = T),\n  sd = ~sd(.x, na.rm = T))\nairquality %&gt;% \n  group_by(Month) %&gt;% \n  summarise(\nacross(1:2, fn))\n\n# A tibble: 5 × 5\n  Month Ozone_mean Ozone_sd Solar.R_mean Solar.R_sd\n  &lt;int&gt;      &lt;dbl&gt;    &lt;dbl&gt;        &lt;dbl&gt;      &lt;dbl&gt;\n1     5       23.6     22.2         181.      115. \n2     6       29.4     18.2         190.       92.9\n3     7       59.1     31.6         216.       80.6\n4     8       60.0     39.7         172.       76.8\n5     9       31.4     24.1         167.       79.1\n\n\n当应用多个函数时，结果列的名称默认为{.col}_{.fn}。函数列表中函数为匿名函数时采用purrr包中匿名函数的形式，即用~替代function(.x)，如果直接引用函数，则无需加~，直接引用函数名(不加后面的括号，如mean)：\n\nfn = list(\n  mean = mean,  # 不能是mean =  mean()\n  sd = sd)\nairquality %&gt;% \n  group_by(Month) %&gt;% \n  summarise(\n    across(.cols = 1:2, \n           .fns = fn, na.rm = T))\n\n# A tibble: 5 × 5\n  Month Ozone_mean Ozone_sd Solar.R_mean Solar.R_sd\n  &lt;int&gt;      &lt;dbl&gt;    &lt;dbl&gt;        &lt;dbl&gt;      &lt;dbl&gt;\n1     5       23.6     22.2         181.      115. \n2     6       29.4     18.2         190.       92.9\n3     7       59.1     31.6         216.       80.6\n4     8       60.0     39.7         172.       76.8\n5     9       31.4     24.1         167.       79.1\n\n\nsummarise()、mutate()与rowwise()协作可实现对行观测的分组汇总统计，前者只返回汇总统计结果，而后者默认返回原始数据与行观测统计结果合并的列(可以通过该函数中参数.keep来改变)。以下代码创建了一个5行×4列的数值变量，然后分别用summarise()和mutate()按行计算均值、标准差与加和：\n\nmatrix(1:20, nrow = 5) %&gt;% \n  as_tibble() %&gt;% \n  rowwise() %&gt;% \n  summarise(mean = mean(c_across(1:4)),\n            sd = sd(c_across(1:4)),\n            sum = sum(c_across(1:4)))\n\n# A tibble: 5 × 3\n   mean    sd   sum\n  &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;\n1   8.5  6.45    34\n2   9.5  6.45    38\n3  10.5  6.45    42\n4  11.5  6.45    46\n5  12.5  6.45    50\n\n\n\nmatrix(1:20, nrow = 5) %&gt;% \n  as_tibble() %&gt;% \n  rowwise() %&gt;% \n  mutate(mean = mean(c_across(1:4)),\n         sd = sd(c_across(1:4)),\n         sum = sum(c_across(1:4)))\n\n# A tibble: 5 × 7\n# Rowwise: \n     V1    V2    V3    V4  mean    sd   sum\n  &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;\n1     1     6    11    16   8.5  6.45    34\n2     2     7    12    17   9.5  6.45    38\n3     3     8    13    18  10.5  6.45    42\n4     4     9    14    19  11.5  6.45    46\n5     5    10    15    20  12.5  6.45    50\n\n\n上面代码中的c_across()函数用于配合rowwise()函数来选择列变量，是结合函数c()的扩展，遵循tidyr包的语法。\n\n\n3.1.2.6 其他操作\narrange()函数可根据指定的一个或多个列对行进行升序排列，降序则需要使用desc()函数：\n\nairquality %&gt;% \n  arrange(Ozone, Month) %&gt;%   # 按Ozone和Month变量升序重新排序\n  head()\n\n  Ozone Solar.R Wind Temp Month Day\n1     1       8  9.7   59     5  21\n2     4      25  9.7   61     5  23\n3     6      78 18.4   57     5  18\n4     7      NA  6.9   74     5  11\n5     7      48 14.3   80     7  15\n6     7      49 10.3   69     9  24\n\nairquality %&gt;% \n  arrange(desc(Ozone), Month) %&gt;%   # 按Ozone变量降序和Month变量升序重新排序\n  head()\n\n  Ozone Solar.R Wind Temp Month Day\n1   168     238  3.4   81     8  25\n2   135     269  4.1   84     7   1\n3   122     255  4.0   89     8   7\n4   118     225  2.3   94     8  29\n5   115     223  5.7   79     5  30\n6   110     207  8.0   90     8   9\n\n\nrelocate()函数可调整列的位置：\n\nairquality %&gt;% \n  relocate(Month, Day, everything()) %&gt;%   # 将Month和Day列调整为第一列和第二列\n  head()\n\n  Month Day Ozone Solar.R Wind Temp\n1     5   1    41     190  7.4   67\n2     5   2    36     118  8.0   72\n3     5   3    12     149 12.6   74\n4     5   4    18     313 11.5   62\n5     5   5    NA      NA 14.3   56\n6     5   6    28      NA 14.9   66\n\n\n\n\n\n3.1.3 错误和重复数据的处理\n数据出现错误往往是在原始数据记录环节和计算机输入环节因人为原因导致，也有可能是测量阶段因方法错误、仪器故障等原因导致。对原始数据采用逻辑检查和计算检查，往往可以发现错误数据，如污染物浓度出现负数，用水总量低于新鲜用水量等，合计数据小于其分项数据，汇总的数据单位不一致等。发现错误数据后，应先溯源，看能否找到正确值，如不能找到正确值，应先以缺失值(NA)替代。要注意，一些分析仪器的检测结果可能因低于方法检测限(Method Detect Limit，MDL)而出现ND(Not Detect)数据，这不属于缺失值，也不是错误值，一般可以用0替代来进行处理。\n以下代码演示了用0替代ND值、用NA替代负值的操作，处理前后的结果分别见表3.15和表3.16。\n\nlibrary(tidyverse)\n\ndf = tibble(id = c(1:8),\n            conc = c(2.3, 4.5, 1.4, 4.0, \"ND\", -2.3, \"ND\", 6.3)) \n            # 有ND字符，强制为字符型数据\n\ndf1 = df %&gt;% \n  mutate(across(2, ~replace(.x, .x == \"ND\", \"0\"))) %&gt;% \n  mutate(across(2, as.numeric)) %&gt;% \n  mutate(across(2, ~replace(.x, .x &lt; 0, NA)))\n\n\n\n\n表 3.9: 异常值处理\n\n\n\n\n\n\n\n(a) 处理前的数据表\n\n\n\n\n\nid\nconc\n\n\n\n\n1\n2.3\n\n\n2\n4.5\n\n\n3\n1.4\n\n\n4\n4\n\n\n5\nND\n\n\n6\n-2.3\n\n\n7\nND\n\n\n8\n6.3\n\n\n\n\n\n\n\n\n\n\n\n(b) 处理后的数据表\n\n\n\n\n\nid\nconc\n\n\n\n\n1\n2.3\n\n\n2\n4.5\n\n\n3\n1.4\n\n\n4\n4.0\n\n\n5\n0.0\n\n\n6\nNA\n\n\n7\n0.0\n\n\n8\n6.3\n\n\n\n\n\n\n\n\n\n\n\ndistinct()函数用于删除数据框中的重复。行对于重复数据，需要严格确认方可删除，以免造成失误。\n\ndf = tibble(id = c(1:5,1:2),\n            conc = c(2.3,3.2,1.4,4.6,6.8,2.3,4.5))\ndf1 = distinct(df)\n\n\n\n\n表 3.10: 重复值处理操作\n\n\n\n\n\n\n\n(a) 处理前的数据表\n\n\n\n\n\nid\nconc\n\n\n\n\n1\n2.3\n\n\n2\n3.2\n\n\n3\n1.4\n\n\n4\n4.6\n\n\n5\n6.8\n\n\n1\n2.3\n\n\n2\n4.5\n\n\n\n\n\n\n\n\n\n\n\n(b) 处理后的数据表\n\n\n\n\n\nid\nconc\n\n\n\n\n1\n2.3\n\n\n2\n3.2\n\n\n3\n1.4\n\n\n4\n4.6\n\n\n5\n6.8\n\n\n2\n4.5\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.1.4 缺失值的识别与处理\n对于大数据而言，数据缺失是比较常见的现象，数据缺失可能出现在采集、记录、输入等环节，包括人为错误、机器故障、无法获取或代价太大而放弃获取等。有些缺失数据可通过再次调查和实验来重新获取，但多数情况下无法重新获取准确的数据，尤其是具有时间属性的数据。\n数据缺失一般可分为三种模式：第一种是完全随机缺失(MCAR)，即数据缺失是完全随机的，不影响样本的无偏性，数据丢失的概率与不完全变量(存在缺失值的变量)本身和其他完全变量(没有缺失值的变量)都无关。这种情况下，缺失数据可以通过一些方法基于已知变量来估计。该类型缺失值对统计分析的影响可忽略。第二种是随机缺失(MAR)，即数据缺失不是完全随机的，缺失数据的概率与不完全变量本身无关，而与其他完全变量有关。例如只对重点污染源收集环保投资数据，则非重点污染源的环保投资数据出现缺失。该模式的缺失值对统计分析的影响也可忽略。第三种是非随机缺失(MNAR)，即数据缺失不是随机的，缺失数据的概率只与不完全变量本身有关，而与其他完全变量无关。例如某几个企业生产的某类产品因污染排放不达标被勒令停止生产，因此，这几个企业的该类产品污染数据缺失。该类型缺失值对统计分析的影响不可忽略。\n\n3.1.4.1 缺失值的识别\nR提供了is.na()函数来识别向量、矩阵、数据框中是否存在NA值，并在相应位置返回逻辑值TRUE或FLASE。也可用complete.cases()函数检查数据框中行数据是否完整(有缺失则不完整，返回FALSE)。对于数据框，可以采用VIM包对缺失值进行可视化和填补。\n\nlibrary(VIM)\nlibrary(magrittr)\nairquality %&gt;% aggr(numbers = TRUE)\n\n\n\n\n\n\n\n图 3.1: 用VIM包对缺失数据进行可视化\n\n\n\n\n\n显然，aggr()函数通过 图 3.1 左图指出缺失数据集中在Ozone和Solar.R两个变量中，并通过右图给出了具体的缺失比例。VIM还提供了barMiss()、scattMiss()、histMiss()、matrixplot()、marginplot()、marginmatirx()等多个可视化函数。\nnaniar包是一个更专业的处理缺失值的工具包，功能更全面，包括丰富的可视化与填补功能，其可视化基于ggplot2，具有与tidyverse一致的tidy风格。naniar包中的vis_miss()(调用visdat包中的同名函数)和gg_miss_*()系列函数提供了丰富的缺失值可视化功能。图 3.2 为naniar包对airquality数据集中缺失值的可视化探索。\n\nlibrary(naniar)\n\nvis_miss(airquality)\ngg_miss_upset(airquality)\n\n\n\n\n\n\n\n\n\n\n\n(a) vis_miss()可视化图形\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) gg_miss_upset()可视化图形\n\n\n\n\n\n\n\n图 3.2: 用naniar对缺失值进行可视化\n\n\n\n\n\n\n3.1.4.2 缺失值的处理\n缺失数据的处理方法包括不处理、删除和填补三种方法。有些分析方法和模型(如基于决策树的模型)对数据缺失不敏感，所以无需删除。当数据量很大时，或缺失比例很小时，或缺失数据在不重要的变量中，可以删除缺失值或不重要的变量；当数据量很少时，或缺失比例很大时，或所属变量为重要变量，删除缺失数据会减少信息，从而影响分析和建模。在必要且符合条件的情况下，可以对缺失数据采取合理的方法进行填补，包括单一填补和多重填补。对于进行缺失值填补后的数据，尽量避免采用距离度量相关的分析方法和模型(如支持向量机SVM、K最近邻模型KNN等)。对于MNAR模式的缺失数据进行填补要特别谨慎，可考虑采用模型预测法填补。填补缺失值之前，需要先明确缺失模式；采用填补方法之前，需要先明确填补方法的使用前提。\n(1)删除缺失值\nR中na.omit()函数可以用于删除数据集中的缺失数据(对于矩阵和数据框，则删除包含缺失值的行)。tidyr中的drop_na()函数用于删除数据框中包含缺失值的行。对于airquality数据集，原本有153行，执行na.omit(airquality)或drop_na(airquality)后，将删除含有缺失值的42行数据，剩下111行完整数据。drop_na()可以指定包含缺失值的列来选择性删除。\n(2)填补缺失值 对于MCAR和MAR模式的缺失值，可以用单一填补法或多重填补法进行填补。 常见的单一填补法包括均值(平均数、中位数、众数)填补、热卡(Hot-Deck)填补(在数据集中找到一个与缺失值最相似的对象并以此来填补，但相似标准很难界定)、K-Means聚类填充(用聚类后的同类均值填补)、KNN填充(用距离最近的若干个同类投票或取均值来填充)等。 多重填补法通常是利用数据集中其他变量来建立缺失变量的预测模型，如多元回归模型、极大似然估计、随机森林模型等。缺失值预测模型可以给出缺失值的单个预测填补值和多重预测填补值，后者为每一个缺失值给出多个填补值，符合缺失值的不确定性，然后对多个填补值根据评分函数来选择，以获得最佳的填补值。\nVIM包提供了多种填补方法：基于迭代鲁棒模型填充irmi()、热卡填充hotdeck()、K-最近邻填充kNN()、基于分类变量的快速匹配填充matchImpute()、随机森林填充rangerImpute()和回归模型填充regressionImp()。naniar包提供了impute_below()、impute_mean()、impute_median()等系列函数，来填补缺失值。simputation包整合了较为丰富的缺失值填补方法，语法简单且统一，易于使用。\nmice是一个对缺失值进行诊断和多重填补的专业工具包，提供了20多种填补方法，所有的方法都可以通过mice()来调用，而且支持自定义填补方法。该函数默认为缺失数据生成5套填补值，但有些方法如“mean”生成的5套填补值都是相同的。mice中的with()和pool()函数可以对所有填补集进行分析(如建模) 并合并各个填补集的分析结果；complete()函数用于选择其中一套填补方案并生成完整数据集。\n下面演示mice()中的随机森林填补模型对airquality数据中的缺失值进行填补：\n\nlibrary(mice)\n\nimp = mice(airquality, method = \"rf\", ntree = 50, \n           printFlag = FALSE, seed = 2024)\nmod = with(imp, lm(Ozone ~ Solar.R + Wind + Temp))\npooled = pool(mod)\nsummary(pooled)\n\n         term     estimate   std.error statistic       df      p.value\n1 (Intercept) -55.15782155 23.08180196 -2.389667 51.79981 2.053847e-02\n2     Solar.R   0.06502871  0.02663235  2.441719 23.65223 2.248481e-02\n3        Wind  -3.12973182  0.64841387 -4.826750 64.24621 8.925236e-06\n4        Temp   1.48934951  0.24510299  6.076423 82.45038 3.639331e-08\n\ndf = complete(imp, action = 1)  # 用第一套填补值生成完整数据集\n\n如果要观察比较填补前后的变化，可以用mice包中的densityplot()函数绘制分布密度函数曲线，结果如 图 3.3 所示，显然，不同填补结果的分布密度差异很大。\n\nlattice::densityplot(imp)\n\n\n\n\n\n\n\n图 3.3: airquality数据集缺失变量填补前后密度图\n\n\n\n\n\n处理缺失值的R包还有Amelia、mi、pan等，missForest是一个采用随机森林对各类型变量缺失值进行估计的包，Hmisc包也提供了缺失值填补的函数。在CRAN网站的任务视图MissingData中可以探索各种与缺失值有关的R包。\n(3)增加虚拟变量\n有时，缺失数据不能删除，也没有合适的方法进行填补，或填补后可能导致分析与建模出现较大偏差。此时，可以根据缺失变量创建相应的虚拟变量(也称哑变量，dummy variable)，即对一个存在缺失值的变量，衍生出一个包含0(对应缺失值)和1(对应非缺失值)的二进制虚拟变量，从而尽可能为一些不支持缺失值的分析方法和模型提供更多的信息。虚拟变量也可以用于处理多水平的因子型变量，以生成与因子水平数相同的新列，每列代表该因子型变量的一个水平。\n\nlibrary(tidyverse)\n\ndf = airquality %&gt;% \n  mutate(across(Solar.R, ~if_else(is.na(.x), 0, 1)))\n\nrecipes包中的step_dummy()函数和caret包中的dummyVars()函数都能够将因子变量或字符变量转换为虚拟变量。fastDummies包提供了专门用于将字符型和因子型变量转换为虚拟变量的函数，包括dummy_cols()和dummy_rows()两个函数，前者用于将字符型和因子型列变量转换为二进制虚拟变量，后者则根据字符型、因子型和日期型列变量的所有组合填充缺失的行，这对于创建平衡的面板数据非常方便。\n\nlibrary(fastDummies)\n\nx = data.frame(\n  comps = LETTERS[1:5],\n  toxi = c(\"剧毒\", \"低毒\", \"高毒\", \"中毒\", \"低毒\"),\n  logP = c(-1.1, 3.2, -0.5, 2.3, 4.1)\n)\nx\n\n  comps toxi logP\n1     A 剧毒 -1.1\n2     B 低毒  3.2\n3     C 高毒 -0.5\n4     D 中毒  2.3\n5     E 低毒  4.1\n\nx_dummy = dummy_cols(x, select_columns = \"toxi\")\nx_dummy\n\n  comps toxi logP toxi_中毒 toxi_低毒 toxi_剧毒 toxi_高毒\n1     A 剧毒 -1.1         0         0         1         0\n2     B 低毒  3.2         0         1         0         0\n3     C 高毒 -0.5         0         0         0         1\n4     D 中毒  2.3         1         0         0         0\n5     E 低毒  4.1         0         1         0         0\n\n\n补全隐性缺失观测:\n\nx = data.frame(\n  city = c(\"HF\", \"AQ\", \"HF\"),\n  year = c(2020, 2021, 2021),\n  volume = c(1000, 670, 1200))\nx\n\n  city year volume\n1   HF 2020   1000\n2   AQ 2021    670\n3   HF 2021   1200\n\nx_dummy = dummy_rows(x, select_columns = c(\"city\", \"year\"))\nx_dummy\n\n  city year volume\n1   HF 2020   1000\n2   AQ 2021    670\n3   HF 2021   1200\n4   AQ 2020     NA\n\n\n增加列变量以确定观测是否为缺失观测:\n\ndummy_rows(x, select_columns = c(\"city\", \"year\"),\n           dummy_indicator = TRUE)\n\n  city year volume dummy_indicator\n1   HF 2020   1000               0\n2   AQ 2021    670               0\n3   HF 2021   1200               0\n4   AQ 2020     NA               1\n\n\n\n\n\n3.1.5 异常值的识别与处理\n异常值也称离群值(outlier)，指与同组其他观测值相距较远的观测值，即与同组其他数据点有显著差异的数据点。异常值往往可以分为两类：极端值(自然存在)和错误值(测量错误、输入错误、抽样错误等)。突发的环境污染事件可能导致极端的情况出现，因此对自然存在的异常值需要关注。同时，要避免因各种错误导致的异常值。\n异常值的存在会显著地影响数据分析结果和建模，因此需要在数据分析之前对异常值进行识别和处理。在大多数情况下，剔除异常值后建立的模型更加稳健。\n\n3.1.5.1 异常值的识别\n简单识别异常值的方法是通过箱线图、散点图、直方图等。箱线图(如 图 3.4 所示)默认将超出1.5倍IQR(四分位间距)的上下极端值视为异常值。散点图和直方图等可直观地从图形上发现异常的数据点。\n\n\n\n\n\n\n\n\n图 3.4: 箱线图及其对异常值的识别\n\n\n\n\n\n对于正态分布的资料，有2种常用的判断异常值的方法：\n\n3σ法：计算数据集的均值μ和标准差σ，将μ±3σ区间之外的数据视为异常值。\nIQR法：将所有数据从小到大排列，25%、50%和75%处的数值称为下四分位数(Q1)、中位数(Median)和上四分位数(Q3)，它们将数据集等分为4部分，Q3-Q1即为四分位间距(IQR)。IQR法将[Q1-1.5IQR, Q3+1.5IQR]区间之外的数据视为异常值，这个方法就是箱线图所采用的识别异常值的方法。\n\nIQR法可通过调用boxplot.stats()函数来实现。如检测airquality数据集中Ozone的异常值：\n\nboxplot.stats(airquality$Ozone)$out\n\n[1] 135 168\n\n\n上边的方法将最大的两个数值识别为异常值。\n如果对Ozone数据进行对数转换，再用IQR法进行异常值检测：\n\nexp(boxplot.stats(log(airquality$Ozone))$out)\n\n[1] 1\n\n\n显然，对数转换之后将最小值识别为异常值，说明异常值随分布变化而变化。实际上，Ozone呈右偏态分布，存在较大的数值一般是正常的。因此，对于偏态分布的数据不能直接用以上方法检查异常值，特别是很多环境数据属于右偏态分布。有些偏态资料可以通过数据变换而近似正态分布，在转换后可尝试用上述方法检测异常值。如果偏态资料给出了明确的分布类型，则可依据其概率分布来检测异常值。很多领域的数据可能存在特定的区间或检测限，根据这些逻辑或规则也可以来检测异常值。\nDMwR包提供的局部异常值因子法(Local Outlier Factor，LOF)可用于非正态资料的异常值识别，该方法是基于密度的经典算法。也可应用K-means方法聚类，然后计算每个数据点到类中心的距离，通过距离排序判断异常值。\nmvoutlier包提供了基于多种原理但主要基于稳健马氏距离(Robust Mahalanobis Distance)的识别异常值的方法。稳健马氏距离算法优于欧氏距离算法，特别是在检测多元数据异常值时，前者会考虑变量间的相关性，且不受量纲的影响。下面演示该包中uni.plot()和aq.plot()函数的应用，两者都基于稳健马氏距离检测多变量数据的异常值，但前者为每个变量绘制一维散点图并标记异常值，而后者则绘制4个图形，包括原始数据散点图、有序马氏距离平方的累积概率分布图(包括卡方累积概率分布曲线和两条垂直的分位数线)和分别基于指定卡方分布分位数和调整的卡方分布分位数的两个异常值散点图。\n首先生成50×2列的数据框a，数据服从正态分布(均值为0，标准差为1)，接着生成5×2的数据框b，数据服从正态分布(均值为3，标准差为1)，然后按行合并到df。最后来观察boxplot.stats()、uni.plot()和aq.plot()能否将b从df中作为异常值识别出来。\n\nset.seed(2023)\na = data.frame(x = rnorm(50), y = rnorm(50)) \nb = data.frame(x = rnorm(5, 3, 1), y = rnorm(5, 3, 1))\ndf = rbind(a, b)\nrm(a, b)  # 删除内存中的a和b\n\n先采用boxplot.stats()函数识别异常值：\n\n# 输出df第一列中异常值的行号\nwhich(df$x %in% boxplot.stats(df$x)$out)\n\n[1] 53 54 55\n\n# 输出df第二列中异常值行号\nwhich(df$y %in% boxplot.stats(df$y)$out)\n\n[1] 55\n\n\n结果第一列只检测到b中第一列的3个数据点，第二列只检测到b中第二列的1个数据点，同时检测为异常值的仅为第55行，未能将第51~55行全部检测出来。\n接下来利用uni.plot()函数来检测异常值：\n\nlibrary(mvoutlier)\n\np1 = uni.plot(x = df, symb = F, quan = 0.5, alpha = 0.05)\n\nwhich(p1$outliers == T)\n\n[1] 30 51 52 53 54 55\n\n\n\n\n\n\n\n\n图 3.5: uni.plot()绘制的异常值散点图\n\n\n\n\n\n结果表明，uni.plot()将5行异常值全部识别出来，但也将原数据中的第30个数据点识别为异常值。异常值散点图如 图 3.5 所示，其中红色数据点为识别出来的异常值。\n接下来利用aq.plot()函数识别异常值：\n\np2 = aq.plot(x = df, quan = 0.5, alpha = 0.05)\n\nwhich(p2$outliers == T)\n\n[1] 30 51 52 53 54 55\n\n\n\n\n\n\n\n\n图 3.6: aq.plot()绘制的异常值诊断图\n\n\n\n\n\naq.plot()同样将5行异常值全部识别出来(也包括原数据中的第30个数据点)，并给出如 图 3.6 所示的四个异常值诊断图。在 图 3.6 中最重要的图是右上图，数据点所在的位置决定于其稳健马氏距离平方值和对应的累积概率，其中洋红色曲线为拟合的理论卡方累积概率曲线，青色垂线为指定的卡方分位数(默认为0.975)，位于该线右边的数据判定为异常数据(对应为左下图，红色编号为异常值)；蓝色垂线为调整的卡方分位数(采用自适应方法从尾部搜索获得)，该线右边数据判定为异常数据(对应为右下图，红色编号为异常值)；左上图为原始数据散点图。当数据点包含2个以上变量(特征)时，则将数据点投影到前2个稳健的主成份上。\n对于稀疏的高维数据，例如生命科学研究中的基因数据，往往有成千上万个变量，建议采用mvoutlier包中的pcout()函数，通过融合主成分降维的思路来检测异常值。此外，该包中还包含多个其他的检测异常值的函数，可阅读帮助信息以进一步学习和掌握。\n异常值的检测除了基于距离、概率密度的方法外，还有基于决策树、角度的多种方法，以及针对时空数据异常值的专用检测方法，如DBScan聚类法、孤立森林法(Isolation Forest)、随机砍伐森林法(Random Cut Forest)等。实现这些方法的函数可以在有关的R包中都可以找到，具体参见Anomaly Detection with R。\n\n\n3.1.5.2 异常值的处理\n异常值的处理一般包括删除法、替代法、转换法等，亦可视为缺失值。删除法简单易行，替代法操作也较为简单，如将超出90%分位数的数据用90%分位数替代，将低于10%分位数的数据用10%分位数替代等；转换法采用数学运算，缩小数据之间的差异，例如平方根法、对数法、去中心缩放法(标准化处理)、归一化缩放法等。也可将异常值转换为缺失值，然后利用缺失值填补方法进行填补。此外，还可以采用分组的方法，将连续型变量转换为离散型变量，转换为等级数据，这也是一种较好的降低异常值对分析结果和建模产生不利影响的方法。\n总之，异常值的检查要根据数据分布选择科学的方法进行。删除异常值要特别谨慎，要分析异常值产生的原因，判断真伪，避免删除真实有效的数据。",
    "crumbs": [
      "基础知识",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>环境数据整理与可视化</span>"
    ]
  },
  {
    "objectID": "CH3.html#制作数据表格",
    "href": "CH3.html#制作数据表格",
    "title": "3  环境数据整理与可视化",
    "section": "3.2 制作数据表格",
    "text": "3.2 制作数据表格\n表格是展示数据特别是数据汇总结果的重要形式，在R的R Markdown、Notebook和Quarto文档中，knitr、DT、kableExtra、htmlTable、table1、formattable、flextable、reactable包都可以帮助用户生成美观的HTML格式的表格，有些包还可以生成pdf格式和图片格式的表格。sjPlot包能够将统计分析的结果呈现为整洁美观的表格。下面介绍R中三种制作三线表的方法。\n\n3.2.1 htmlTable包制作三线表\nhtmlTable包生成的三线表为HTML格式，可以嵌入支持HTML格式的文档中，如果不支持HTML格式，则可以通过截图方式嵌入。以下代码将iris数据前8行数据整理成三线表：\n\nlibrary(htmlTable)\n\nhtmlTable(iris[1:8,])\n\n\n\n\n\n\nSepal.Length\nSepal.Width\nPetal.Length\nPetal.Width\nSpecies\n\n\n\n\n1\n5.1\n3.5\n1.4\n0.2\nsetosa\n\n\n2\n4.9\n3\n1.4\n0.2\nsetosa\n\n\n3\n4.7\n3.2\n1.3\n0.2\nsetosa\n\n\n4\n4.6\n3.1\n1.5\n0.2\nsetosa\n\n\n5\n5\n3.6\n1.4\n0.2\nsetosa\n\n\n6\n5.4\n3.9\n1.7\n0.4\nsetosa\n\n\n7\n4.6\n3.4\n1.4\n0.3\nsetosa\n\n\n8\n5\n3.4\n1.5\n0.2\nsetosa\n\n\n\n\n\n\n显然，只需要先将数据表的行名、列名以及数据整理好，htmlTable包就可以快速生成三线表，以及利用更多的分类变量进行分组，实现更复杂的分组表格，并添加标题和脚注等，具体可参考How-to use htmlTable。\n\n\n3.2.2 table1包制作分组统计三线表\ntable1不仅能绘制三线表，而且能分组计算一些描述性的统计量，如总量、算术平均数、中位数、分位数、标准差、缺失值等，制作出所谓的“表1”，这在医学相关研究论文中几乎是一个固定格式。table1采用了公式化的方法来指定行变量和列变量，易于组织，非常简单。下面是用该包对airquality数据集制作“表1”。\n\nlibrary(table1)\n\ntable1(~ Ozone + Solar.R + Wind + Temp | Month, data = airquality)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n5\n(N=31)\n6\n(N=30)\n7\n(N=31)\n8\n(N=31)\n9\n(N=30)\nOverall\n(N=153)\n\n\n\n\nOzone\n\n\n\n\n\n\n\n\nMean (SD)\n23.6 (22.2)\n29.4 (18.2)\n59.1 (31.6)\n60.0 (39.7)\n31.4 (24.1)\n42.1 (33.0)\n\n\nMedian [Min, Max]\n18.0 [1.00, 115]\n23.0 [12.0, 71.0]\n60.0 [7.00, 135]\n52.0 [9.00, 168]\n23.0 [7.00, 96.0]\n31.5 [1.00, 168]\n\n\nMissing\n5 (16.1%)\n21 (70.0%)\n5 (16.1%)\n5 (16.1%)\n1 (3.3%)\n37 (24.2%)\n\n\nSolar.R\n\n\n\n\n\n\n\n\nMean (SD)\n181 (115)\n190 (92.9)\n216 (80.6)\n172 (76.8)\n167 (79.1)\n186 (90.1)\n\n\nMedian [Min, Max]\n194 [8.00, 334]\n189 [31.0, 332]\n253 [7.00, 314]\n198 [24.0, 273]\n192 [14.0, 259]\n205 [7.00, 334]\n\n\nMissing\n4 (12.9%)\n0 (0%)\n0 (0%)\n3 (9.7%)\n0 (0%)\n7 (4.6%)\n\n\nWind\n\n\n\n\n\n\n\n\nMean (SD)\n11.6 (3.53)\n10.3 (3.77)\n8.94 (3.04)\n8.79 (3.23)\n10.2 (3.46)\n9.96 (3.52)\n\n\nMedian [Min, Max]\n11.5 [5.70, 20.1]\n9.70 [1.70, 20.7]\n8.60 [4.10, 14.9]\n8.60 [2.30, 15.5]\n10.3 [2.80, 16.6]\n9.70 [1.70, 20.7]\n\n\nTemp\n\n\n\n\n\n\n\n\nMean (SD)\n65.5 (6.85)\n79.1 (6.60)\n83.9 (4.32)\n84.0 (6.59)\n76.9 (8.36)\n77.9 (9.47)\n\n\nMedian [Min, Max]\n66.0 [56.0, 81.0]\n78.0 [65.0, 93.0]\n84.0 [73.0, 92.0]\n82.0 [72.0, 97.0]\n76.0 [63.0, 93.0]\n79.0 [56.0, 97.0]\n\n\n\n\n\n\n\n\n从该表可见，它将前4列变量按第5列变量Month做了分组统计，给出均值及标准差、中位数及最大最小值，还统计了缺失值的个数及比例，从而给出数据的全貌特征。table1包可通过其他函数对表格进一步调整和美化，例如给行组变量和列组变量添加标签、指定要计算的统计量等。更详细地使用方法可阅读Using the table1 Package to Create HTML Tables of Descriptive Statistics。\n\n\n3.2.3 ggpubr包制作三线表\nggpurr是基于ggplot2用于快速制作出版图形的R包，除了极为方便地绘制满足各类学术期刊要求的专业图形以外，它也能通过ggtexttable()函数来制作表格，不仅能绘制三线表，还能实现多种样式，特别是ggpubr能将绘制的表格方便地整合到指定图形中，从而呈现更全面的信息。要注意，ggtexttable()函数不具备分组统计功能，但提供了丰富的表格样式以及针对单元格的调整功能。\n\nlibrary(ggpubr)\n\nggtexttable(iris[1:8,], theme = ttheme(\"blank\")) %&gt;%\n  tab_add_hline(at.row = c(1, 2), row.side = \"top\", linewidth = 2) %&gt;%\n  tab_add_hline(at.row = c(9), row.side = \"bottom\", linewidth = 2)\n\n\n\n\n\n\n\n\n添加标题和脚注：\n\nmain.title &lt;- \"鸢尾花数据\"\nsubtitle &lt;- \"三种类型，共150个观测数据\"\ntab &lt;- ggtexttable(head(iris), theme = ttheme(\"blank\"))\ntab %&gt;%\n   tab_add_title(text = subtitle, face = \"plain\", size = 10) %&gt;%\n   tab_add_title(text = main.title, face = \"bold\", \npadding = unit(0.2, \"line\")) %&gt;%\n   tab_add_hline(at.row = c(3, 4), row.side = \"top\", linewidth = 1) %&gt;% \n   tab_add_hline(at.row = c(9), row.side = \"bottom\", linewidth = 1) %&gt;% \n   tab_add_footnote(text = \"*表格使用ggpubr绘制\", size = 8, face = \"italic\")\n\n\n\n\n\n\n\n\nggpubr制作的表格是图片格式，可以通过R的图形输出功能，输出为pdf格式。\n还有很多R包致力于通过表格更好地展示数据，用户可以利用这些R包来制作各种形式的数据表，包括动态交互式表格，以及具有搜索、排序、筛选等功能的数据表。",
    "crumbs": [
      "基础知识",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>环境数据整理与可视化</span>"
    ]
  },
  {
    "objectID": "CH3.html#r绘图基础",
    "href": "CH3.html#r绘图基础",
    "title": "3  环境数据整理与可视化",
    "section": "3.3 R绘图基础",
    "text": "3.3 R绘图基础\n图形是最直观、最生动地呈现数据的方式。R在绘制图形方面具有极大的优势，在很多的专业出版物中都可以看到用R绘制的精美图形，已经成为数据可视化的重要工具。 R的绘图系统较多，包括基础绘图系统、lattice绘图系统和ggplot2绘图系统。其中ggplot2已经通过各种扩展包http://exts.ggplot2.tidyverse.org/形成了比较完善的生态系统，但ggplot2本身不能绘制动态交互式图形和三维图形，需要借助扩展包。目前，已经有很多第三方R包实现了非常好的交互式绘图体验，包括plotly、echarts4r、recharts、highcharter、leaflet等。rgl包能够绘制非常逼真的三维图形，plot3D也是一个不错的绘制三维图形的工具包。\n\n3.3.1 R的图形设备\nR语言中绘图设备包括屏幕窗口显示设备和文件打印设备两种。grDevices包中包含了操作管理图形设备的函数。\n函数dev.list()列出所有图形设备的类型和编号；函数dev.cur()显示当前图形设备类型和编号；函数dev.off()关闭指定图形设备，如不指定编号，则关闭当前图形设备，同时返回可用的当前图形设备；函数graphics.off()关闭所有图形设备；函数dev.set()设置下一个窗口作为当前图形设备，并返回设备名称和编号。\n\n3.3.1.1 窗口显示设备\n窗口显示设备即通过在屏幕上创建一个窗口来显示图形。创建窗口设备的函数有X11()、x11()、windows()和win.graph()。前2个可用于Windows系统和Linux系统，后2个仅用于Windows系统。创建的窗口设备会有设备编号以及是否为激活状态(inactive或Active)。\ndev.new()默认创建新的RStudio图形设备(即RStudioGD)以及伴随的png文件设备，如果设置参数noRStudioGD = TRUE则创建的是窗口设备。\n\n\n3.3.1.2 文件打印设备\n文件打印设备即将图形输出到指定格式的图形文件中。R支持将图形输出到多种格式，svg()、postscript()、pdf()、png()、jpeg()、bmp()、tiff()函数分别创建svg(矢量图形)、ps(矢量图形)、pdf、png、jpg、bmp、tiff格式的图形文件设备，pictex()、xifg()、win.metafile()能创建Latex、xfig以及emf格式的图形文件。cairo_ps()和cairo_pdf()也能创建ps和pdf格式，但与postscript()和pdf()不同，一是前二者可以输出为位图格式，二是前二者可以使用更广泛的UTF-8符号，嵌入更多所使用的字体。\n要注意，在使用这些文件打印设备时，最后需要使用dev.off()来关闭该设备，以恢复屏幕作为默认的图形输出设备。如以下代码将绘制的图形输出到一个指定位置的pdf格式文档中：\n\npdf(\"d:/test.pdf\")\nplot(iris)\ndev.off()\n\n\n\n\n3.3.2 R基础绘图系统\nR的基础绘图系统基于graphics包。基础绘图系统给用户提供一张白纸，让用户自由地设计图形中的每一个元素。用R的基础绘图系统绘制图形一般包括两个步骤： (1)用高级绘图函数创建图形的主体，比如散点图； (2)然后在主体图形的基础上进行修饰和添加，比如绘制散点图的回归线，调整坐标轴、文字等。\n\n3.3.2.1 高级绘图函数\n所谓高级绘图函数，是在初次调用该绘图函数时，就会打开一个图形设备，并在该设备上绘制图形。R基础绘图系统的高级绘图函数有：\n\nplot()：通用绘图函数，根据输入的数据结构不同绘制不同的图形。\npairs()：绘制双变量散点图或指定可用图形。\nbarplot()：绘制水平或垂直条形图。\nmatplot()：绘制向量或矩阵列的点线图。\ncoplot()：绘制给定条件(因子变量)下两个变量之间依赖关系的散点图或指定可用图形。\nhist()：绘制直方图。\nboxplot()：绘制箱线图。\ndotchart()：绘制数据点图(克利夫兰图)。\nimage()：绘制三个变量的像素图(x与y为坐标，z值用等面积色块表征)。\ncontour()：绘制三个变量的等值线图(x与y为坐标，z值用等值线表征)。\npersp()：绘制三个变量的曲面图(x与y为坐标，z值用3D曲面表征)。\npie()：绘制饼图。\n\n这些高级绘图函数的主要参数有：\n\nadd=TRUE：强制高级绘图函数作为低级绘图函数，将图形叠加到当前图形设备，仅部分函数支持。\naxes=FALSE：禁止生成轴。当需要用axis函数自定义轴时使用。\nlog=“x”、log=“y”、log=“xy”：生成对数轴，大部分图形可用，但不是全部。\ntype：该参数控制生成图形的类型，type=“p”绘制散点图(默认)，type=“l”绘制线图，type=“b”绘制线条连接点的图形，type=“o”绘制线条覆盖点的图形，type=“h”绘制点到零轴的垂线图(高密度)，type=“s”和type=“S”绘制阶跃图形，type=“n”不绘制图形，用于创建自定义图形。该参数主要用于plot()函数。\nxlab、ylab：生成x和y的轴标签。\nmain、sub：生成图形标题和副标题。\nlty、lwd：设置线型和线宽。线型可指定为数字0-7，分别对应”blank”、“solid”、“dashed”、“dotted”、“dotdash”、“longdash”和”twodash”，其中”blank”线条不可见。\ncol、cex：前者设置颜色，后者设置数字、符号、文本缩放系数。可以用colors()函数查询R中的颜色名称，也可用demo(\"colors\")查看各种色盘。\npch：设置点的形状(见 图 3.7 )，其中21-25为可上色的点形状，col为边框设置颜色，bg为内部填充设置颜色。除了0-25以外，pch还支持其他的取值，包括多种字符，可参考帮助信息。\n\n\n\n\n\n\n\n\n\n图 3.7: R基础绘图系统重点的形状(对应pch参数值0-25)\n\n\n\n\n\n不同的绘图函数可能有不同的参数，具体参考有关帮助信息。 以下代码演示了hist()绘制airquality数据集中Ozone变量的直方图(自动分组后统计频数，在绘图)，并对xlab、main和col参数值进行了设置，结果如 图 3.8 所示：\n\nhist(airquality$Ozone,\n     xlab = \"Ozone Concentration\",\n     main = \"Ozone Histogram\",\n     col = \"orange\")\n\n\n\n\n\n\n\n图 3.8: 用hist()绘制直方图\n\n\n\n\n\n\n\n3.3.2.2 低级绘图函数\n低级绘图函数用于在当前图形上创建额外的元素，如点、线、文字等。主要的常用低级绘图函数如下：\n\npoints(x, y)、lines(x, y)：在当前图形上添加点、线。\ntext(x, y, labels, …)：在当前图形上指定的位置添加文本内容。\nabline(a, b)、abline(h=y)、abline(v=x)、abline(lm.obj)：在当前图形上添加直线。第一个用于添加斜率为b、截距为a的直线，第二个添加水平直线，第三个添加垂直直线，第四个添加一个由直线回归模型确定的拟合直线。\npolygon(x, y, …)：在当前图形上根据有序顶点绘制多边形。\nlegend(x, y, legend, …)：在指定位置添加图例，包括字符、线型、颜色等。\ntitle(main, sub)：在当前图形上添加图形标题和副标题。\naxis(side, …)：在指定的边(参数side，1-4分别代表下、左、上、右)上添加坐标轴。其他参数控制坐标轴、轴刻度线以轴标签的位置。用于自定义轴坐标。\n\n低级绘图函数一般都需要指定新添加元素的位置信息(x、y坐标)。坐标是根据高级绘图函数产生的用户坐标系来确定的。\n下面的代码自定义了一个根据最小至和最大值来进行数据归一化的函数，通过箱线图比较了数据归一化前后的分布差异。数据归一化消除了不同变量的量纲影响，从而可以更好地比较分布差异。先用boxplot()函数绘制箱线图，再用title()函数定义图形标题和轴标题，前者作为高级绘图函数会创建新的图形，而后者作为低级绘图函数会在当前图形上添加图形标题和轴标题。\ndf = airquality[-c(5, 6)]\n\n# 自定义函数，根据最小最大值缩放数据\nmmscale = function(x){\n   mi = min(x, na.rm = TRUE)\n   mx = max(x, na.rm = TRUE)\n   sc = (x - mi) / (mx - mi)\n   return(sc)\n}\n\ndfmm = apply(df, 2, FUN = mmscale)\n\nboxplot(df, col = rainbow(4)) \ntitle(main = \"归一化处理前的箱线图\", \n       xlab = \"变量\", ylab = \"变量值\")\n\nboxplot(dfmm, col = rainbow(4))\ntitle(main = \"归一化处理后的箱线图\", \n       xlab = \"变量\", ylab = \"归一化后的变量值\" )\n\n\n\n\n\n\n\n\n\n\n\n(a) 变量值归一化前\n\n\n\n\n\n\n\n\n\n\n\n(b) 变量值归一化后\n\n\n\n\n\n\n\n图 3.9: 用boxplot()绘制箱线图\n\n\n\n函数legend()用于为图形添加自定义的图例，下面的代码演示该函数的使用，结果如 图 3.10 所示：\n\ndf = data.frame(\n  PM2.5 = c(62, 51, 25, 87, 54, 35, 28, 77, 50, 43, 20, 59), \n  year = rep(2018:2020, each = 4),\n  season = rep(c(\"Spring\", \"Summer\", \"Autuam\", \"Winter\"), times = 3)\n  )\n# 绘制折线图\nplot(df$PM2.5[df$year == 2018], type = \"b\", col = \"red\", lty = 1, lwd = 2,\n       ylim = c(20, 90), xlab = \"Season\", ylab = \"Concentration\",\n       mgp = c(1.5, 0.5, 0))\nlines(df$PM2.5[df$year == 2019], type = \"b\", col = \"darkgreen\", lty = 2, lwd = 2)\nlines(df$PM2.5[df$year == 2020], type = \"b\", col = \"blue\", lty = 3, lwd = 2)\nlegend(\"top\", legend = c(\"2018\",\"2019\",\"2020\"),\n       col = c(\"red\", \"darkgreen\", \"blue\"), \n       lty=c(1, 2, 3), lwd = 2, cex = 1)\n\n\n\n\n\n\n\n图 3.10: 用legend()为图形添加图例\n\n\n\n\n\n下面的代码可以将6种可见线型绘制出来，结果如 图 3.11 所示：\n\nx1 = rep(0,6); y1 = c(1:6)\nx2 = rep(3,6); y2 = c(1:6)\n\nlinetype = c(\"solid\", \"dashed\", \"dotted\", \"dotdash\", \"longdash\", \"twodash\")\n\nplot(x1, y1, type = \"n\", xlim = c(0, 4), ylim = c(0, 7),\n     xlab = \"\", ylab = \"\", axes = FALSE)\n\nfor(i in c(1:6)){\n   x=c(x1[i], x2[i])\n   y=c(y1[i], y2[i])\n   lines(x, y, lty = i, lwd = 2)\n}\n\ntext(x = x2 + 0.5, y = y2, labels = linetype)\n\n\n\n\n\n\n\n图 3.11: 打印R基础u绘图系统的六种可见线型\n\n\n\n\n\n\n\n3.3.2.3 绘图全局参数配置\nR提供了par()函数来设置和查询绘图的各种参数。这些参数分为三类：第一类是只读参数(cin、cra、csi、cxy、din、page)，第二类是只能通过par()函数来设置的参数(ask、fig、fin、lheight、mai、mar、mex、mfcol、mfrow、mfg、new、oma、omd、omi、pin、plt、ps、pty、usr、xlog、ylog、ylbias)，剩下的参数为第三类，既可以通过par()函数来设置，也可以通过各种高级绘图函数来设置。通过op = par()返回所有参数，如果设置参数no.readonly = TRUE，则返回所有可以修改的参数。主要的可被修改的参数如下：\n\n符号和线条相关参数：如pch、lty、lwd、cex分别设置点型、线型、线宽和符号大小。在前面的高级绘图函数中也可以设置这些参数。\n\n颜色相关参数：col、col.axis、col.lab、col.main、col.sub、fg、bg分别设置图形(点、线、文本等)、轴刻度、轴标签、标题、副标题、前景和背景颜色。\n\n文本属性相关参数：cex.axis、cex.lab、cex.main、cex.sub、font分别设置轴刻度、轴标签、标题、副标题文字的缩放系数以及指定字体样式，font值为1~4，分别表示常规、粗体、斜体和粗斜体。同col和cex一样，font也可以加上.*(*为axis、lab、main、sub)形成4个参数，以对轴刻度、轴标签、标题和副标题文字指定字体样式。ps设置字体磅值(最终等于ps × cex)，family设置文本字体族，标准取值为”serif”(衬线)、“sans”(无衬线)、“mono”(等宽)，也可以设置为系统其他字体。\n\n图形尺寸及边界：pin以英寸设置图形宽度和高度；mai和mar分别是以英寸和行数为单位来设置图形内部(包括轴标题和图形标题)“下左上右”四个边距。omi、oma分别以英寸和行为单位设置图形外部边距。mar、oma都以mex作为距离单位，默认值为1，为0.2英寸。mgp指定轴标题、轴标签和轴线的边距，默认是c(3,1,0)，mgp[1]的值影响轴标题，mgp[2]影响轴标签，mgp[3]影响轴线。\n\n图形布局：mfrow、mfcol分别按指定向量设置后续图形按行或列排列。\n\n轴相关参数：las设置轴标签与轴的关系，0为平行(默认)，1为总是水平，2为总是垂直于轴，3是总是竖直。tck和tcl用于设置轴刻度标记的长度，后者单位更小，可以实现更精细的调整。xaxs和yaxs设置x、y轴刻度样式，支持”r”和”i”，前者在数据范围两端扩展4%后确定最佳轴刻度标签，后者在原始数据范围内确定最佳轴刻度标签。xaxt和yaxt设置为”n”则不绘制x、y轴。xlog和ylog为TRUE时表示绘制对数轴刻度。xaxp和yaxp以c(min, max, n)设置在min和max范围内设置n个轴刻度线与标签。\n\n在控制台输入?par可以查询所有参数的设置详情和功能。\n\n\n\n\n\n\n\n\n图 3.12: 图形内部和外部边界参数\n\n\n\n\n\n\n\n\n3.3.3 lattice绘图系统\nlattice绘图系统优化了基础绘图的参数默认值，并以框格(trelli)的形式展示多个变量之间的关系，其特点是支持以公式(formula)形式来指定变量进行条件绘图，特别适合多维数据以控制变量来做分组可视化分析。lattice绘图系统的主要高级绘图函数见 表 3.11 所示。\n\n\n\n\n表 3.11: lattice绘图系统高级绘图函数\n\n\n\n\n\n\n图形函数\n图形名称\n公式\n\n\n\n\nbarchart\n条形图\nxA或Ax\n\n\nbwplot\n箱线图\nxA或Ax\n\n\ncloud\n3D散点图\nz~x*y|A\n\n\ncontourplot\n3D等高线图\nz~x*y\n\n\ndensityplot\n核密度图\n~x|A*B\n\n\ndotplot\n克利夫兰点图\n~x|A\n\n\nhistogram\n直方图\n~x\n\n\nsmoothScatter\n平滑密度散点图\nx\n\n\nlevelplot\n3D层次图\nz~y*x\n\n\nparallel\n平行坐标图\n数据框对象\n\n\nsplom\n散点图矩阵\n数据框对象\n\n\nstripplot\n条纹图\nAx或xA\n\n\nxyplot\n散点图\ny~x|A\n\n\nwireframe\n3D网格图\nz~y*x\n\n\nqqmath\n理论分位数图\nx\n\n\nqq\n双变量分位数图\ny~x\n\n\n\n\n\n\n\n\nformula主要是用来指定图中展示的变量及其关系，比如公式”x|A”就是指将A变量作为因子，绘制变量x在A的不同水平的关系；而”yx|A*B”则是在因子A和B的不同水平组合中绘制y和x之间的关系；“~x”表示只绘制变量x。\n使用lattice绘图系统绘图，需要用library()或require()函数加载lattice包。例如，绘制R内置数据集airquality中不同月份中臭氧与气温的关系，结果如 图 3.13 所示：\n\nlibrary(lattice)\n\ndf = airquality\ndf$Month = as.factor(df$Month)\nlattice::xyplot(Ozone ~ Temp | Month,data = df,\n                panel = function(y = df$Ozone, x = df$Temp){\n         panel.xyplot(x, y, col = \"darkgrey\", pch = 20)\n         panel.abline(v = mean(x, na.rm = TRUE),\n                      h = mean(y, na.rm = TRUE),\n                      lty = 2, col = \"blue\")\n         panel.lmline(x, y, col = \"red\")\n})\n\n\n\n\n\n\n\n图 3.13: lattice绘制分面散点图\n\n\n\n\n\n以上代码通过自定义函数的形式重新定义了panel参数，为臭氧和气温两个变量添加了平均值虚线和拟合直线，可以更直觉地观察和对比不同月份中臭氧、气温的变化以及二者之间的关系。\nlattice包提供的自定义panel参数的功能，可以根据需要综合多个绘图函数的功能，从而将分析结果更全面地呈现出来。\nR基础绘图系统的绘图函数的参数基本上也适用于lattice绘图函数。此外，lattice绘图函数的其他参数如下所示，这些参数可以方便地调整lattcie图形：\n\naspect: 指定绘图面板的纵横比，即高度/宽度\ngroup: 指定分组变量(因子)\nindex.coord: 指定面板排列顺序\nkey、 auto.key: 设置图例\nlayout: 指定面板布局，列数与行数\npanel: 指定在每个面板中生成的图形\nscales: 指定如何绘制坐标轴\nstrip: 指定是否绘制条带\nsubset: 创建数据子集用于绘图数据\ntype: 指定散点图绘图选项(“p”、“l”、“r”、“smooth”、“g”分别表示点、线、直线回归线、局部多项式回归拟合和网格图形)\n\n更多参数及功能参考各绘图函数的帮助信息。尽管lattice中的绝大多数绘图函数都可以在基础绘图系统中找到相应的替代，如contourplot()和levelplot()可以用基础绘图系统中的contour()和image()替代，但默认出图效果上，lattice优于基础绘图系统，这是因为lattice根据不同图形类型做了较好的参数配置和优化。\n\n\n3.3.4 ggplot2绘图系统\nggplot2是R中功能强大的著名绘图工具包，基于映射(mapping)和图层(layer)的概念，将数据变量映射到相应的图形美学特征，并层层叠加，从而创建出各种类型的图形，本质上是一种优雅的绘图语法。ggplot2汲取了基础绘图系统和lattice绘图系统的优点，兼具绘图功能强大与图形美观漂亮于一体，具有统一的绘图语法。 使用ggplot2绘图系统需要用library()或require()函数加载ggplot2包。该包是tidyverse套件成员，加载tidyverse套件会自动加载ggplot2包。\n\n3.3.4.1 ggplot2图形元素\nggplot2将图形分为以下8个元素：\n(1)数据(data)：绘图的基础，提供绘图所需要的变量。\n(2)美学映射(aesthetic mapping)：将数据中的变量与x轴、y轴、颜色、大小、形状、线宽、透明度等美学特征进行连接，这些都是在aes()函数中完成。\n(3)几何图形(geometry)：用于表征变量或变量间关系的几何图形，如点、线、柱形、条形、面等。在ggplot2中对应以geom_开头的各种函数。\n(4)统计变换(statistical transform)：对原始数据进行统计变换，在将变换后的统计信息添加到图形。在ggplot2中对应以stat_开头的各种函数。统计变化实际上是将特定的数据操作隐藏在可视化的背后。\n(5)标度(scale)：控制数据映射到图形的细节，如位置、颜色、大小、位置、形状、透明度等。在ggpplot2中对应以sacle_开头的各种函数。标度有助于创建更好的坐标轴刻度和图例。\n(6)坐标系(coordinate)：坐标系设置，包括笛卡尔坐标系、极坐标系等。在ggplot2中对应以coord_开头的函数。\n(7)分面(facet)：将数据集根据某个类别或因子型变量分组(分成子集)绘制图形，与lattice中的面板(panel)类似。在ggplot2中对应以facet_开头的各种函数。\n(8)主题(theme)：整个图形的风格设置，包括背景、网格、轴、字体、字号、颜色等。在ggplot2中对应theme()函数和以theme_开头的各种函数。\n\n其中最核心的元素是数据、美学映射和几何图形/统计变换(二者可以成为图层)，其中数据和美学映射一般在主函数ggplot()中指定，也可以在几何图形和统计变换函数中指定。ggplot2图形通过“+”将元素一层一层叠加到一起，流程清晰，代码优雅。\n\n\n\n\n\n\n\n\n图 3.14: ggplot2图形组件\n\n\n\n\n\n\n\n3.3.4.2 ggplot2绘图流程\nggplot2绘图流程的核心是ggplot()主函数和相关图层函数(geom_*和stat_*系列函数，*代表关键字)，完成从数据到图形的映射和图形的绘制，这是基本的数据组件。根据需要还可以增加其他标度的美学映射，如颜色、形状、大小、线型、填充、透明度等，将数据中其他变量进行可视化。有一些图形需要调整坐标系，如风速风向玫瑰图需要采用极坐标系，地图需要采用地图坐标系并注意设置投影方法等。各种scale_*系列函数可用于各种标度的调整。facet_*系列函数可以根据一个或多个分类变量分面绘图。最后是图形的美化，包括各种标题、标签、图例以及主题风格，属于非数据组件。\n下面用代码演示ggplot2绘图的各个环节：\n(1)数据、坐标轴映射与图元类型\n基于airquality，绘制散点图可视化其中Ozone与Temp两个变量的关系，并分别叠加折线图和拟合线图，结果如 图 3.14 所示。\nlibrary(ggplot2)\ndf = airquality\np = ggplot(data = df, mapping = aes(x = Temp, y = Ozone))\n# 省略参数名的代码\n# p = ggplot(df, aes(Temp, Ozone))  \n#散点图\np + geom_point()\n# 散点图叠加折线图\np + geom_point() + geom_line()  # 因缺失值导致折线有中断\n# 散点图叠加拟合线图\np + geom_point() + geom_smooth()  # 默认拟合方法是method = \"loess\"\n\n\n\n\n\n\n\n\n\n\n\n(a) 散点图\n\n\n\n\n\n\n\n\n\n\n\n(b) 散点图叠加折线图\n\n\n\n\n\n\n\n\n\n\n\n(c) 散点图叠加拟合线图\n\n\n\n\n\n\n\n图 3.15: ggplot2散点图及其与折线图和拟合线图的叠加\n\n\n\nggplot()中的参数data =和mapping=分别用于指定绘图数据来源和映射关系，可以省略。其中aes()函数用来指定标度与数据中变量的映射关系，上例中分别将Temp和Ozone变量映射到图形的x和y轴，这是指定图元的位置(position)标度，参数x=和y=也可以省略。geom_point()、geom_line()和geom_smooth()分别指定图元类型为散点图、折线图和平滑拟合线图。\n只要不冲突，ggplot2可以实现多种图元类型的叠加，从而可视化更多的信息。这种叠加用“+”号即可实现，但最先绘制的图层在整个图形的底层，而最后添加的图层在顶层。\nggplot2创建的对象，其存储结构为list类型，可用str()函数查看其数据结构：\n\nstr(p)\n\nList of 11\n $ data       :'data.frame':    153 obs. of  6 variables:\n  ..$ Ozone  : int [1:153] 41 36 12 18 NA 28 23 19 8 NA ...\n  ..$ Solar.R: int [1:153] 190 118 149 313 NA NA 299 99 19 194 ...\n  ..$ Wind   : num [1:153] 7.4 8 12.6 11.5 14.3 14.9 8.6 13.8 20.1 8.6 ...\n  ..$ Temp   : int [1:153] 67 72 74 62 56 66 65 59 61 69 ...\n  ..$ Month  : int [1:153] 5 5 5 5 5 5 5 5 5 5 ...\n  ..$ Day    : int [1:153] 1 2 3 4 5 6 7 8 9 10 ...\n $ layers     : list()\n $ scales     :Classes 'ScalesList', 'ggproto', 'gg' &lt;ggproto object: Class ScalesList, gg&gt;\n    add: function\n    add_defaults: function\n    add_missing: function\n    backtransform_df: function\n    clone: function\n    find: function\n    get_scales: function\n    has_scale: function\n    input: function\n    map_df: function\n    n: function\n    non_position_scales: function\n    scales: NULL\n    train_df: function\n    transform_df: function\n    super:  &lt;ggproto object: Class ScalesList, gg&gt; \n $ guides     :Classes 'Guides', 'ggproto', 'gg' &lt;ggproto object: Class Guides, gg&gt;\n    add: function\n    assemble: function\n    build: function\n    draw: function\n    get_custom: function\n    get_guide: function\n    get_params: function\n    get_position: function\n    guides: NULL\n    merge: function\n    missing: &lt;ggproto object: Class GuideNone, Guide, gg&gt;\n        add_title: function\n        arrange_layout: function\n        assemble_drawing: function\n        available_aes: any\n        build_decor: function\n        build_labels: function\n        build_ticks: function\n        build_title: function\n        draw: function\n        draw_early_exit: function\n        elements: list\n        extract_decor: function\n        extract_key: function\n        extract_params: function\n        get_layer_key: function\n        hashables: list\n        measure_grobs: function\n        merge: function\n        override_elements: function\n        params: list\n        process_layers: function\n        setup_elements: function\n        setup_params: function\n        train: function\n        transform: function\n        super:  &lt;ggproto object: Class GuideNone, Guide, gg&gt;\n    package_box: function\n    print: function\n    process_layers: function\n    setup: function\n    subset_guides: function\n    train: function\n    update_params: function\n    super:  &lt;ggproto object: Class Guides, gg&gt; \n $ mapping    :List of 2\n  ..$ x: language ~Temp\n  .. ..- attr(*, \".Environment\")=&lt;environment: R_GlobalEnv&gt; \n  ..$ y: language ~Ozone\n  .. ..- attr(*, \".Environment\")=&lt;environment: R_GlobalEnv&gt; \n  ..- attr(*, \"class\")= chr \"uneval\"\n $ theme      : list()\n $ coordinates:Classes 'CoordCartesian', 'Coord', 'ggproto', 'gg' &lt;ggproto object: Class CoordCartesian, Coord, gg&gt;\n    aspect: function\n    backtransform_range: function\n    clip: on\n    default: TRUE\n    distance: function\n    expand: TRUE\n    is_free: function\n    is_linear: function\n    labels: function\n    limits: list\n    modify_scales: function\n    range: function\n    render_axis_h: function\n    render_axis_v: function\n    render_bg: function\n    render_fg: function\n    setup_data: function\n    setup_layout: function\n    setup_panel_guides: function\n    setup_panel_params: function\n    setup_params: function\n    train_panel_guides: function\n    transform: function\n    super:  &lt;ggproto object: Class CoordCartesian, Coord, gg&gt; \n $ facet      :Classes 'FacetNull', 'Facet', 'ggproto', 'gg' &lt;ggproto object: Class FacetNull, Facet, gg&gt;\n    compute_layout: function\n    draw_back: function\n    draw_front: function\n    draw_labels: function\n    draw_panels: function\n    finish_data: function\n    init_scales: function\n    map_data: function\n    params: list\n    setup_data: function\n    setup_params: function\n    shrink: TRUE\n    train_scales: function\n    vars: function\n    super:  &lt;ggproto object: Class FacetNull, Facet, gg&gt; \n $ plot_env   :&lt;environment: R_GlobalEnv&gt; \n $ layout     :Classes 'Layout', 'ggproto', 'gg' &lt;ggproto object: Class Layout, gg&gt;\n    coord: NULL\n    coord_params: list\n    facet: NULL\n    facet_params: list\n    finish_data: function\n    get_scales: function\n    layout: NULL\n    map_position: function\n    panel_params: NULL\n    panel_scales_x: NULL\n    panel_scales_y: NULL\n    render: function\n    render_labels: function\n    reset_scales: function\n    resolve_label: function\n    setup: function\n    setup_panel_guides: function\n    setup_panel_params: function\n    train_position: function\n    super:  &lt;ggproto object: Class Layout, gg&gt; \n $ labels     :List of 2\n  ..$ x: chr \"Temp\"\n  ..$ y: chr \"Ozone\"\n - attr(*, \"class\")= chr [1:2] \"gg\" \"ggplot\"\n\n\n最新的ggplot2函数参考信息见https://ggplot2.tidyverse.org/reference/index.html\n(2)颜色、形状等其他标度的映射\n除了数据位置标度的映射(x、y轴)，aes()函数还可以指定图元其他的标度如颜色(color和fill)、大小(size)、形状(shape)、线型(linetype)、透明度(alpha)等与数据变量的映射。\n例如，要给 图 3.15 (a) 中的数据点设置颜色，有两种方法：一种是设置统一的颜色，另一种是根据数据集中某个变量来给数据点设置不同的颜色。前者与数据映射无关，不能在aes()函数中设置；后者与数据映射有关，需要在aes()函数中设置。下面的代码演示了这两种设置颜色的方法，结果如 图 3.16 所示，注意两者的区别：\n# 设置统一颜色\np + geom_point(color = \"blue\")\n# 根据Month变量设置颜色\np + geom_point(aes(color = Month))\n\n\n\n\n\n\n\n\n\n\n\n(a) 设置统一颜色\n\n\n\n\n\n\n\n\n\n\n\n(b) 根据变量设置颜色\n\n\n\n\n\n\n\n图 3.16: 设置颜色标度\n\n\n\n由于变量Month是连续型变量，所以自动使用了渐变色，同时自动添加图例，以指示颜色标度与相应数据变量的关系。显然，将月份以渐变色来表征与期望不相符。下面的代码将Month变量转换为因子型变量，ggplot2就会自动使用离散色系来表征，可视化效果更好。结果如图 图 3.17 (a) 所示。用户还可以用不同点的形状来映射Month变量，结果如 图 3.17 (b) 所示。\n# 将Month变量因子化\np + geom_point(aes(color = as.factor(Month)))\n# 将Month变量因子化并映射至shape标度\np + geom_point(aes(shape = as.factor(Month)))\n\n\n\n\n\n\n\n\n\n\n\n(a) 设置统一颜色\n\n\n\n\n\n\n\n\n\n\n\n(b) 根据变量设置颜色\n\n\n\n\n\n\n\n图 3.17: 设置颜色标度\n\n\n\n对于连续型和离散型数据，其标度映射存在差异，如 表 3.12 所示。\n\n\n\n\n表 3.12: ggplot2图元分类及标度映射\n\n\n\n\n\n\n\n\n\n\n\n\n图元\n函数\n离散标度\n连续标度\n\n\n\n\n点\ngeom_point、geom_jitter、geom_dotplot等\ncolor、fill、shape\ncolor、fill、alpha、size\n\n\n线\ngeom_line、geom_path、geom_curve、geom_density、geom_linerange、 geom_|step、geom_abline、geom_hline等\ncolor、linetype\ncolor、size、alpha\n\n\n形\ngeom_polygon、geom_rect、geom_bar、geom_ribbon、geom_area、 geom_histogram、geom_|violin等\ncolor、fill\ncolor、fill、alpha\n\n\n文本\ngeom_text、geom_label\ncolor\ncolor、angle、vjust、hjust\n\n\n\n\n\n\n\n\n(3)坐标系\n坐标系的主要功能是将两个位置上的图元属性组合起来形成二维方位系统。常见的图元位置属性在笛卡尔坐标系中一般被称为x和y，但在极坐标系中称作角度和长度。ggplot2提供了多种坐标系，不同绘图函数的默认坐标系不同，各坐标系的相关函数与功能如下：\n\ncoord_cartesian()：笛卡尔坐标系\n\ncoord_fixed()、coord_equal()：同尺度笛卡尔坐标系\n\ncoord_polar()：极坐标系\n\ncoord_flip()：翻转的笛卡尔坐标系\n\ncoord_munch()：切分线段实现独立转换\n\ncoord_sf()：绘制sf地图对象的坐标系\n\ncoord_map()、coord_quickmap()：地图投影坐标系\n\ncoord_trans()：转换连续型变量的坐标系\n\n以下代码基于默认的笛卡尔坐标系和极坐标系绘制了airquality数据集中Ozone变量的频数分布图，结果如 图 3.18 所示。\nlibrary(ggplot2)\n\np = ggplot(airquality, aes(Ozone, fill = as.factor(Month))) \n# 基于默认的笛卡尔坐标系\np + geom_histogram()\n# 采用极坐标系\np + geom_histogram() + coord_polar()\n\n\n\n\n\n\n\n\n\n\n\n(a) 笛卡尔坐标系\n\n\n\n\n\n\n\n\n\n\n\n(b) 极坐标系\n\n\n\n\n\n\n\n图 3.18: 笛卡尔坐标系和极坐标系\n\n\n\n最常用的也是默认的坐标系是笛卡尔坐标系，在coord_cartesian()函数中，可通过xlim和ylim参数设置x和y轴的显示范围，expand参数为TRUE时，会给轴显示范围添加一个小的扩展，以避免数据与轴线重叠。coord_fixed()函数中提供了一个aspect参数来设置纵横比(y与x轴单位长度比)，这在y与x轴对应变量的量纲可比时非常有用。极坐标系用于各种圆形图，如饼图、风速风向玫瑰图等，coord_polar()函数中，参数theta设置角度映射到x还是y轴，参数start设置起始点与12点钟方向的偏移，参数direction设置角度正方向，1为顺时针，-1为逆时针。\n(4)坐标轴标度\n如果要对x、y轴的刻度进行变换(对数转换、平方根转换等)或逆序排列、调整刻度间隔、改变刻度标签等，可以利用scale_x_和scale_y_系列函数来实现:\n\nscale_x_continous()、scale_y_continous()：调整连续型变量x、y轴标度\n\nscale_x_discrete()、scale_y_discrete()：调整离散型变量x、y轴标度\nscale_x_reverse()、scale_y_reverse()：连续型变量轴标度逆序\n\nscale_x_log10()、scale_y_log10()：连续型变量轴标度对数化\nscale_x_sqrt()、scale_y_sqrt()：连续型变量轴标度平方根化\nscale_x_binned()、scale_y_binned()：以分箱方法离散化连续型数据\nscale_x_date()、scale_y_date()：调整日期型数据的轴标度\nscale_x_datetime()、scale_y_datetime()：调整日期时间型数据的轴标度\nscale_x_time()、scale_y_time()：调整时间型数据的轴标度\n\n在这些函数中通用的参数有：\n\nname：轴标度名称(轴标题)\n\nbreaks：调整坐标刻度(NULL不显示刻度，或一组指定的数值向量；默认采用waiver()自动计算)\n\nlabels：调整坐标刻度显示标签(NULL不显示刻度，或一组指定的字符向量；默认是采用waiver()自动计算)\n\nlimits：调整轴标度显示范围(NULL为默认显示范围，或指定两个元素的数值向量或一组字符向量)\n\nexpand：轴两端的扩展，通过expansion()函数为其赋值\n\nposition：y轴可赋值left或right，x轴可赋值top和bottom\n\n其他参数详见系统函数帮助信息。\n下面的代码调整了 图 3.18 中x轴刻度的显示，结果如 图 3.19 所示。\n# x轴刻度逆序\np + geom_histogram() + scale_x_reverse()   \n# x轴刻度对数化\np + geom_histogram() + scale_x_log10(name=\"log10(Ozone)\",    \n                                     breaks=c(1,5,10,50,100),\n                                     expand = expansion(add = 0.5))\n\n\n\n\n\n\n\n\n\n\n\n(a) x轴刻度逆序显示\n\n\n\n\n\n\n\n\n\n\n\n(b) x轴刻度对数变换\n\n\n\n\n\n\n\n图 3.19: 调整x轴刻度显示\n\n\n\n图 3.19 (a) 实现了x轴刻度逆序，图 3.19 (b) 实现了x轴刻度对数化，并调整了刻度显示和轴两端的扩展。在连续型数据轴标度调整函数(scale_x_continous()和scale_y_continous())中有一个trans参数可以实现更多类型的数据变换，包括自定义变换函数。\n其他标度如alpha(透明度)、color(颜色)、fill(填充色)、linetype(线型)、size(大小)、shape(形状)等的调整，可以利用相应的scale_*_xxx()系列函数实现，其中*为alpha、color、fill、linetype、shape、size等标度关键字，具体使用方法参考各函数的帮助信息。\n(5)分面绘图\nfecet_wrap()和facet_grid()两个函数可以用于实现根据一个或多个分类变量(因子型变量)进行分面绘图，从而比较分析不同因子或因子组合条件下，x和y轴对应变量的关系。前者通常将每个面板尽可能绘制成正方形，以充分利用绘图空间，后者可以将面板形成一行多列、多行一列或多行多列的网格排列。对于单个分类变量分面绘图的，建议采用前者，反之，则建议采用后者。\n下面的代码演示了这两个函数根据airquality数据集中Month变量来分面绘制呈现Temp和Ozone变量关系的散点图，结果如 图 3.20 所示。\nlibrary(ggplot2)\n\ndf = airquality\n\np = ggplot(df, aes(Temp, Ozone)) + \n  geom_point() + \n  geom_smooth(method = \"lm\")\n\np + facet_wrap(. ~ Month) \np + facet_grid(. ~ Month)\np + facet_grid(Month ~ .)\n\n\n\n\n\n\n\n\n\n\n\n(a) facet_wrap()分面\n\n\n\n\n\n\n\n\n\n\n\n(b) facet_grid()垂直分面\n\n\n\n\n\n\n\n\n\n\n\n(c) facet_grid()水平分面\n\n\n\n\n\n\n\n图 3.20: 分面绘图\n\n\n\n(6)图例\n除了x、y轴映射之外的其他标度与变量的映射，都会自动产生默认的图例。图例的调整主要通过guides()函数和theme()函数中与图例相关的参数来调整。\n删除图例，可用guides(xxx=\"none\")，其中xxx为相应标度的关键字；也可用在theme()函数中将参数legend.position的值设置为”none”来达到同样的目的。\n修改图例标题，可用labs()函数中给相应标度关键字赋值字符串即可；如果同时修改图例标签，建议在相应关键字的scale_*_xxx()系列标度函数中，通过name参数指定图例标题，labels参数指定图例标签。\n下面的代码演示了两种修改图例的方法，结果如 图 3.21 所示。\nlibrary(ggplot2)\n\np = ggplot(airquality, \n           aes(Month, Ozone, fill = as.factor(Month))) + \n  geom_boxplot()\n\n# 自动生成的默认图例\np\n# 修改图例标题\np + labs(fill = \"月份\")\n# 修改图例标题和标签\np + scale_fill_discrete(name = \"月份\",\n                        labels = paste0(5:9,\"月\"))\n\n\n\n\n\n\n\n\n\n\n\n(a) 自动生成的图例\n\n\n\n\n\n\n\n\n\n\n\n(b) 修改图例标题\n\n\n\n\n\n\n\n\n\n\n\n(c) 修改图例标题和标签\n\n\n\n\n\n\n\n图 3.21: 修改图例\n\n\n\n图 3.21 (a) 中图例为自动生成的默认图例， 图 3.21 (b) 中图图例用labs()函数修改了图例标题，右图用scale_fill_discrete()函数修改了图例标题和标签。\n图例的位置可以在theme()函数中用legend.position参数来调整，其取值有”none”、“left”、“right”、“bottom”、“top”和”inside”，或者一个二元素的数值向量(0靠近左和下，1靠近右和上)，一般用于将图例放置在图形中的某个位置。该函数中的legend.direction参数用来调整图例方向，取值有”horizontal”和”vertical”，另一个参数legend.justification用来指图例定对齐。\n下面的代码演示了图例位置的调整方法，结果如 图 3.22 所示。\n# 图例调整到图形区下方\np + theme(legend.position = \"bottom\") + \n  labs(fill = \"Month\")\n# 图例调整图形区左上角\np + theme(legend.position = \"inside\",\n          legend.position.inside = c(0.1,0.95),\n          legend.direction = \"horizontal\",\n          legend.justification = c(0.1,0.95)) + \n  labs(fill = \"Month\")\n\n\n\n\n\n\n\n\n\n\n\n(a) 调整图例至图形区下方\n\n\n\n\n\n\n\n\n\n\n\n(b) 调整图例至图形区左上角\n\n\n\n\n\n\n\n图 3.22: 调整图例位置\n\n\n\n(7)主题\nggplot2的主题系统用于精细调整图形中的非数据元素，不会影响几何对象和标度等数据元素，使图像更美观，特别是满足整体一致性的要求。R的基础绘图系统和lattice绘图系统没有将数据和非数据元素的控制进行分离，图形精细调整比较复杂。\nggplot2主题系统的设置通过theme()函数完成。在theme()中指定要调整的非数据元素，并通过与之相应的绑定函数来完成该元素的属性(颜色、大小、位置、线型、填充等)调整操作。主题系统将非数据元素分为三大类，即线(line)、矩形(rect)和文本(text)，与之对应的绑定函数分别是element_line()、element_rect()和element_text()。从文本衍生出的标题(title)，用于控制图形、轴以及图例的标题，也与element_text()函数绑定。这些元素的从属对象包括图形(plot)、轴(axis)、图例(legend)、分面面板(panel)和分面面板上部的条带区(strip)，此外还有用于调整绘图面板纵横比的参数aspect.ratio。在控制台输入?theme，可以打开theme()函数的帮助页面，了解所有可以被控制的图形非数据元素，例如title元素同时调整图形、轴和图例的标题，axis.title调整所有坐标轴的标题，axis.title.x调整x轴的标题，axis.title.x.top调整上x轴的标题，这些标题元素都是对文本元素的继承。\n下面的代码用于调整 图 3.22 (a) 中轴标题(axis.title)和轴刻度标签(axis.text)大小,结果如 图 3.23 (a) 所示。\n\np1 = p + theme(legend.position = \"bottom\") +\n  theme(axis.text = element_text(size = 12),\n        axis.title = element_text(size = 15))\n\n以下代码利用element_blank()函清除图形面板背景(panel.background)，再用element_rect()函数绘制图形面板边框并设置填充为透明色，结果如 图 3.23 (b) 所示。\n\np2 = p + theme(legend.position = \"bottom\") +\n  theme(axis.text = element_text(size = 12),\n        axis.title = element_text(size=15),\n        panel.background = element_blank(),\n        panel.border = element_rect(fill = \"transparent\")) # 绘制图形边框\n\n\n\n\n\n\n\n\n\n\n\n\n(a) 调整轴标题和轴刻度标签文本大小\n\n\n\n\n\n\n\n\n\n\n\n(b) 修改图形面板背景和绘制图形面板边框\n\n\n\n\n\n\n\n图 3.23: 主题元素调节\n\n\n\nelement_text()函数用于调整图形文字外观，使用非常频繁。该函数提供了size、family、face、color、angle、vjust和hjust等参数，分别用于设置和调整文字字号、字体族、字体风格、样色、倾斜角度以及水平和垂直位置调整。对于family参数，默认值可以用windowsFonts()函数查看，Windows系统包括”serif”、“sans”和”mono”三个字体族，分别对应字体Time New Roman、Arial和Courier New。如果需要调用windows系统的字体，可以先用windowsFonts()函数将Windows系统中的指定字体赋值给变量，然后在element_text()函数中用windowFont()函数调用相应字体的变量即可：\n\nwindowsFonts(MSYH = windowsFont(\"微软雅黑\"))\np + theme(axis.title = element_text(family = \"MSYH\"))\n\n参数face提供了4种可选的字体风格：plain(常规)、bold(粗体)、italic(斜体)和bold.italic(粗斜体)，可根据需要来设置使用。\n除了提供了theme()函数调整图形的各种非数据元素外，ggplot2还提供了8个预置的主题：theme_bw()、theme_classic()、theme_dark()、theme_gray()(默认主题)、theme_light()、theme_linedraw()、theme_minimal()、theme_void()(空主题)和theme_test()(用于视觉测试的主题)。使用预置主题后，用户仍然可以通过theme()函数来调整非数据元素。\n(8)添加注释\n当需要在绘图区为每个图形元素添加文字注释时，可以使用geom_text()和geom_label()，在其中指定映射到参数label的变量(通常是一个包含在绘图数据框中的字符型变量)。下面的代码演示了这两个函数添加文字注释的方法和视觉差异，结果如 图 3.24 所示。\nlibrary(ggplot2)\n\ndf = data.frame(x = 1:5, y = 2:6, label = letters[1:5])\n\nggplot(df, aes(x, y)) + geom_point() +\n  geom_label(aes(label = label), hjust = -0.5, vjust = 0.5) +\n  theme_bw() + xlim(1, 6) + ylim(1, 7)\n\nggplot(df, aes(x, y)) + geom_point() +\n  geom_text(aes(label = label), hjust = -0.5, vjust = 0.5) +\n  theme_bw() + xlim(1, 6) + ylim(1, 7)\n\n\n\n\n\n\n\n\n\n\n\n(a) geom_label()添加文字注释\n\n\n\n\n\n\n\n\n\n\n\n(b) geom_text()添加文字注释\n\n\n\n\n\n\n\n图 3.24: 添加文字注释\n\n\n\n如果要添加的注释不是来自绘图数据集中变量的映射，可使用annotate()函数。该函数可以在指定位置添加文字(text)、矩形(rect)、线段(segment)、点线段(pointrange)等。以下代码演示了annotate()在图形中添加文字、矩形、线段(用arrow()函数绘制箭头)和点线段的方法，结果如 图 3.25 所示。\ndf = data.frame(x = 1:5, y = 2:6)\np = ggplot(df, aes(x, y)) + geom_point() \n# 原始图形\np\n# 添加文字和矩形\np + annotate(\"text\", x = 2, y = 5,\n             label = \"italic(R)^2 == 0.88\", parse = TRUE) +\n  annotate(\"rect\", xmin = 2.5, xmax = 3.5,\n           ymin = 3.5, ymax = 4.5, alpha = 0.3)\n# 添加线段和点线\np + annotate(\"segment\", x = 2.5, xend = 3.5, y = 3.5,\n             yend = 4.5, color = \"blue\", size = 1,\n             arrow = arrow(angle = 40, length = unit(0.25, \"cm\"),\n                           ends = \"last\", type = \"open\")) +\n  annotate(\"pointrange\", x = 2, y = 4, ymin = 3, ymax = 5,\n           color = \"red\", size = 1)\n\n\n\n\n\n\n\n\n\n\n\n(a) 原始图形\n\n\n\n\n\n\n\n\n\n\n\n(b) 添加文字和矩形\n\n\n\n\n\n\n\n\n\n\n\n(c) 添加线段和点线\n\n\n\n\n\n\n\n图 3.25: annotate()添加多种注释\n\n\n\n如果要添加曲线，可使用geom_curve()函数，同样可以在其中用arrow()为曲线两端绘制箭头。\n(9)标题和脚注\nlabs()函数可以设置和调整图形标题(title)、副标题(subtitle)、脚注(caption)、图号标签(tag)、轴标题(x，y)、图例标题(参数为产生图例的标度关键字，如fill、color等)等。ggtitle()函数仅能够设置图形标题(label)和副标题(subtitle)。设置和调整轴标题还可以直接使用xlab()和ylab()函数。以下代码演示为图形设置各种文本元素，结果如 图 3.26 所示。\nlibrary(ggplot2)\n\np = ggplot(airquality, \n           aes(Month, Ozone, fill = as.factor(Month))) + \n  geom_boxplot() +\n  scale_fill_discrete(name = \"月份\",\n                      labels = paste0(5:9,\"月\"))\n\nwindowsFonts(MSYH = windowsFont(\"msyh.tcc\"))\n\n# 原始图形\np\n# 添加\np + labs(title = \"空气质量数据箱线图\",\n         subtitle = \"臭氧浓度数据\",\n         caption = \"数据来源: 美国纽约1973年5月至9月监测数据.\",\n         tag = \"A\") +\n  xlab(\"月份\") + \n  ylab(\"臭氧浓度\") +\n  theme(plot.title = element_text(family = \"MSYH\",\n                                  face = \"bold\"))\n\n\n\n\n\n\n\n\n\n\n\n(a) 原始图形\n\n\n\n\n\n\n\n\n\n\n\n(b) 添加标题、脚注等元素\n\n\n\n\n\n\n\n图 3.26: 设置图形标题、脚注等元素\n\n\n\n在ggplot2生态中，有很多辅助功能的扩展包，如ggthemes包提供了更多风格的主题、标度和几何图形，ggsci包提供了知名学术期刊的配色，RColorBrewer包也提供了比较丰富的配色方案，ggrepel包可以添加更美观的文本标签，patchwork包可以用很简单的方式将多个图形组合起来。在ggplot2 extensions网站可以查询和了解更多各种ggplot2的扩展包。\n\n\n3.3.4.3 ggplot2图形保存\n除了可以用pdf()、svg()、tiff()、png()等文件设备函数保存ggplot2的图形外，ggplot2也提供了一个专用的保存图形的函数——ggsave()，建议ggplot2绘制的图形只用该函数来保存。该函数的参数有：\n\nfilename：指定文件名\n\nplot：保存的图形，默认是保存最后显示的图形\n\ndevice：指定图形设备，即图形保存类型；可以使用设备函数，也可以是文件类型扩展名，如pdf、png、svg、tiff等\n\npath：指定文件保存路径\n\nscale：缩放系数，默认为1\n\nwidth：图形宽度\n\nheight：图形高度\n\nunits：图形尺寸的单位，可选英寸(in)、厘米(cm)、毫米(mm)或像素(px)\n\ndpi：图形分辨率(每英寸点数)，默认为 300，仅应用于栅格图形\n\nlimitsize：为TRUE时，ggsave()函数不会保存尺寸超过50英寸×50英寸的图形\n\nbg：设置图形背景颜色\n\n此外，在图形设备函数中可用的参数也可用于ggsave()函数中。\n电脑屏幕显示的图形和ggsave()保存的图形是有差异的，以下代码演示了两者的差异。先将 图 3.26 (b) 用ggsave()函数保存到当前目录的“images”子目录中：\n\np1 = p + labs(title = \"空气质量数据箱线图\",\n         subtitle = \"臭氧浓度数据\",\n         caption = \"数据来源: 美国纽约1973年5月至9月监测数据.\",\n         tag = \"A\") +\n  xlab(\"月份\") + \n  ylab(\"臭氧浓度\") +\n  theme(plot.title = element_text(family = \"MSYH\",\n                                  face = \"bold\"))\nggsave(\"fig3.27-2.png\", p1, \n       path = \"./images/\", device = \"png\",\n       height = 3, width = 5, units = \"in\", \n       dpi = 300)\n\nWarning: Removed 37 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) 屏幕显示的图形\n\n\n\n\n\n\n\n\n\n\n\n(b) ggsave()保存的图形\n\n\n\n\n\n\n\n图 3.27: 屏幕显示的图形和ggsave()保存的图形\n\n\n\n\n练习 3.1 \n(1)将airquality数据集赋值给df，用summary()函数进行摘要分析。然后利用VIM包中的rangerimpute()函数和数据集中其他变量信息对Ozone变量的缺失值进行填补。填补后再用summary()函数进行摘要分析。\n(2)利用data.frame()函数生成一个新的数据框data，其中第一个变量名为date，用seq()函数结合as.Date()函数生成1973年5月1日至1973年9月30日的日期型数据，第二个变量为ozone，取值为df中已经填补缺失值的Ozone变量；然后用summary()函数进行摘要分析。\n(3)利用ggplot2将该数据框绘制成散点图叠加连线图，点和线分别采用红色和蓝色。然后调整x和y轴的标题分别为“日期”和“臭氧浓度”，并将字体改为“微软雅黑”，删除绘图面板中的背景色和网格线。\n(4)利用ggsave()函数将图形保存为tiff格式，宽度为10cm，高度为6cm，dpi为300。",
    "crumbs": [
      "基础知识",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>环境数据整理与可视化</span>"
    ]
  },
  {
    "objectID": "CH3.html#r数据可视化实践",
    "href": "CH3.html#r数据可视化实践",
    "title": "3  环境数据整理与可视化",
    "section": "3.4 R数据可视化实践",
    "text": "3.4 R数据可视化实践\n\n3.4.1 数据关系与可视化类型\n数据通常反映5种关系：构成、比较、趋势、分布和联系。\n构成反映整体与局部的关系，以呈现整体中每个部分占总体比例的多少。例如在某个地区全年COD总排放量中，不同行业的排放量所占的百分比，此时可选择饼图(pie chart)。\n比较反映相同变量在不同分组或地点或时间上的多少、大小、高低等关系。例如某地自2010年至2020年COD排放总量，又如全国各省会城市2021年PM2.5年平均浓度，此时可选择条形图(bar chart)。\n趋势反映变量随时间、纬度、海拔等变化的关系，常用于时序类数据的分析。例如某地大气中臭氧浓度随小时、月份和季度的变化，此时通常选择线图(line chart)来表示。\n分布反映变量在连续的不同数值范围内或不同地理位置上出现的频数，从而观察在哪个范围或位置上比较集中，是否对称，近似哪种概率分布类型等。例如展示一年内PM2.5日均浓度落在≤35、36-75、76-115、116-150、151-250、≥251μg/m3六个浓度范围内的个数，来观察在哪个范围频数最多、整体分布是否为正偏态分布，此时往往选用频数分布柱状图(histogram chart)。\n联系反映两个变量之间存在的某种相关关系，例如变量1随变量2增大而减小，或变量1随变量2增大而先增大后减小等，此时一般选择散点图(scatter chart)\n此外，反映变量与地理空间位置的关系，一般要结合地图(map)来展示。雷达图(radar chart)常用于展示高维数据(3维以上)的关系。词云图(word cloud chart)用于展示海量文本中重要关键词出现的频率。在这些基本图形类型的基础上，还有很多衍生出来的图形类型，能够更好地可视化各种具体的数据关系，以及更好地呈现变量内或变量间的联系。\n\n\n3.4.2 可视化整体局部关系\n\n3.4.2.1 饼图\nR基础绘图系统中提供了pie()函数用于绘制饼图，ggpubr包提供了ggpie()函数绘制饼图。下面的代码演示用ggplot2来间接绘制饼图：\nlibrary(tidyverse)\n\ndf = data.frame(\n  pollutant = c(\"粉尘\", \"氮氧化物\", \"硫氧化物\", \"挥发性有机物\"),\n  emission = c(15, 3.5, 2.3, 1.2))\n\np = ggplot(df, aes (x=\"\", y = emission, fill = pollutant)) + \n  geom_col(position = 'stack', width = 1) +\n  theme_void() +\n  labs(fill = \"pollutant\",\n       x = NULL,\n       y = NULL,\n       title = \"ggplot2绘制的饼图\") + \n  coord_polar(theta = \"y\")\n\n# 添加原始数据作为标签\np + geom_text(aes(label = emission, x = 1.3),\n              position = position_stack(vjust = 0.5))\n# 添加百分比数据作为标签\np + geom_text(aes(label = paste(round(emission / sum(emission) * 100, 1), \"%\"), x = 1.3),\n              position = position_stack(vjust = 0.5)) \n\n\n\n\n\n\n\n\n\n\n\n(a) 添加原始数据\n\n\n\n\n\n\n\n\n\n\n\n(b) 添加构成百分比\n\n\n\n\n\n\n\n图 3.28: ggplot2绘制饼图\n\n\n\n\n\n3.4.2.2 圆环图\n圆环图是饼图的衍生类型，ggpubr中的ggdonutchart()函数能够直接圆环图圆环图(donut chart)。下面演示用ggplot2包来绘制，但使用(图3-39右图)：\n\nhsize = 2\ndf = df %&gt;% \n  mutate(x = hsize)\n\np = ggplot(df, aes (x = x, y = emission, fill = pollutant)) +\n  geom_col() +\n  coord_polar(theta = \"y\") +\n  xlim(c(0.2, hsize + 0.5)) +\n  theme_void()\n\np + geom_text(aes(label = paste(round(emission / sum(emission) * 100, 1), \"%\"), x = 2),\n              position = position_stack(vjust = 0.5))\n\n\n\n\n\n\n\n图 3.29: ggplot2绘制圆环图\n\n\n\n\n\n\n\n3.4.2.3 矩形树状图\n矩形树状图(rectangular treemap)通过嵌套矩形形式可视化数据的层次结构关系，其原理是用面积大小表征每个部分的数量。下面基于 图 3.28 的数据，利用ggplot2的扩展包treemapif包绘制矩形树状图：\n\nlibrary(treemapify)\nggplot(df, aes(area = emission, fill = pollutant,\n               label = paste(round(emission / sum(emission) * 100, 1), \"%\"))) +\n  geom_treemap() +\n  geom_treemap_text(colour = \"white\", place = \"center\", reflow = T)\n\n\n\n\n\n\n\n图 3.30: treemapify绘制矩形树状图\n\n\n\n\n\n此外， ggraph包提供了绘制冰柱图(icicle chart)和旭日图(sunburst chart)的功能。ggraph是ggplot2的一个扩展，擅长于可视化网络结构、图结构和树状结构的数据关系。华夫饼图(waffle chart)用统一大小的方块或点来量化呈现各部分在整体中所占的数量或百分比，waffle包中的waffle()函数可以快速绘制华夫饼图。马赛克图(mosaic chart)是一种类似矩形树状图的可视化方案，也是用面积大小来表征数量或百分比，R的基础绘图系统提供的mosaicplot()函数、ggmosaic包中的geom_mosai()函数以及vcd包中的mosaic()函数都可以绘制马赛克图。\n\n\n\n3.4.3 可视化类别比较关系\n\n3.4.3.1 条形图\n条形图(bar chart)通过条柱的高低来比较各类别中相同的一个或多个变量的差异，还能以百分比的形式比较各类别中相同变量的构成关系。ggplot2包提供了geom_bar()和geom_col()两个绘制柱形图的函数，前者默认使用stat_count()统计函数，后者默认使用stat_identity()统计函数。\nlibrary(tidyverse)\n\n# 创建绘图数据\nset.seed(2024)\n\ndf = data.frame(\n  emission = round(rnorm(15, mean = 20, sd = 4), 1),\n  city = rep(LETTERS[1:3], each = 5),\n  pollutant = rep(c(\"PM2.5\", \"PM10\", \"SOx\", \"NOx\", \"O3\"), time = 3))\n\ndf = df %&gt;% \n  group_by(city) %&gt;% \n  mutate(percent = round(emission / sum(emission), 2))\n\n# 单变量柱形图\nggplot(df[df$pollutant == \"PM10\", ], aes(city, emission, fill = city)) +\n  geom_bar(stat = \"identity\", color = \"white\") + \n  theme_bw() + ylab(\"PM10\") +\n  geom_text(aes(label = df[df$pollutant == \"PM10\", ]$emission),\n            vjust = 2, color = \"white\")\n  \n\n# 多变量条形图\np = ggplot(df, aes(city, emission, \n                   fill = pollutant, \n                   group = city)) + \n  theme_bw()\n  \n# position = dodge2，绝对数非堆积形式\np + geom_bar(stat = \"identity\", color = \"white\", position = \"dodge2\") +\n  geom_text(aes(label = emission), color = \"white\", \n            position = position_dodge2(0.9),\n            vjust = 2) \n\n# position = stack，绝对数堆积形式\np + geom_bar(stat = \"identity\", color = \"white\", position = \"stack\") +\n  geom_text(aes(label = emission), color = \"white\", \n            position = position_stack(0.5))\n\n# position = fill，相对数堆积形式\np + geom_bar(stat = \"identity\", color = \"white\", position = \"fill\") +\n  geom_text(aes(label = percent), color = \"white\", \n            position = position_fill(0.5))\n\n\n\n\n\n\n\n\n\n\n\n(a) 单变量条形图\n\n\n\n\n\n\n\n\n\n\n\n(b) 绝对数非堆积形式多变量条形图\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) 绝对数堆积形式多变量条形图\n\n\n\n\n\n\n\n\n\n\n\n(d) 相对数堆积形式多变量条形图\n\n\n\n\n\n\n\n图 3.31: 不同的条形图\n\n\n\nggplot2的几何图形函数和统计变换函数中都有一个参数position，其默认值为”identity”，表示不调整几何图元的位置，由映射变量控制。但在一些图形绘制中，需要改变图元位置，此时可利用position_*()系列函数调整图元位置：\n\npositon_dodge()：图元并排，需要指定分组变量，组内图元无间隙。\n\npositon_dodge2()：图元并排，不需要指定分组变量，组内图元有间隙。对于条形和矩形图元，可以设置可变宽度。\n\npositon_fill()：图元按组内比例堆积。\n\npositon_identity()：不调整图元位置。\n\npositon_jitter()：添加抖动，减少重叠，对于小数据散点图特别有用。\n\npositon_jitterdodge()：图元并排的同时添加抖动，在散点图和箱线图重叠时特别有用。\n\npositon_nudge()：通过指定x和y的值来平移图元位置，在geom_text()函数中内置了该函数，以避免文本与图元重叠。\n\npositon_stack()：图元按绝对数堆积。\n\n如果需要，可使用coord_flip()函数翻转x和y轴，形成横向条形图。\n如果需要用条形的宽度表征另一个变量，可通过绘制不等宽的条形图来实现。例如下面的数据集，需要用条形的高度代表城市的PM2.5年均浓度(μg/m3)，柱形的宽度代表城市的年GDP(10亿元)。这可以利用ggplot2中的geom_rect()来绘制，但在使用该函数之前，需要计算出相关的绘图参数。以下代码演示了这一方法，结果如 图 3.32 所示。\n\ndf = data.frame(city = LETTERS[1:3],\n                pm25 = c(67, 45, 23),\n                gdp = c(20, 10, 5))\n\n# 计算每个柱形在x轴的起点和终点\ndf$xmin = 0\nfor(i in 2:3) df$xmin[i] = sum(df$gdp[1:i - 1])\nfor(i in 1:3) df$xmax[i] = sum(df$gdp[1:i])\n\n# 构造x轴刻度标签的坐标\nfor(i in 1:3) df$xpos[i] = sum(df$gdp[1:i]) - df$gdp[i] / 2\n\n# 绘制不等宽柱形图\nggplot(df) +\n  geom_rect(aes(xmin = xmin, xmax = xmax, ymin = 0, ymax = pm25,\n                fill = city), color = \"white\", linewidth = 0.25) +\n  geom_text(aes(x = xpos, y = pm25 + 3, label = pm25), size = 3) +\n  geom_text(aes(x = xpos, y = -4, label = city), size = 3) +\n  theme_bw() + \n  labs(x = \"city\", y = expression(PM[2.5]))\n\n\n\n\n\n\n\n图 3.32: 不等宽条形图\n\n\n\n\n\n\n\n3.4.3.2 克利夫兰点图\n克利夫兰点图(Cleveland’s dot plot)及其变体棒棒糖图(lollipop chart，增加了从轴连接到点的线段)、哑铃图(dumbbell plot，线段连接两个端点，用于表征某个变量在两个不同分组上的值)是将柱形图的柱形简化为点和线，减少空间占用，聚焦数据展示，相对而言更加简洁和美观。绘制这三种图可用ggplot2中的geom_segment()和/或geom_point()来实现，也可直接用R基础绘图系统提供的dotchart()函数，更简便 的方法是利用ggpubr包中的ggdotchart()函数绘制。\n例如，现有某年15个地区的煤炭使用量(万吨)数据(随机构造)：\n\ndf = data.frame(city = LETTERS[1:15],\n                coal = round(rnorm(15, 40, 10)))\ndf\n\n   city coal\n1     A   39\n2     B   31\n3     C   45\n4     D   24\n5     E   20\n6     F   51\n7     G   43\n8     H   50\n9     I   38\n10    J   44\n11    K   25\n12    L   39\n13    M   47\n14    N   19\n15    O   39\n\n\n使用ggpubr包绘制，结果如图 图 3.33 (a) 所示；使用ggplot2包绘制，结果如图 图 3.33 (b) 所示。\nlibrary(ggpubr)\nggdotchart(df, x = \"city\", y = \"coal\",\n           color = \"city\", palette = rainbow(20),\n           sorting = \"descending\", add = \"segments\",\n           rotate = TRUE, dot.size = 3, label = \"coal\",\n           font.label = list(size = 8, hjust = -1, vjust = 0.5),\n           ggtheme = theme_pubr()) + \n  theme(legend.position = \"none\",\n        axis.text = element_text(size = 9),\n        axis.title = element_text(size = 12)) +\n  ylim(0, 70)\n\nlibrary(ggplot2)\nggplot(df, aes(coal, city, label = coal)) +\n  geom_segment(aes(x = 0, xend = coal,\n                   y = reorder(city, coal),\n                   yend = reorder(city, coal)),\n               color = \"gray60\") +\n  geom_point(aes(fill = coal), shape = 21, size = 3, color = \"transparent\") +\n  geom_text(size = 3, hjust = -1, vjust = 0.5) +\n  theme_classic() +\n  theme(legend.position = \"none\") +\n  xlim(0, 70)\n\n\n\n\n\n\n\n\n\n\n\n(a) ggpubr绘制\n\n\n\n\n\n\n\n\n\n\n\n(b) ggplot2绘制\n\n\n\n\n\n\n\n图 3.33: 棒棒糖图\n\n\n\n\n\n3.4.3.3 坡度图\n坡度图(slope chart)本质上是多组折现图，能够很好地比较在两个或多个不同时间或条件下某些变量的数据变化关系。该图形同样可以利用ggplot2中的geom_segment()和geom_point()函数来绘制出来。\n下面的代码构造了5个国家2010年与2020年PM2.5年均浓度，然后绘制坡度图比较变化关系，结果如 图 3.34 所示。\n\n# 输入数据\ndf = data.frame(Country = paste0(\"C\", 1:5),\n                Y_2010 = c(60, 73, 46, 53, 32),\n                Y_2020 = c(34, 56, 68, 40, 13))\n\n# 增加绘图坐标\ndf$x0 = rep(0, 5) ; df$x1 = rep(1, 5)\n\n# 绘制图形\nggplot(df) +\n  geom_segment(aes(x = x0, xend = x1,\n                   y = Y_2010, yend = Y_2020),\n               color = \"gray\", size = 1, alpha = 0.5) +\n  geom_vline(xintercept = 0, color = \"lightblue\", size = 1) +\n  geom_vline(xintercept = 1, color = \"orange\", size = 1) +\n  geom_point(aes(x = x0, y = Y_2010, color = Country), size = 6) +\n  geom_point(aes(x = x1, y = Y_2020, color = Country), size = 6) +\n  scale_x_continuous(expand = expansion(mult = 0.2),\n                     breaks = c(0, 1),\n                     labels = c(2010, 2020)) +\n  theme_bw() + labs(x = \"\", y = expression(PM[2.5]))\n\n\n\n\n\n\n\n图 3.34: 坡度图\n\n\n\n\n\n\n\n3.4.3.4 玫瑰图\n玫瑰图(rose chart)本质上是极坐标柱形图，特别是适合于风速风向数据的可视化，圆的半径用于度量风向频率，角度用于度量风向(气象数据中0°为正北风向)。ggplot2可以通过使用极坐标很方便地实现玫瑰图。这里特别推荐使用openair包来绘制风速风向玫瑰图，该包是分析、理解和解释空气污染数据的工具，还集成了英国空气污染物监测数据下载的功能。\n下面的代码演示利用worldmet包下载气象数据，然后利用openair包对风速风向数据进行可视化，结果如 图 3.35 所示。\n\nlibrary(worldmet)\nlibrary(openair)\n# 从NOAA下载和合肥骆岗气象站2021年气象数据\ndfmet = importNOAA(code = \"583210-99999\", year = 2021)\nhead(dfmet)\n\n# A tibble: 6 × 22\n  code         station  date                latitude longitude  elev    ws    wd\n  &lt;fct&gt;        &lt;fct&gt;    &lt;dttm&gt;                 &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 583210-99999 LUOGANG… 2021-01-01 00:00:00     31.8      117.  32.9   1.2  140.\n2 583210-99999 LUOGANG… 2021-01-01 01:00:00     31.8      117.  32.9   3    180 \n3 583210-99999 LUOGANG… 2021-01-01 02:00:00     31.8      117.  32.9   3    180 \n4 583210-99999 LUOGANG… 2021-01-01 03:00:00     31.8      117.  32.9   2.9  176.\n5 583210-99999 LUOGANG… 2021-01-01 04:00:00     31.8      117.  32.9   3    210 \n6 583210-99999 LUOGANG… 2021-01-01 05:00:00     31.8      117.  32.9   3    240 \n# ℹ 14 more variables: air_temp &lt;dbl&gt;, atmos_pres &lt;dbl&gt;, visibility &lt;dbl&gt;,\n#   dew_point &lt;dbl&gt;, RH &lt;dbl&gt;, ceil_hgt &lt;dbl&gt;, cl_1 &lt;dbl&gt;, cl_2 &lt;dbl&gt;,\n#   cl_3 &lt;dbl&gt;, cl &lt;dbl&gt;, cl_1_height &lt;dbl&gt;, cl_2_height &lt;dbl&gt;,\n#   cl_3_height &lt;dbl&gt;, precip_6 &lt;dbl&gt;\n\n# 对风速风向数据进行可视化，按季节分面绘制，布局为4列1行\nwindRose(dfmet, ws = \"ws\", wd = \"wd\", type = \"season\", layout = c(4, 1))\n\n\n\n\n\n\n\n图 3.35: 风速风向玫瑰图\n\n\n\n\n\n从 图 3.35 可以分析合肥市2021年四个季节风速风向的差异。openair能够结合风速风向分析空气污染物浓度变化的规律并进行建模以及开展轨迹分析(trajectory analysis)，是研究空气污染的有力工具。\n\n\n3.4.3.5 雷达图\n雷达图(radar chart)也称为蜘蛛网图，用与比较若干个研究对象的多个数值型变量(属性)，例如比较两个城市的人口、燃油车辆保有数量、燃煤消费量、GDP、PM2.5浓度等指标，观察哪些指标相近，哪些指标差异很大。又如比较几个厂家生产的某类环保设备的若干指标。雷达图也存在明显的缺点，如不能表征太多的研究对象和变量，否则可视化效果很混乱，不易于阅读。\nfmsb包集成了简便绘制雷达图的函数。下面使用ggradar包来演示雷达图的绘制，因为该包是ggplot2的扩展包，绘图语法相同。在绘制雷达图之前，需要将各类变量进行标准化，一般建议将所有观测值按最大最小值缩放到[0, 1]区间。\n安装ggradar包：\n\nremotes::install_github(\"ricardo-bion/ggradar\", dependencies = TRUE)\n\n\nlibrary(ggradar)\nlibrary(scales)\nlibrary(tidyverse)\n\n# 构造绘图数据集：\nset.seed(2022)\ncity = LETTERS[1:26]\ndf = data.frame(\n  人口 = round(rnorm(26, 50, 10)),\n  GDP = round(rnorm(26, 30, 10)),\n  煤炭 = round(rnorm(26, 500, 100)),\n  汽车 = round(rnorm(26, 10, 2)),\n  PM2.5 = round(rnorm(26, 50, 20)))\nrownames(df) = city\n\n# 数据归一化，并选出4个城市作为比较研究的对象\ndfradar = df %&gt;%  \n  as_tibble(rownames = \"city\") %&gt;% \n  mutate_at(vars(-city), scales::rescale) %&gt;%  # 利用scales中rescale()函数执行归一化\n  slice_head(n = 4)  # 选取前4个城市\n\n# 绘制雷达图\nggradar(dfradar, base.size = 6,\n        grid.label.size = 5,\n        axis.label.size = 5,\n        group.point.size = 3,\n        legend.text.size = 10,\n        legend.position = \"right\") \n\n\n\n\n\n\n\n图 3.36: 雷达图\n\n\n\n\n\n\n\n3.4.3.6 词云图\n词云图(word cloud chart)将关键词的大小与其出现的频率成正比，然后通过某种方式(方形、圆形、星星等)排列在一起。词云图可用于探索论文、报告等文章中的哪些关键词出现的频率高，可有助于掌握该文章的重点。 绘制词云图需要提前准备好关键词及其频率的数据。先利用jiebaR包对原始文本内容进行分词，再统计关键词的频数，构造关键词与出现频率的数据集，最后用wordcloud2包绘制词云图。\n以下代码先复制中国生态环境部网站2020年中国环境公报综述部分的内容，保存到cneb2020-rev.txt文件中，保存时选择编码方式为UTF-8，并置于当前目录下的data子目录中。\n\nlibrary(readr) \nlibrary(jiebaR) \nlibrary(jiebaRD) \nlibrary(wordcloud2) \n\nword = read_lines(\"./data/cneb2020-rev.txt\")\nengine = worker(\"mix\")\ndata = segment(word, engine)\nword = jiebaR::freq(data)\nword_s = word[word$freq &gt;= 5, ] \nwordcloud2(word_s, color = \"random-light\")\n\n\n\n\n\n\n\n图 3.37: 词云图\n\n\n\n\n\n\n\n3.4.4 可视化趋势发展变化\n\n3.4.4.1 折线图\n折线图(line chart)是最常用的可视化时序类型数据的方法，可以显示变量在时间上的变化趋势，同图可视化同时间段的其他变量，可以观察变量之间的关系。R中绘制折线图的方法非常多，这里演示利用ggplot2包中的geom_line()函数绘制折线图。\n下面的案例需要从UCI 机器学习数据库网站下载北京PM2.5数据，该数据集包括北京市2010年1月1日至2014年12月31日的PM2.5小时浓度数据以及同时间内的气象数据，缺失值用NA表示。\n\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(ggTimeSeries)\n\n# filepath = \"https://archive.ics.uci.edu/static/public/381/beijing+pm2+5+data.zip\"\n# savepath = \"./data/bj-pm2.5-data.zip\"\n# download.file(filepath, savepath)\n\ndf = read_csv(unzip(\"./data/bj-pm2.5-data.zip\",\"PRSA_data_2010.1.1-2014.12.31.csv\"),\n              col_names = TRUE)\n\n# 截取2012年的数据子集\nstart = ymd_h(\"2012/01/01/0\", tz = \"Asia/Shanghai\")\nend = ymd_h(\"2012/12/31/23\", tz = \"Asia/Shanghai\")\ndf = df %&gt;% \n  dplyr::filter(year == 2012) %&gt;% \n  mutate(datetime = seq(start, end, by = \"hours\")) %&gt;%  # 生成新变量datetime\ndplyr::filter(month == 6)\n\n# 用geom_line绘制PM2.5时序数据\nggplot(df, aes(datetime, pm2.5)) +\n  geom_line(color = \"darkblue\") + \n  theme_bw()\n\n\n\n\n\n\n\n图 3.38: 折线图\n\n\n\n\n\n可以将同时期内的多个时序变量绘制到同一个图，以用于分析这些变量之间的关系。但考虑到变量的量纲差异，一般需要进行对各变量的数据进行归一化处理(根据最大最小值，线性压缩到[0, 1]区间，这可借助scales包中的 rescale()函数实现)。下面的ggplot2代码将pm2.5、TEMP和Iws三个时序变量绘制到同一个图形中, 图 3.29 所示。\n\nlibrary(dplyr)\n\ndfsub = df %&gt;% \n  dplyr::select(datetime, pm2.5, TEMP, Iws) %&gt;% \n  drop_na()\n\n# 对三类数值变量进行归一化处理\ndfnorm = dfsub %&gt;% \n  mutate_at(vars(-datetime), scales::rescale)\ndfnorm$datetime = dfsub$datetime\n\n# 将三列变量汇总成一列变量(宽表转成长表)\ndfnorm = dfnorm %&gt;% \n  pivot_longer(cols = !datetime,\n               names_to = \"vars\",\n               values_to = \"value\")\n# 绘制图形\nggplot(dfnorm, aes(datetime, value, group = vars)) +\n  geom_line(aes(color = vars)) +\n  theme_bw() + \n  labs(x = \"Date time\", y = \"Value\") + \n  scale_x_datetime(date_labels = \"%m-%d\", date_breaks = \"5 days\") +\n  theme(legend.title = element_blank())\n\n\n\n\n\n\n\n图 3.39: 折线图\n\n\n\n\n\n在变量较多、数据量很大的情况下，不建议同图绘制多个变量的时序折线图。此时可以考虑用分面绘制，结果如 图 3.40 所示。\n\nggplot(dfnorm, aes(datetime, value, color = vars)) +\n  geom_line() +\n  facet_grid(vars ~ .) + theme_bw() +\n  labs(x = \"Date time\", y = \"Value\") + \n  scale_x_datetime(date_labels = \"%m-%d\", date_breaks = \"5 days\") +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n图 3.40: 多维时序数据分面折线图\n\n\n\n\n\n\n\n3.4.4.2 面积图\n面积图(area plot)是在折线图基础上，在折线与x轴之间填充颜色或纹理而得到的视觉效果更美观的图形。在绘制多个时序变量时，应设置填充的透明度，避免变量数据之间因遮挡和覆盖而无法观察的情况。\n在网站Our World in Data展示了亚洲、非洲、欧洲、南美洲和北美洲自1850-2010年的二氧化硫排放量时序面积图，在该页面下载原始数据到工作目录，用ggplot2中的geom_area()和geom_line()函数绘制出类似的面积图，结果如 图 3.41 所示。\n\ndf = read_csv(\"./data/so-emissions-by-world-region-in-million-tonnes.csv\",\n              col_names = TRUE)\n\ndf = df %&gt;% dplyr::filter(Entity != \"World\")\ncolnames(df) = c(\"Entity\", \"Code\", \"Year\", \"SO2\")\n\nggplot(df) +\n  geom_area(aes(Year, SO2, group = rev(Entity), fill=Entity),\n            alpha = 0.5, position = \"identity\") +\n  geom_line(aes(Year, SO2, color = Entity), size = 1) +\n  theme_bw() +\n  theme(panel.border = element_blank()) +\n  scale_y_continuous(breaks = c(0, 20, 40, 60, 80),\n                     labels = paste(c(0, 20, 40, 60, 80), \"m tonnes\")) +\n  labs(x = \"\", y = \"\",\n       title = \"Global sulphur dioxide emissions by world region\",\n       subtitle = \"Annual sulphur dioxide emissions in million tonnes\",\n       caption = \"Source: Clio Infra, Klimont. et al (2013).\")\n\n\n\n\n\n\n\n图 3.41: 多维时序数据面积图\n\n\n\n\n\n图 3.41 所示的面积图没有采用堆积(stack)的方式，而是直接显示原始时序数据以便比较。和多维折线图一样，当变量和数据过多时，多维面积图的视觉效果很差，建议分面绘制。\n\n\n3.4.4.3 日历热图\n日历热图(calendar heatmap)可以展示以日(day)为时间单位的时序数据，例如展示大气污染物日均浓度，从而可以观察污染物浓度随日、周、月等时间尺度上的变化规律。绘制日历热图可以借助ggTimeSeries包中的stat_calendar_heatmap()函数：\n\nlibrary(ggplot2)\nlibrary(ggTimeSeries)\nlibrary(lubridate)\nlibrary(RColorBrewer)\n\n# 构造绘图数据\nset.seed(2022) \ndf &lt;- data.frame(\n  date = seq(as.Date(\"2020-01-01\"), as.Date(\"2021-12-31\"), \"days\"),\n  value = round(rnorm(731, 120, 30)))\n\n# 生成新的变量\ndf = df %&gt;% mutate(\n  year = as.integer(year(date)),\n  month = as.integer(month(date)),\n  week = as.integer(week(date)))\n\n# 计算x轴刻度位置并生成相应刻度标签\nmonthlbs = data.frame(\n  month = month.abb,\n  mean = (group_by(df, month) %&gt;% summarise(mean(week)))[ , 2])\ncolnames(monthlbs) = c(\"month\", \"mean\")\n\n# 绘制图形\nggplot(df, aes(date = date, fill = value)) + \n  stat_calendar_heatmap() + \n  scale_fill_gradientn(colours = rev(brewer.pal(11, \"Spectral\"))) + \n  scale_y_continuous(name = NULL,\n                     breaks = seq(7, 1, -1), \n                     labels = c(\"Mon\", \"Tue\", \"Wed\", \n                                \"Thu\", \"Fri\", \"Sat\", \"Sun\")) + \n  scale_x_continuous(name = NULL, \n                     breaks = monthlbs$mean, \n                     labels = monthlbs$month, \n                     expand = c(0,0)) + \n  facet_wrap(~ year, ncol = 1, strip.position = \"right\") + \n  theme(panel.background = element_blank(),\n        panel.border = element_blank(),\n        strip.background = element_blank(),\n        strip.text = element_text(size = 13, face = \"plain\", color = \"black\"),\n        axis.line = element_line(colour = \"black\", size = 0.25),\n        axis.title = element_text(size = 10, face = \"plain\", color = \"black\"),\n        axis.text = element_text(size = 10, face = \"plain\", color = \"black\"))\n\n\n\n\n\n\n\n图 3.42: ggTimeSeries包绘制的时序数据日历热图\n\n\n\n\n\nopenair包中的calendarPlot()函数是基于lattice包levelplot()函数封装而成，是最简单的绘制绘制日历热图的方法。该函数绘制的日历热图风格与ggTimeSeries有所不同：\n\nlibrary(openair)\n# 将R环境改为英文模式，以让日期显示更美观\nSys.setlocale(category  = \"LC_ALL\", locale = \"C\")\n\n[1] \"C\"\n\ncalendarPlot(df, pollutant = \"value\", year = 2021)\n\n\n\n\n\n\n\n图 3.43: openair包绘制的时序数据日历热图\n\n\n\n\n\nggplot2的geom_tile()函数结合分面绘图方法也可以实现日期热图，但需要对时序数据进行加工，生成新的绘图变量。例如生成周天(weekday，周一至周日)映射到x轴，生成月周数(monthweek，16)变量映射到y轴，用geom_tile()函数生成方块，将时序数值变量以颜色填充到方块中，利用geom_text()函数将月日数(monthday，131)变量填充到方块中，再以月份(month)变量分面绘图，最后对图形细节进行调整。\n\n\n\n3.4.5 可视化变量频数分布\n\n3.4.5.1 统计直方图和核密度估计图\n统计直方图(histogram)和前面提到的柱形图(bar chart)很相似，但后者主要用于类别变量的比较，而前者用于对数值变量执行分组(也称分箱，bin)统计并可视化，x轴为数值变量的取值范围，y轴为频数，柱形高度是该落入对应分箱中的频数，柱形宽度是该分箱的取值范围，所有分箱宽度相同。直方图能够显示各组频数的分布情况，可以判断该数值变量的分布类型和偏态。 核密度估计图(kernel density plot)是直方图的变体，使用平滑曲线来拟合频数分布，与直方图相比，可密度图不受分组(分箱)数量多少的影响，能够更清晰地呈现数据分布形状，用户可以将这2种可视化方法结合在一起使用。\nggplot2中的geom_histogram()函数和geom_density()函数分别用于绘制直方图和核密度估计图，前者中重要的参数是binwidth(分箱宽度)和bins(分箱总数)，后者中的主要参数是bw(平滑带宽)和kernel(核函数，默认是”gaussian”)。绘制统计直方图，y轴映射可以选择频数或频率，这需要在aes()函数设置y = ..count..或y = ..density..。\n下面以 图 3.38 中使用的北京市PM2.5时序数据集为例，介绍这两种图形的绘制：\nlibrary(tidyverse)\nlibrary(lubridate)\n\ndf = read_csv(unzip(\"./data/bj-pm2.5-data.zip\",\"PRSA_data_2010.1.1-2014.12.31.csv\"),\n              col_names = TRUE)\n\ndfs = df %&gt;% dplyr::filter(year == 2014)\n\nggplot(dfs, aes(x = pm2.5, y = ..count..)) +\n  geom_histogram(bins = 30, stat = \"bin\", fill = \"orange\", color = \"gray\") +\n  theme_bw()\nggplot(dfs, aes(x = pm2.5, y = ..density..)) +\n  geom_histogram(bins = 30, stat = \"bin\", fill = \"orange\", color = \"gray\") +\n  geom_density(stat = \"density\", color = \"blue\", size = 1) +\n  theme_bw()\n\n\n\n\n\n\n\n\n\n\n\n(a) 频率分布直方图”\n\n\n\n\n\n\n\n\n\n\n\n(b) 叠加核密度估计曲线\n\n\n\n\n\n\n\n图 3.44: 频率分布直方图与核密度估计曲线\n\n\n\n如果要比较2010至2014年连续5年的PM2.5分布差异，可以用geom_density()同图绘制核密度估计曲线图，如 图 3.45 (a) 所示。显然，在曲线条数较多时，该方法并不可取，此时，建议使用ggplot2扩展包ggridges中的geom_density_ridges()来绘制峰峦图(ridge plot)，结果如 图 3.45 (b) 所示。\ndfs = df %&gt;% dplyr::filter(year == 2010:2014)\n\ndfs$year = as.factor(dfs$year) # year变量因子化\n\nggplot(dfs, aes(x = pm2.5, group = year)) +\n  geom_density(aes(color = year), size = 1, alpha = 0.5) +\n  theme_bw() +\n  theme(legend.position = \"right\",\n        legend.title = element_blank())\n\nlibrary(ggridges)\n\nggplot(dfs, aes(x = pm2.5, y = year, fill = year)) +\n  geom_density_ridges(alpha = 0.5) +\n  theme_bw() +\n  theme(legend.position = \"right\",\n        legend.title = element_blank())\n\n\n\n\n\n\n\n\n\n\n\n(a) ggplot2绘制\n\n\n\n\n\n\n\n\n\n\n\n(b) ggridges绘制\n\n\n\n\n\n\n\n图 3.45: 多条核密度估计曲线对比\n\n\n\nggplot2还提供了二维统计直方图和核密度估计图的绘制函数：geom_bin2d()、geom_hex()和stat_density_2d()。通过gplots包中的hist2d()函数计算二维统计直方图数值，再用plot3D包中的hist3D()函数绘制三维直方图。\n\n\n3.4.5.2 箱线图与小提琴图\n箱线图(box plot)用箱体和线段来显示数据集的最大值、最小值、中位数以及上下四分位数，可以呈现连续型数据分布的中心位置和分布范围，还可以用来判断异常值。小提琴图(violin plot)则用对称的分布密度曲线来反映数据的分布范围和形状。可以将箱线图和小提琴图结合起来，弥补箱线图不能识别数据多峰的缺陷，以更全面地呈现数据的分布特征。\nggplot2中的geom_boxplot()和geom_violin()函数分别用于绘制箱线图和小提琴图。下面的代码仍然基于北京市PM2.5数据集，演示了箱线图和小提琴图的绘制，结果如 图 3.46 所示。\n\nggplot(dfs, aes(x = year, y = pm2.5, fill = year)) +\n  geom_violin() +\n  geom_boxplot(width = 0.2) +\n  theme_bw() +\n  labs(x = \"Year\", y = expression(PM[2.5]))\n\n\n\n\n\n\n\n图 3.46: 箱线图与小提琴图的结合\n\n\n\n\n\n对于超过10000个观测的大数据集而言，由于箱线图模糊了太多数据信息，不再建议使用。\n\n\n\n3.4.6 可视化变量相关关系\n\n3.4.6.1 二维散点图\n散点图(scatter plot)是最常见的用于呈现2-3个变量之间关系的可视化方法，可以显示变量之间是否存在数量关联趋势及其类型(线性关系还是非线性关系)，还可以用于观察是否存在异常值。\n对于R内置数据集airquality，可以先用VIM包对Ozone和Solar.R变量的缺失值进行填补(K最近邻填补算法)，然后分别绘制Ozone变量与Solar.R、Temp、Wind变量的散点图：\nlibrary(ggplot2)\n\ndf =  airquality\n\nggplot(df, aes(Solar.R, Ozone)) + \n  geom_point() + \n  geom_smooth() +\n  theme_bw()\nggplot(df, aes(Wind, Ozone)) + \n  geom_point() + \n  geom_smooth() +\n  theme_bw()\nggplot(df, aes(Temp, Ozone)) + \n  geom_point() + \n  geom_smooth() +\n  theme_bw()\n\n\n\n\n\n\n\n\n\n\n\n(a) Ozone与Solar.R\n\n\n\n\n\n\n\n\n\n\n\n(b) Ozone与Wind\n\n\n\n\n\n\n\n\n\n\n\n(c) Ozone与Temp\n\n\n\n\n\n\n\n图 3.47: 散点图\n\n\n\n绘制散点图时，可附加其他变量到点大小(size)、点颜色(color)、点形状(shape)的映射，从而显示更多维变量关系。当数据点过多而出现较多重叠时，可设置透明度(alpha)以反映重叠程度。\n如果需要绘制三个变量的散点图，可采用scatterplot3d包中scatterplot3d()函数、rgl包中的plot3D()函数、或plot3D包中的scatter3D()函数。\n\n\n3.4.6.2 相关矩阵图\n对于多维变量数据集，为了探索各变量之间的相关关系，可以先计算相关系数矩阵，然后再用颜色(颜色深浅表征相关系数大小，两种颜色对应相关方向)、圆形(面积表征相关系数大小，两种颜色对应相关方向)或椭圆(离心率表征相关系数大小，长轴方向表征相关方向)等来可视化相关系数。这样的图称为相关矩阵图(correlation matrix plot)。\nggplot2包中的geom_tile()函数可用来绘制相关矩阵图，但使用corrplot包提供的corrplot()函数和ggcorrplot包提供的ggcorrplot()函数更简单。\nlibrary(corrplot)\ndfm = df[, 1:6]  # 数据接图3-57\nmat = Hmisc::rcorr(as.matrix(dfm))\ncorrplot(mat$r, method = \"circle\", diag = FALSE, tl.pos = \"d\")\ncorrplot(mat$r, method = \"ellipse\", diag = FALSE, tl.pos = \"d\")\ncorrplot(mat$r, method = \"color\", type = {\"lower\"}, order = \"AOE\",\n         addCoef.col = \"black\", diag = FALSE)\n\n\n\n\n\n\n\n\n\n\n\n(a) method = ‘circle’\n\n\n\n\n\n\n\n\n\n\n\n(b) method = ‘ellipse’\n\n\n\n\n\n\n\n\n\n\n\n(c) method = ‘color’\n\n\n\n\n\n\n\n图 3.48: corrplot包绘制的相关矩阵图\n\n\n\n还有很多可视化方法来呈现变量或对象间的关系，如和弦图(chord diagram)、桑基图(sankey diagram)、韦恩图(venn diagram)等，都可以用R来实现，具体参考The R Graph Gallery和R CHARTS上的示例。\n\n\n\n3.4.7 其他可视化类型\n\n3.4.7.1 地理空间数据可视化\n现实世界中的数据几乎都具有地理位置信息(经纬度)。在需要展示地理位置信息时，就需要绘制相关的地图，将有关属性数据与地理位置信息结合在一起可视化。由于地球是一个不规则的椭球，将曲面转化为平面是一个复杂的转换过程，导致地理空间数据比一般数据要复杂，除了位置信息，还要有CRS(coordinate reference system)信息，包括坐标系(地心坐标系和参心坐标系)和投影方法(墨卡托投影、高斯-克吕格投影、阿伯斯投影、兰勃特投影等)等。我国平面地图一般采用高斯-克吕格投影，参心坐标系有北京54坐标系、西安80坐标系和2000国家大地坐标系，世界通用的地心坐标系是WGS-84坐标系。\n地理空间数据包括矢量数据(vector data)和栅格数据(raster data)，对应矢量地图和栅格地图。矢量数据由顶点和路径的几何特征组成，分为点、线和面(多边形)；栅格数据是单元格矩阵(例如像素)，每个单元格包含高度、温度、降雨量、污染物浓度等信息，例如遥感卫星图像等。由于GIS软件很多，标准不统一，导致地理空间数据存储格式多种多样，最常见是矢量地图数据格式是shp(ESRI，在同目录下包含多个同名的其他文件，如.shx，.dbf等)和json(GeoJSON)，遥感影像的格式一般是GeoTIFF类型，扩展名为tiff或tif。\n可视化地理空间数据的R包有很多，包括ggplot2、ggspatial、ggmap、tmap、sf、sp、terra、raster、rasterVis、leaflet等，可绘制静态和动态的地图。\n下面比较一下terra、sf和ggplot2可视化lux.shp地图数据(terra包自带地图数据集)的效果，结果如 图 3.49 所示。\nlibrary(terra)\nlibrary(sf)\nlibrary(ggplot2)\n\nmf = system.file(\"ex/lux.shp\", package = \"terra\")\np_lux = vect(mf)\nd_lux = sf::read_sf(mf)\n# 绘制terra的SpatVector地图对象\nplot(p_lux, \"NAME_1\")\n# 绘制sf地图对象\nplot(d_lux[\"NAME_1\"], key.pos = 4, main = \"\")\n# 基于ggplot2绘制sf地图对象\nggplot(d_lux) +\n  geom_sf(aes(fill = NAME_1)) +\n  theme_bw() +\n  theme(legend.title = element_blank())\n\n\n\n\n\n\n\n\n\n\n\n(a) terra包绘制SpatVector地图对象\n\n\n\n\n\n\n\n\n\n\n\n(b) sf包绘制sf地图对象\n\n\n\n\n\n\n\n\n\n\n\n(c) ggplot2包绘制sf地图对象\n\n\n\n\n\n\n\n图 3.49: 绘制地图\n\n\n\nterra和sf都是基于扩展的plot()函数来绘制地图。ggplot2可以通过geom_map()和geom_sf()函数来绘制地图，其中前者支持sp对象(SpatialPolygonsDataFrame)，后者支持sf(simple feature)对象，最为常用。 tmap可以采用类似ggplot2的语法创建多个图层的地图，ggspatial是ggplot2的扩展，而leaflet包则可以创建动态交互的地图。具体使用可参考相应工具包的帮助信息。地图最重要的应用是结合其他可视化类型，如密度图、气泡图、柱形图等。\n下面的案例从阿里云导入合肥市辖区地区，然后添加合肥市10个空气质量国控监测点及某时点PM2.5浓度值，结果如 图 3.50 所示。\n\nlibrary(ggplot2)\nlibrary(sf)\nlibrary(ggrepel)    # 用于解决文本标签重叠问题\n\n# 从阿里云下载合肥市地图，并转化为sf类型\nmap_hefei = read_sf(\n  \"https://geo.datav.aliyun.com/areas_v3/bound/340100_full.json\"\n  )\n# 选择四个市辖区\nmap_hefei = map_hefei[\n  map_hefei$name %in% c(\"蜀山区\", \"瑶海区\", \"包河区\", \"庐阳区\"), ]\ncp1 = as.data.frame(map_hefei[c(\"name\", \"centroid\")])\n# 将中心坐标一分为二的自定义函数\nfunc_cp &lt;-function(x){data.frame(x1 = x[1], x2 = x[2])}   \ndfcp1 = cbind(\n  name = cp1$name,\n  data.frame(matrix(unlist(lapply(cp1$centroid, func_cp)),\n                    ncol = 2, byrow = TRUE)))\nhf_pm25 = data.frame(\n  stid = paste0(\"X\", 1270:1279, \"A\"),\n  name = c(\"明珠广场\", \"三里街\", \"琥珀山庄\", \"董铺水库\", \"长江中路\",\n           \"庐阳区\", \"瑶海区\", \"包河区\", \"滨湖新区\", \"高新区\"),\n  lon = c(117.1959, 117.3072, 117.2588, 117.1604, 117.25,\n          117.266, 117.336, 117.3027, 117.2776, 117.1318),\n  lat = c(31.7848, 31.8764, 31.8707, 31.9052, 31.8571,\n          31.9436, 31.8586, 31.7964, 31.7385, 31.8403),\n  pm2.5 = c(34, 36, 35, 29, 32, 35, 36, 38, 33, 36))\n\nggplot() +\n  geom_sf(data = map_hefei, fill = \"white\", \n          size = .5, color = \"black\") +\n  geom_text(data = dfcp1, aes(x = X1, y = X2, label = name),\n            check_overlap = T, size = 3, color=\"gray\") +\n  theme_bw() + \n  labs(x=\"经度\", y=\"纬度\") + \n  geom_point(data = hf_pm25, aes(x=lon, y=lat),\n             color = \"green\", size = 3) +\n  geom_text_repel(data = hf_pm25, aes(x = lon, y = lat * 1.0003, label = name),\n            color = \"blue\", size = 3) +\n  geom_text_repel(data = hf_pm25, aes(x = lon, y = lat * 0.9996, label = pm2.5),\n            color = \"black\", size =3)\n\n\n\n\n\n\n\n\n\n图 3.50: 地图与其他数据的集成可视化\n\n\n\n\n\n除此以外，R语言还可以处理高程数据(DEM)和遥感影像数据。rayshader包使用高程数据以及光线跟踪、山坡阴影算法和覆盖的组合来生成效果惊人的2D和3D地图，下面的代码为该包中的示例代码，效果如 图 3.51 所示。\nlibrary(rayshader)\n\n# 下载高程图压缩包并解压，然后以raster包来加载数据\ndownload.file(\"https://tylermw.com/data/dem_01.tif.zip\", \"./data/dem_01.zip\")\ndem_tif = raster::raster(unzip(\"./data/dem_01.zip\", \"dem_01.tif\"))\n\n# 将高程图数据转换为矩阵\nelmat = raster_to_matrix(dem_tif)\n\n# 2D图形绘制\nelmat %&gt;%\n  sphere_shade(texture = \"desert\") %&gt;%  # 添加内置纹理\n  add_water(detect_water(elmat), color = \"desert\") %&gt;%  # 检测并添加水系\n  rayshader::add_shadow(ray_shade(elmat), 0.5) %&gt;%  # 添加阴影\n  plot_map()\n\n# 3D图形绘制\nelmat %&gt;%\n  sphere_shade(texture = \"desert\") %&gt;%\n  add_water(detect_water(elmat), color = \"desert\") %&gt;%\n  rayshader::add_shadow(ray_shade(elmat, zscale = 3), 0.5) %&gt;%\n  rayshader::add_shadow(ambient_shade(elmat), 0) %&gt;%\n  # 设置3D图形参数\n  plot_3d(elmat, zscale = 10, fov = 0, theta = 135, zoom = 0.75, \n          phi = 45, windowsize = c(1000, 800))  \nSys.sleep(0.2)\nrender_snapshot()  # 渲染\n\n\n\n\n\n\n\n\n\n\n\n(a) 2D高程图\n\n\n\n\n\n\n\n\n\n\n\n(b) 3D高程图\n\n\n\n\n\n\n\n图 3.51: 高程图\n\n\n\n特别需要注意的是，地图涉及国家主权，使用时必须严格遵守相关法律法规。\n\n\n3.4.7.2 标量与向量场可视化\n等高线图(contour map)是以二维图形呈现三维变量关系的一种可视化方法，这在标量场(scalar filed)的可视化中经常使用，例如气象预报中的等压线、等温线以及地图中的等高线等，可以探究某个变量与其他相关的两个变量的关系。\nR内置数据集faithfuld包含美国黄石公园Old Faithful喷泉喷发时长(eruptions)、间隔时长(waiting)及概率密度估计(density)数据。以下代码演示了ggplot2基于该数据集绘制等高线图以及颜色填充的等高线图的方法，结果如 图 3.52 所示。\nlibrary(ggplot2)\n\np = ggplot(faithfuld, aes(waiting, eruptions, z = density))\n# 默认等高线图\np + geom_contour()\n# 颜色填充的等高线图\np + geom_contour_filled() \n# 等高线叠加颜色填充的等高线图\np + geom_raster(aes(fill = density)) + \n  geom_contour(colour = \"white\") \n\n\n\n\n\n\n\n\n\n\n\n(a) 默认等高线图\n\n\n\n\n\n\n\n\n\n\n\n(b) 颜色填充的等高线图\n\n\n\n\n\n\n\n\n\n\n\n(c) 等高线叠加颜色填充的等高线图\n\n\n\n\n\n\n\n图 3.52: 等高线图\n\n\n\n风场数据包括风速和风向，流体动力学数据也包括流速和流向，此类向量场(vector field)或流场(flow field)数据的可视化与标量场不同，一般采用带箭头的线段来进行可视化。在可视化前，一般将通过三角函数将向量(d,\\theta)转化为水平和垂直两个方向上的分量(u,v)。ggplot2包中的geom_segment()函数和rasterVis包中的vectorplot()函数以及pracma包中的vectorplot()函数都可以实现向量场数据的可视化。\n下面演示用rasterVis包来可视化流场数据。\nlibrary(raster)\nlibrary(rasterVis)\n\n# 构造两个RasterLayer对象u和v\nu &lt;- v &lt;- raster(xmn=0, xmx=2, ymn=0, ymx=2, ncol=1e3, nrow=1e3)\n# 根据u和v生成具有初始值的RasterLayer对象\nx &lt;- raster::init(u, fun='x')\ny &lt;- raster::init(u, fun='y')\n# 由x和y重新构造u和v\nu &lt;- y * cos(x)\nv &lt;- y * sin(x) \n# 构造RasterStack对象(具有相同空间范围和分辨率的RasterLayer对象的集合)\nfield &lt;- stack(u, v)\nnames(field) &lt;- c('u', 'v')\n# 以u和v合成值为背景的流场图\nvectorplot(field, isField='dXY', narrows=5e2)\n# 以u和v各自值为背景的流场图\nvectorplot(field, isField='dXY', narrows=5e2, region=field)\n\n\n\n\n\n\n\n\n\n\n\n(a) 以u和v合成值为背景的流场图\n\n\n\n\n\n\n\n\n\n\n\n(b) 以u和v各自值为背景的流场图\n\n\n\n\n\n\n\n图 3.53: rasterVis包绘制的风场图\n\n\n\n总而言之，R语言的可视化功能非常强大，而且学习资源丰富，用户可根据需要，选择和学习相关的工具包，快速掌握R语言的数据可视化方法。\n\n练习 3.2 \n从https://archive.ics.uci.edu/dataset/106/water+treatment+plant网址下载Water Treatment Plant数据集，在RStudio中建立一个基于Quarto格式的 数据处理项目，项目目录名为WTP，并在其中创建一个名为data的子目录，将下载的数据压缩包中的文件解压到data目录中。在项目中新建一个名为main.qmd的Quarto文件，其在中对该数据集进行预处理和可视化，主要内容包括：读入数据(见water-treatment.data文件)并添加列名(列名与故障类型等见water-treatment.names文件)，统计摘要分析，对缺失值进行可视化并采取相应处理措施，通过合理的可视化方法展示进水变量和出水变量以及与水处理性能变量之间的变化关系，各个环节应添加必要的描述和结果分析，绘制图形不少于10个，类型不少于5个。注意，数据集中第一列数据为日期数据，其后为38个属性数据，其中问号表示数据缺失。",
    "crumbs": [
      "基础知识",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>环境数据整理与可视化</span>"
    ]
  },
  {
    "objectID": "CH1.html",
    "href": "CH1.html",
    "title": "1  绪论",
    "section": "",
    "text": "1.1 数据和大数据以及基本术语",
    "crumbs": [
      "基础知识",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>绪论</span>"
    ]
  },
  {
    "objectID": "CH1.html#数据和大数据以及基本术语",
    "href": "CH1.html#数据和大数据以及基本术语",
    "title": "1  绪论",
    "section": "",
    "text": "1.1.1 数据与大数据\n数据(Data)是指用来记载或描述客观事物性质、状态以及相互关系的、可识别的抽象物理符号或/及其组合。数据的形式不只是数值，还包括文字、图像、视频、音频、符号、地理位置等，在计算机系统中，不同形式的数据都是以二进制数字0和1的形式储存。与环境保护相关的各类数据统称为环境数据(Environmental Data)，包括污染源数据、污染物排放数据、环境本底与背景数据、环境污染现状数据、环境因子数据、环境工程治理效果数据等，如污染物浓度、排放量、种类、污染程度等。\n数据只有经过处理和加工，才能转化为更有意义和价值的信息(Information)。信息是隐藏在数据背后的规律，需要探索、挖掘才能发现。信息之间产生联系，可以形成更加高级和抽象的知识(Knowledge)。\n大数据的独特之处就是“大”，这表现为数据的数量(观测或样例，即数据表的行)和变量(特征或属性，即数据表的列)非常多，至少几百万行或上千列。从数据存储占用存储设备的空间来看，大数据动辄以GB、TB乃至ZB为单位。大数据的特点可以归纳为5V：数量(Volume)巨多、种类(Variety)繁杂、产生速度(Velocity)飞快、真实性(Veracity)存疑以及价值(Value)密度低。\n随着各种传感器、遥感卫星等自动和高效获取环境数据的硬件设施日益完善，海量的环境数据将越来越多，迫切需要环境保护工作者掌握相应的分析方法和建模工具，将环境大数据转化为更加有价值的信息，用于对环境问题深入的理解和分析以及更好的模拟和预测，增强解决复杂环境科学与工程问题的能力。\n\n\n1.1.2 统计学基本术语\n统计学是一门收集、处理并分析事实与数据的应用科学。随着大数据特别是海量非结构性数据的增长，对数据的收集、存储和分析都发生了根本性的革命，数据科学(Data Science)也应运而生。数据科学使用科学方法、过程、算法和系统从复杂数据中提取或推断知识和见解，将统计学、数学、计算机科学、信息科学和领域知识中的相关技术和理论统一起来，更好地用大数据“理解和分析实际现象”。 数据科学不同于计算机科学和信息科学。图灵奖得主James Nicholas Gray认为数据科学是科学的“第四范式”(实证、理论、计算，现在是数据驱动)，并断言“由于信息技术和数据洪流的影响，科学的一切都在改变”。\n尽管如此，统计学的基本思想和方法仍然是大数据分析和机器学习的基础，对基本统计术语和概念的熟悉有助于更好地学习数据分析与机器学习建模。\n\n1.1.2.1 总体与个体\n总体(Population)是统计所研究的客观事物全体的简称。构成总体的基本单位或个别事物，称为个体(Individual)，也称为总体单位(Population Unit) 。根据个体数是否有限，总体可分为无限总体和有限总体。\n有研究价值的总体需要具备三个基本特征：同质性、大量性和差异性。同质性是指构成总体的所有个体必须具有至少一种共同性质，这是构成总体的客观依据。大量性是指总体必须由足够数量的个体组成，一个或少数个体构成的总体没有统计研究的意义，因为统计研究的目的是揭示大量事物的普遍性规律。差异性是指构成总体的所有个体除共同性质外，在其他性质上必须存在差异。差异性是统计研究的基础和前提，是信息存在的特征和反映，没有差异的总体没有研究的价值。\n\n\n1.1.2.2 标志与指标\n标志是个体各种特征或属性的名称，可分为数量标志和品质标志。数量标志描述个体的数量属性，标志值为数值，属于定量数据；品质标志描述总体单位的品质属性，标志值为文字和符号，属于定性数据。如对某河流的描述：河流级别为小型河流，水质等级为IV类，NH3-N含量为10.21mg/L，TP含量为13mg/L。这里的河流级别和水质等级为品质标志，而NH3-N和TP含量为数量标志。\n指标是表征总体特征或属性的统计术语，分为数量指标和质量指标。数量指标描述总体规模或某类标志的分布特征，例如A市2020年COD排放总量；质量指标描述总体构成或某类标志水平的高低，如A市2020年PM2.5平均浓度。指标值一般是由总体中所有个体的标志值计算得到的，均以数值表示。\n数量指标与质量指标的划分是相对的，在特定场景下可以互相转换。指标和标志随研究总体范围的变化也可以互相转换。\n\n\n1.1.2.3 样本与抽样推断\n在实际工作中，往往是从被研究的总体中抽取部分个体进行研究。这些抽取出来的部分个体构成的集合称为样本(Sample)或子样(Subset)。样本所包含的个体数量n称为样本容量或样本大小(Sample Size)。统计学研究要求样本的抽取方法必须遵循随机性原则，但在环境保护工作及其他一些领域中，有时需要采用非随机性抽样方法。\n从总体中随机抽取样本用以研究总体的方法称为抽样推断(Statistical Inference)或抽样研究。抽样推断基于抽样分布理论。非随机性抽样获得的资料不能用于抽样推断。\n\n\n1.1.2.4 参数与统计量\n由总体资料计算出的统计指标称为参数(Parameter)，用于描述总体特征。由样本资料计算出的统计指标称为统计量(Statistic)，用于描述样本特征。抽样推断就是用样本统计量推断总体参数的过程。\n\n\n1.1.2.5 变异与误差\n变异(Variation)是指总体中各个体的同一标志值之间实际存在的差异，是总体差异性的来源，也是统计研究的基础和前提。误差(Error)是指某个标志值与其真值之差，也指统计量与参数之差。误差包括过失误差(Gross Error，也称粗差)、系统误差(Systematic Error，也称恒定误差或可测误差)和随机误差(Random Error，也称偶然误差或不可测误差)。过失误差可以避免，系统误差可以减少，而随机误差无法消除。",
    "crumbs": [
      "基础知识",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>绪论</span>"
    ]
  },
  {
    "objectID": "CH1.html#环境数据的类型",
    "href": "CH1.html#环境数据的类型",
    "title": "1  绪论",
    "section": "1.2 环境数据的类型",
    "text": "1.2 环境数据的类型\n环境数据是与环境保护有关的所有数据，其分类与数据分类一致。 根据分类的依据不同，数据有不同的分类体系。\n\n1.2.1 根据数据的性质分类\n\n1.2.1.1 定性数据\n定性数据(Qualitative Data)是指对客观事物性质、状态或相互关系进行定性分类的数据，一般以文字或符号表示，也称为类别型数据(Categorical Data)。有的定性数据之间是平等关系，如污染物超标状况之超标和未超标，又如性别之男和女，称为定类数据(Nominal Data)，这类数据的类别具有穷尽性和互斥性，且不具备任何数学特性，是最低级的数据。有的定性数据的类别之间存在特定顺序关系，如毒性分类数据，从无毒到剧毒，毒性逐渐增大，则称为定序数据(Ordinal Data)，这类数据的类别表示强度、程度或等级的不同，具有大于和小于的数学特征。\n在计算机分析中，往往用数值对分类数据进行编码，实现数值化表示，以方便计算机进行处理和分析以及储存。如污染物排放超标状况数据，未超标用0表示，超标用1表示；又如污染物毒性分类数据，无毒、低毒、中毒、高毒和剧毒分别用0、1、2、3和4表示。\n\n\n1.2.1.2 定量数据\n定量数据(Quantitative Data)是指对客观事物性质、状态或相互关系进行定量描述的数据，以具体数字表示，也称为数值型数据(Numeric Data)。定量数据根据度量方法分为定距数据(Interval Data)和定比数据(Ratio Data)。前者如智商、温度等，不仅能将现象或事物划分为不同类别和等级，还可以数字化衡量不同类别或等级的间隔或距离，具有加减运算的数学特征，但不能进行乘除运算。后者如浓度、人口密度、收入、出生率等，能够进行加减乘除运算。有无具有实际意义的零点(绝对零点)，是定距数据和定比数据的唯一区别。\n从信息含量上看，定比数据 &gt; 定距数据 &gt; 定序数据 &gt; 定类数据。\n定量数据还可根据取值的连续性与否分为离散型和连续型。有的定量数据的取值在取值范围内只能取整数，取值个数是有限的，则称为离散型数据，如某省铬浓度超标河流数为8，不可能是8.1或其他小数。有的定量数据的取值在取值范围内可以是任意数值，取值个数是无限的，就称为连续型数据，如湖水中总磷浓度为0.23mg/L；大气中SO2浓度为3.1μg/m3(虽然给出的是一个具体数值，但实际上，在取值范围内，这些数值可能是“任意”数值)。\n定性数据和定量数据之间可以进行转换。例如对定性数据进行清点可以得到离散型定量数据，从离散型定量数据计算得到的相对数，如率、比等，就属于连续型定量数据。同样，连续型定量数据可以根据特定分类界值转化为离散型定量数据和定性数据。\n\n\n\n1.2.2 根据数据的时空维度分类\n\n1.2.2.1 截面数据\n截面数据(Cross-sectional Data)是指在同一时间(时点或时期)对不同客观事物的相同属性或特征进行观察而获得的静态数据。如表1.1所示的同一时间不同城市的空气中PM2.5的浓度。\n截面数据主要用于研究现象在不同客观事物之间的差异。因此，截面数据的显著特点就是离散度高，往往表现为无规律但非纯随机的变化，即所谓的“无法观测的异质性”。\n分析截面数据要特别注意两个问题：一是异方差问题，因为截面数据采集自不同空间的样本，差异可能极大，例如工业区和风景区的空气污染物数据；二是一致性问题，主要包括样本容量是否一致、取样时间是否一致、测量标准或统计口径是否一致等。\n\n\n\n\n表 1.1: 2021年7月1日我国主要城市的PM2.5平均浓度(μg/m3)\n\n\n\n\n\n\n城市\n浓度\n\n\n\n\n北京\n12\n\n\n上海\n17\n\n\n天津\n17\n\n\n重庆\n17\n\n\n合肥\n30\n\n\n…\n…\n\n\n\n\n\n\n\n\n\n\n1.2.2.2 时间序列数据\n时间序列数据(Time-series Data)是指在不同时间上连续观察到的同一客观事物的一个或多个属性或特征的动态数据。时间序列数据用于研究现象随时间变化的趋势或规律。时间序列数据一般要求时间间隔相同或相同时长，但也有时间间隔不规则的。后一种时序数据的处理分析相对比较复杂。时间序列数据最显著的特征就是具有自相关性，可以通过自相关性分析时序数据的模式，进而建立时序预测模型。\n\n\n\n\n表 1.2: 2021年7月合肥市每日PM2.5平均浓度(μg/m3)\n\n\n\n\n\n\n日期\n浓度\n\n\n\n\n2021/7/1\n30\n\n\n2021/7/2\n14\n\n\n2021/7/3\n6\n\n\n2021/7/4\n11\n\n\n2021/7/5\n10\n\n\n…\n…\n\n\n\n\n\n\n\n\n时间序列数据的显著特点是具有日期或日期时间属性，一般包含长期趋势、季节变动、循环变动和不规则波动(包括随机变动和突发性变动)。现实世界中，时间序列数据无处不在，几乎所有的数据都有时间属性，只是时间属性在不同分析角度中的重要性不同而已。统计学上对时间序列的研究已经形成较为系统的分析方法和理论。\n此外，一些不以时间为轴线的序列数据也可以应用时间序列分析的理论和方法，如随着海拔高度的增加而收集到的相应生态环境特征数据序列，又如随着土层深度增加的相关土壤特征数据序列等。表1.2即为时间序列数据的示例，它列出了同一城市在连续时间序列上的PM2.5浓度。\n\n\n1.2.2.3 面板数据\n面板数据(Panel Data)是截面数据与时间序列数据的组合，通常用于分析有关联的不同客观事物在相同时间序列上的特征，既可以对比分析不同客观事物随相同时间序列动态变化的规律和差异(纵向对比分析)，又可以分析同一时间上不同客观事物的差异(横向对比分析)。表1.3即为面板数据，它列出了多个城市在同一时间序列上的PM2.5浓度。\n\n\n\n\n表 1.3: 全国主要城市2021年7月PM2.5平均浓度(μg/m3)\n\n\n\n\n\n\n日期\n北京\n合肥\n\n\n\n\n2021/7/1\n80\n30\n\n\n2021/7/2\n34\n14\n\n\n2021/7/3\n99\n6\n\n\n2021/7/4\n96\n11\n\n\n2021/7/5\n0\n10\n\n\n…\n…\n…\n\n\n\n\n\n\n\n\n当数据附加不同的地理空间信息(如经纬度)时，分析空间位置对其他特征的影响时需要采用空间数据分析方法。空间数据和时间序列数据一样，也具有自相关性，因为很多特征如植被、气候等随空间变化而变化。\n除了以上常见的数据分类以外，还可以将数据分为结构化数据、半结构化数据和非结构化数据。结构化数据是指可以使用关系型数据库表示和存储，可以用二维数据表来呈现。半结构化数据难以通过二维数据表的形式来呈现，一般包含相关标记，用来分隔语义元素及对记录和字段进行分层，数据结构与内容混杂，但有特定的形式或规则，如HTML文档、JSON、XML等形式的数据。非结构化数据是没有固定结构的数据，如各种格式的文档、图片、图像、音频、视频等，此类数据一般直接存储为二进制的数据格式。",
    "crumbs": [
      "基础知识",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>绪论</span>"
    ]
  },
  {
    "objectID": "CH1.html#环境数据的来源",
    "href": "CH1.html#环境数据的来源",
    "title": "1  绪论",
    "section": "1.3 环境数据的来源",
    "text": "1.3 环境数据的来源\n围绕研究的问题收集数据是第一步工作，也是最重要的环节，因为数据的真实性、代表性以及质量等会直接影响后续分析和建模的可靠性。环境数据主要来源于以下渠道：调查、实验、自动在线监测网络以及文献资料。从调查、实验以及自动在线监测网络(气象、水文、水质、大气等自动监测站，也包括航空和航天遥感等)直接获取的数据，称为初级环境数据(Primary Data)；从文献资料，包括从有关政府和非政府组织或机构发布的环境统计公报、环境统计年鉴、环境统计周报/月报/季报/年报以及学术期刊和专业网站获取的环境资料，一般都经过了加工和处理，称为次级环境数据(Secondary Data)。\n随着信息与通信技术(ICT)和物联网(IoT)的发展，包括遥感技术在内的自动在线监测网络日益覆盖更多环境监测的领域。自动在线监测网络在无时无刻不间断地生产环境大数据，同时也存在极大的冗余，还会因仪器故障等因素造成数据缺失或异常。因此，对这类大数据的分析之前需要进行整理或清洗，以保障数据质量。\n调查是研究者根据调查目的，对调查对象不施加任何干预因素，通过对调查对象的客观观察、记录和描述来收集数据。实验是研究者根据实验目的，主动地对实验对象施加特定干预因素，并控制非干预因素的影响，通过对实验效应的客观观测、记录和描述来收集数据。调查为实验提供线索，实验对调查加以验证。调查和实验是科学研究中最重要的手段，全球各地的环境科学与工程人员在源源不断地通过调查和实验生产各种领域的环境数据，可以综合利用这些数据实现在更广的空间范围对同类环境问题进行分析和建模。\n\n1.3.1 自动在线监测网络\n\n1.3.1.1 环境遥感\n遥感是(Remote Sensing)是以电磁波(包括紫外-可见光、红外-微波的范围)为媒介，在高空或外层空间，通过飞机或卫星等运载工具携带的各种传感器(摄影仪、扫描仪、雷达等)获取地物信息，传递给地面接收站后进行处理和分析。环境遥感是利用遥感技术，对生态环境的动态变化进行监测，服务于环境质量预警、评价和预测以及进一步的决策等。遥感是快速、大规模获取环境数据的重要技术手段。遥感技术在环境领域的应用，主要体现在大面积的宏观环境质量和生态监测方面，在大气环境质量、水体环境质量和植被生态监测等方面都有非常广泛的应用。\n环境遥感通过摄影和扫描两种方法获取遥感图像。摄影有黑白全色摄影、黑白红外摄影、天然彩色摄影和彩色红外摄影。彩色红外摄影效果最好，获得的环境污染影像轮廓清晰。扫描主要是多光谱扫描和红外扫描，用于观测河流、湖泊、水库、海洋的水体污染和热污染有较好效果。在红外扫描图像上常能发现污水排入水体后的影响范围和扩散特征。\n我国目前主要的商业用途的遥感卫星有资源系列、环境系列和高分系列。国外知名的遥感卫星有美国Landsat系列、欧洲的Sentinel系列等。越来也多的高光谱分辨率遥感卫星的成功发射和应用，将推动应用遥感技术的各个领域的发展。环境遥感数据可以在有关网站申请下载，高分辨率遥感数据可以通过付费方式获得。\n\n\n1.3.1.2 自动监测网络\n集成先进的传感器技术和通信技术，在不同空间位置建立监测点，组成自动在线监测网络，对大气、水、噪声、土壤等环境进行持续不断地自动监测，并将数据实时传输到数据中心。图1.1显示了合肥市环境空气质量自动监测站的位置。\n\n\n\n\n\n\n\n\n图 1.1: 合肥市环境空气质量自动监测站分布(来源：&lt;www.aqistudy.cn&gt;)\n\n\n\n\n\n我国已经制定并颁布了《地表水自动监测技术规范(试行)》(HJ 915-2017)、《环境空气质量自动监测技术规范》(HJ/T 193-2005)、《近岸海域水质自动监测技术规范》(HJ 731-2014)、《功能区声环境质量自动监测技术规范》(HJ 906-2017)等几十项与自动监测有关的标准、规范和规定。还有很多自动监测或监控的技术标准、规范与规定正在制定之中。\n目前我国城市环境空气质量自动监测网络和地表水水质监测网络基本建成，监测站点数仍在持续增加。相关监测数据通过中国环境监测总站和各地的生态环境监测中心实时发布给社会公众。\n自动监测网络获取的数据因仪器故障、外界干扰等因素可能导致数据缺失和异常。理论上，每分钟都可以获得监测数据，但如此小的时间尺度上的监测数据会存在严重的冗余，因此，我国目前对外发布的自动监测数据一般是小时尺度上的平均值。\n\n\n\n1.3.2 环境调查与实验研究\n\n1.3.2.1 调查研究\n根据调查范围，调查分为普查(也称全面调查)和抽样调查。普查是对总体的全部个体进行调查的方法。普查必须统一调查方法和调查技术要求。由于普查需要耗费极高的人力、物力、财力和时间成本，往往间隔很长时间才开展一次，例如人口普查。抽样调查是从总体中抽取部分个体进行调查的方法。根据是否遵循随机抽样原则，抽样调查分为非随机抽样调查和随机抽样调查。\n\n\n\n\n\n\n\n\n图 1.2: 主要的抽样调查方法\n\n\n\n\n\n非随机抽样调查方法有典型调查和重点调查等。典型调查是根据调查目的，在对总体全面分析的基础上，从中选择具有代表性的若干个体进行调查的方法。典型调查的关键是所选择的个体具有典型性。例如研究某些行业的产排污系数，就需要抽取这个行业中的典型企业进行调查研究。重点调查是根据调查目的，在对总体全面分析的基础上，从中选择少数重点个体进行调查的方法。重点调查抽取的个体，在总体中的数量不多但在与调查研究相关的标志总量中占比极大。\n随机抽样调查的方法主要有简单随机抽样、系统抽样、分层抽样和整群抽样等。随机抽样调查的数据资料对总体具有很好的代表性，可以用来推断总体特征，是获取推断性统计分析数据的重要途径。\n按调查组织形式，调查可分为统计报表制度和专门调查。统计报表制度是根据统计法规建立的定期自基层组织逐级向上报告数据的制度，有统一规定的报表样式、上报内容、计算方法、上报时间和上报程序，是很多行业获取全面数据的重要基础设施。我国自上世纪80年代建立环境统计报表制度，经过近40年的发展完善，逐步成熟，“十三五”期间的环境报表制度包括18个综合年报、10个基层年报、6个综合季报和6个基层季报。专门调查是为了研究某个问题或某种现象而专门组织的调查形式，通常都是一时性的调查。\n随着互联网络特别是移动互联网应用的普及，网络调查日趋流行。通常将通过网站、电话、社交软件等平台开展的调查归为网络调查。网络调查的对象局限于上网群体，其结果需要注意代表性、可信度以及对调查内容理解的准确性问题。\n\n\n1.3.2.2 实验研究\n实验研究不仅要求系统性和周密性，更要求严谨性和针对性，要能够找到现象背后的原因，揭示现象发生或变化的内在规律。实验研究的基本要素包括实验因素、受试对象和实验效应。实验因素可分为物理因素、化学因素、生物因素。受试对象要符合明确的规定而具有一定的同质性。实验效应即实验中所要观测的指标，分为计数和计量两种，这些指标应具有客观性、精确性、特异性和灵敏性。\n实验研究及相应的实验设计必须遵循对照、随机、重复、均衡四个原则。对照原则要求设立基准处理作为对照，为实验因素的处理效应提供比较的基准；随机原则要求将各实验处理随机地分配给受试对象，以减少人为因素的影响；重复原则要求对实验处理设立多个重复，通过计算各重复的平均值来减少非实验因素的干扰。均衡原则要求各处理的非实验因素均衡一致，以准确揭示实验因素的效应规律，这除了要求受试对象具有尽可能高的同质性以外，还要求环境条件要控制在规定的状态。此外，实验研究对样本含量的大小也有具体的要求，以确保试验结果可靠。\n根据实验研究的类型和目的不同，而有对应的实验设计方法。实验设计的重要性在于既要满足实验研究的稳定性和可靠性，又要降低实验研究的成本。主要的实验设计方法有完全随机设计、随机区组设计、不完全随机区组设计、拉丁方设计、格子设计、\\alpha设计、析因设计、正交设计、均匀设计等。",
    "crumbs": [
      "基础知识",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>绪论</span>"
    ]
  },
  {
    "objectID": "CH1.html#环境数据分析的一般步骤",
    "href": "CH1.html#环境数据分析的一般步骤",
    "title": "1  绪论",
    "section": "1.4 环境数据分析的一般步骤",
    "text": "1.4 环境数据分析的一般步骤\n环境数据分析的步骤包括明确目的、收集数据、整理数据、分析数据、可视化及总结与应用。\n\n1.4.1 明确目的\n数据分析任务的第一步就是要明确分析研究的目的。例如对于研究合肥市空气污染的动态规律和研究巢湖的蓝藻水华发生机制两个任务，收集数据的方向和范围完全不同。\n\n\n1.4.2 收集数据\n根据分析研究目的，通过合理的途径收集相关数据，并力求准确、全面和完整。在有些数据不可获得时，应尝试修订分析研究的目的。在获取次级环境资料的时候，要注意来源的权威性，以确保数据的质量。当以调查和实验来获取初级环境资料时，应注意调查方案和实验设计的可行性和科学性。\n\n\n1.4.3 整理数据\n整理数据是在分析数据之前保障数据质量的最后一个环节。所有来源的数据都可能存在缺失值、异常值、空白值、重复值、错误值等情况，如果不进行处理，可能对后续的数据分析产生干扰，甚至不能完成分析。数据整理也称为数据清洗，主要任务就是对数据中的缺失值、异常值、空白值、重复值、错误值等进行处理，提升数据的质量。\n\n\n1.4.4 分析数据\n数据经过整理之后，采用相关的分析方法进行深入的分析。数据分析存在多个相互关联的环节，例如先通过分析特征属性与研究靶标之间的相关性，筛选出相关性高的特征，然后利用筛选出的特征建立研究靶标的模拟和预测模型。常用的分析方法有传统的统计分析方法，如描述性统计方法(统计汇总、平均指标、变异指标、相对指标)、推断性统计分析(置信区间、假设检验、方差分析)、相关分析、聚类分析、主成分分析、回归分析等，还包括大数据挖掘中关联规则分析方法等，以及机器学习和深度学习方法在特征提取、数据降维、模拟和预测等方面的应用。\n\n\n1.4.5 可视化\n可视化可以应用在数据分析的各个阶段，包括原始数据、整理后数据、分析后的结果(包括模型模拟和预测的结果)以及最终的总结阶段。最重要的应属整理和分析结果的可视化。\n可视化的方法有表和图。表主要用于保存数据整理和分析的结果，不是十分直观，在数据较多时，不能让受众快速理解。图是应用最广泛且类型最丰富的可视化形式，直观明了，易于理解。\n\n1.4.5.1 表\n表是指按一定顺序和格式排列数据的表格。表从构造元素上看，由文字、数字、表线组成；从外形上看包括一个标题和一个表体；从内容上看主要包括各种标目和数据，其中标目分为横标目(主语)和纵标目(谓语)。\n为了避免过多线条影响数据的呈现，一般采用三线表形式(如图1.3所示)。\n\n\n\n\n\n\n\n\n图 1.3: 三线表的基本形式\n\n\n\n\n\n复杂的表可以允许有更多的横线和必要的竖线以进行分组。表1.4就是一个增加了一条横线作为分隔线的衍生三线表。\n\n\n\n\n表 1.4: 三线表扩展形式\n\n\n\n\n\n\n\n年度\n \n污染物浓度\n\n\nyear\n \nP1\nP2\nP3\n\n\n\n\n2017\n \n0.05\n0.07\n0.35\n\n\n2018\n \n0.04\n0.08\n0.47\n\n\n\n注：网页格式可能会以斑马色样式显示行数据\n\n\n\n\n\n\n\n\n\n\n列表原则有两点：一是重点突出，简单明了。即一张表只包含一个中心内容，不要包罗万象，应让人一目了然。二是主谓分明，层次清楚。即主谓语的位置不要错乱，标目的排列及分组要层次清晰，符合专业逻辑。\n表包括标题、标目、表线、数字和说明五个部分，其基本要求如下：\n(1)标题。简要概括统计表的基本内容和表中资料所属的时空特征，一般位于表格的上方。\n(2)标目。标目文字应简明，并注明单位；排列要遵循一定顺序；项目齐全时，先局部后整体，即上(左)面列出各组分类，最下(右)列出总计的数据；项目不全时，一般先列出总计的数据，后列出其中部分重要项目的资料。如果行列过多，通常要加上编号。\n(3)表线。表线不宜太多，尽量简化，应采用三线表及三线表的扩展形式，纵线一般只在做必要分隔时才采用。\n(4)数字。表中数字应对准数位，填写整齐。数字为0则填写0；数字缺失或未记录则填“…”或“NA”或其他规定符号；无法计算或没有数字则填“—”或“NULL”或其他规定符号。\n(5)说明。不列入表内，一般放置在表的下方，用于注明表中某些资料的来源或对某些数据的计算方法、计算口径做出说明。通常要在表格中对要说明的标目或数字用“*”标注，要说明的项目比较多时，采用字母或数字标注。\n\n\n1.4.5.2 图\n图是指利用各种图形来表现统计资料的可视化形式，具有简明、直观、形象和感染力强等优点。图从结构上看由文字、数字和图形组成，从外形上看由一个标题和一个图体组成。图一般包含五大元素：图题、图形、图例、单位和脚注(包括数据来源)，如图1.4所示。\n\n\n\n\n\n\n\n\n图 1.4: 图的基本形式\n\n\n\n\n\n图一般根据用途可以分为：\n(1)描述相互独立的资料或离散型数据，宜用直方图等。\n(2)描述总体构成情况，常用圆形图/饼图、百分比直方图等。\n(3)描述连续型数量指标的频数分布，宜用直方图等。\n(4)描述事物在时间上的连续变化，可用线图等。\n(5)描述事物在空间上的分布情况，宜用地图等。\n(6)描述总体分布形态特征，可用密度曲线图、箱线图、小提琴图等。\n创建图的基本要求如下：\n(1)标题。简明扼要地说明图的内容及时空属性，一般位于图的下方。\n(2)坐标。绝大部分图有坐标轴，坐标轴应有标目，标目应注明单位。\n(3)尺度。一般要求等距或有一定规律(如对数尺度)，并标明数值。遇到特殊情况，尺度可用“\\”断开，表示尺度的跳跃。\n(4)图例。同一个图中存在多组或多类时，应用不同颜色或形状区分，并设图例说明。 (5)外观。要求美观，图的长宽比例要协调。\n\n\n\n1.4.6 总结与应用\n撰写总结报告，即对整个数据分析过程进行总结，重点是呈现结果并提出建议，包括将分析结果(如模型)应用于分析、模拟、预测和解决相关问题，供决策参考。一个好的数据分析总结报告，不仅要有明确的结论、建议和解决方案，而且要图文结合、层次明晰，可以让读者一目了然。",
    "crumbs": [
      "基础知识",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>绪论</span>"
    ]
  },
  {
    "objectID": "CH1.html#环境数据分析的软件工具",
    "href": "CH1.html#环境数据分析的软件工具",
    "title": "1  绪论",
    "section": "1.5 环境数据分析的软件工具",
    "text": "1.5 环境数据分析的软件工具\n可用于数据分析的软件皆可用于环境数据分析。传统统计分析与建模软件，如SPSS、SAS、Stata等，均是需要付费使用的商业软件；Excel、WPS表格等办公电子表格软件可以处理一些较小规模数据，对大数据无能为力；Matlab等科学计算软件也可用于数据分析和建模；FineBI、Tableau等企业级数据分析商业软件，具有强大的数据分析与可视化功能，但也是需要付费使用。通常，建议初学者采用免费开源的软件工具，如Python、Julia和R等。Python和R是解释型语言，在交互性上有优势，Julia是编译型语言，在运行效率上有优势。\nR语言由新西兰奥克兰大学的Robert Gentleman和Ross Ihaka等人基于S语言开发，是一个自由、免费、开源的软件，主要用于统计分析和绘图，可运行于Linux、macOS和Windows等操作系统的计算机上，2000年2月发布了1.0.0版本，截至2024年4月的最新版本为4.4.0版。\nR拥有完整的数据分析功能和帮助系统，是一套完善的、简单的、有效的编程语言，具有条件、循环、自定义函数、输入输出等完整的程序设计方法，是数据科学家的必备工具之一，已广泛应用于医学、生态学、地理科学、环境科学、气象学、农学、物理学、数学、生物信息学、数据挖掘、金融分析、机器学习等诸多领域，其核心功能是数据处理、统计分析、可视化和建立模型。\n因此，基于免费开源、完整数据分析的功能以及易于学习和使用的考虑，本书介绍R语言及其在环境数据分析与机器学习中的应用。\n\n1.5.1 R语言下载和安装\n在浏览器访问CRAN网站https://cran.r-project.org/，在网页左侧点击“mirrors”，进入镜像网站页面，在“China”下面的镜像网站中选择其中之一打开，即可进入国内的R镜像网站。然后根据计算机操作系统类型(Linux、macOS、Windows)来点击相应的链接(如图1.5所示)。\n\n\n\n\n\n\n\n\n图 1.5: 不同操作系统的R程序下载链接页面\n\n\n\n\n\n以Windows系统为例，点击“Download R for Windows”，打开如图1.6所示的页面：\n\n\n\n\n\n\n\n\n图 1.6: R for Windows下载页面\n\n\n\n\n\n点击“base”链接，进入R安装程序下载页面(附录图1.7)：\n\n\n\n\n\n\n\n\n图 1.7: R安装程序下载页面\n\n\n\n\n\n点击最上面的链接即可下载最新的R安装程序。如需安装以往版本，在该页面下方找到并点击“Previous releases”，即可找到以往版本的链接，根据版本号再点击相应链接下载即可。\nR安装程序下载完成后，双击运行，根据提示即可顺利完成安装。Windows系统安装时建议修改默认安装路径，将R安装到非系统盘，如“D:\\R”。其他操作系统安装R参考安装帮助。\n安装完成后，桌面就会有R程序的快捷图标，双击就可以打开R的图形操作界面，即RGUI，但其功能过于简单，为了更好地使用R语言，需要安装一个功能更强大的R集成开发环境(IDE)——RSutdio。\n\n\n1.5.2 RStudio的下载与安装\n在浏览器访问Posit公司网页https://posit.co/download/rstudio-desktop/，下滑页面，找到如图1.8所示的内容，根据计算机操作系统类型选择下载相应的RStudio Desktop版本安装程序。\n\n\n\n\n\n\n\n\n图 1.8: 不同操作系统的RStudio版本下载链接页面\n\n\n\n\n\n下载后双击启动安装程序，按提示操作即可完成RStudio的安装。Windows操作系统建议将默认安装目录改到非系统盘，如“D:\\Rstudio”。其他系统安装RStudio或安装RStudio Sever版本应先阅读安装帮助。\n安装完成后，在开始菜单栏中找到RStudio，点击右键，在弹出的菜单中点击“创建快捷方式”，就可以在桌面上创建RStudio快捷图标。双击快捷图标，即可进入RStudio的操作界面(如图1.9)。建议初学者先花20分钟时间来了解操作界面，包括菜单、工具栏、各种操作窗口等。\n\n\n\n\n\n\n\n\n图 1.9: RStudio操作界面\n\n\n\n\n\n“File”菜单创建和保存各种文件和项目，以及导入各种类型的数据。控制台即“Console”窗口以命令行操作的方式来执行R的简单和较少的代码，大量复杂的代码应该在R Scripts文件(保存为*.R，保存R代码且可以创建注释，注释是以“#”开头的单行文字)，如果有很多的文字论述，建议创建R Notebook(保存为*.Rmd，可输出为html文件)、R Markdown文件(同样保存为*.Rmd，可输出为html、pdf、doc文件，并拥有各种展示功能)或R Quarto文件(保存为*.qmd，是R Markdown的下一代版本，支持更多编程语言)，将代码和文章融合在一起，以便于重现代码执行结果和分享。\n“Edit”菜单用于编辑的各种操作。\n“Code”菜单是关于代码的有关操作。\n“View”菜单主要用于视图操作。\n“Plots”菜单用于绘图方面的一些操作。\n“Session”菜单用于会话(R的工作环境和工作空间)的操作。\n“Build”菜单主要应用R的项目管理。\n“Debug”菜单用于代码或程序调试操作，以便找到代码和程序出错的地方。\n“Profile”菜单用于分析代码和程序，以提高代码运行效率。\n“Tools”菜单用于工具包安装和更新、R项目的版本控制、全局环境配置、内存管理、启动终端及Shell程序等。\n“Help”菜单是最好的入门“老师”，关于R和RStudio使用的问题，基本上都可以在这里找到答案，其中的“Cheat sheets”链接到RStudio和一些工具包的快捷学习资料。\nRstudio的操作界面除了菜单和工具栏以外，就是下面的四个子窗口：\n(1)左上工作区为程序代码撰写区，并有相应的代码编辑和运行的工具图标。启动RStudio后可能看不到该区域，二是被右下区域完全占据。当通过“File菜单”创建一个代码文件后，即可显示此区域。此区域输入的代码，可以通过点击“Run”图标执行光标所在的代码行，也可以键入Ctrl+EnterCtrl+Enter组合键实现同等效果，运行的代码、结果以及警告、错误等信息会在右上的“Console”窗口中显示。\n(2)左下工作区包括Environment(查看和管理当前工作空间中的所有对象包括变量)、History(查看和管理在 Console区内代码执行的历史记录)、Connections(用来连接外部数据库)以及Tutorial(执行基于learnr工具包的R、Rstudio和一些重要工具包的使用教程，运行信息会显示在右上的Jobs标签的窗口中)。\n(3)右上工作区包括Console(控制台，类似RGUI工作区，以命令行方式输入和运行代码并查看运行结果)、Terminal(打开操作系统的终端窗口，以命令行防治执行操作系统的操作)和Jobs(显示某些操作Tutorial执行的结果)。\n(4)右下工作区包括Files(文件夹及文件显示区，可通过右上角“…”图标来修改文件夹)、Plots(绘图显示区)、Packages(工具包管理区，前面显示的表示已经加载到当前工作空间)、Help(帮助系统，执行有关帮助操作的结果也会在这里显示)和Viewer(用来显示本地网页文件)。 在代码文件编辑窗口和Console中输入代码时，按 Tab 键可以帮助补全代码和提供有关函数的提示信息。Ctrl+LCtrl+L组合键用于清除Console窗\nRstudio软件具有非常强大且好用的功能，建议花一些时间去官网或其他网站学习Rstudio的使用，特别是掌握一些快捷键的使用，如Alt+-Alt+-用于输入R的左赋值符号&lt;-，Ctrl+Shift+MCtrl+Shift+M用于输入管道操作符%&gt;%或|&gt;，Ctrl+Alt+ICtrl+Alt+I用于在R Markdown文件中插入一个代码块，Ctrl+Shift+CCtrl+Shift+C用于将选中的代码行转换为注释行等。\n此外，建议安装Rtools软件工具(同样在CRAN网站下载)，以便安装需要编译的包(package)。\n\n练习 1.1 下载并安装R语言。Windows系统安装到“D:\\R\\”目录。\n\n\n练习 1.2 下载安装R Studio软件。Windows系统安装到“D:\\RStudio\\”目录。\n\n\n练习 1.3 下载并安装Rtools软件。按默认目录安装。",
    "crumbs": [
      "基础知识",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>绪论</span>"
    ]
  },
  {
    "objectID": "CH2.html",
    "href": "CH2.html",
    "title": "2  R语言编程基础",
    "section": "",
    "text": "2.1 帮助系统和工具包管理",
    "crumbs": [
      "基础知识",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>R语言编程基础</span>"
    ]
  },
  {
    "objectID": "CH2.html#帮助系统和工具包管理",
    "href": "CH2.html#帮助系统和工具包管理",
    "title": "2  R语言编程基础",
    "section": "",
    "text": "2.1.1 帮助系统\n\n2.1.1.1 帮助菜单\n在RStudio的“Help”菜单中，点击“R Help”菜单项，会在工作界面右下区的Help标签内打开基于HTML格式的帮助系统页面，内容包括： R的学习资源、RStudio的学习资源、R的学习手册、包索引和关键字搜索、其他资料。利用以上资源可以系统地学习R和RStudio的使用。\n\n\n\n\n\n\n\n\n图 2.1: R和RStudio的帮助页面\n\n\n\n\n\n\n\n2.1.1.2 帮助命令\n在R中，用户可以在控制台中快捷地获取有关函数或R包的帮助信息。\n?包名::函数名：精确查询R包中函数的帮助信息，如?ggplot2::qplot执行。如果包已经加载，则直接执行?函数名，如?qplot。\n??关键词：通过关键词进行模糊搜索。等同于help.search(‘关键词’)，注意要加英文引号(单引号和双引号均可)。如??mean。\nhelp(package = 包名)：打开R包的帮助页面，如help(package = mlr3)。\n\n\n2.1.1.3 帮助图形界面\n在RStudio工作界面中Help标签区，有一个关键字搜索的输入框(图 2.1)，也可通过单击“Help”菜单中“Search R Help”或使用组合键Ctrl+Alt+F1Ctrl+Alt+F1快速定位至此，输入关键字来搜索相关的帮助信息。注意组合键可能与其他软件的冲突。\n\n练习 2.1 (1)打开R分别查询函数plot()、summary()、mean()、var()、sd()的帮助信息。\n(2)查询stats包的帮助信息。\n\n\n\n\n2.1.2 R包管理\n众多工具包为R提供了很多专业功能的扩展。大多数R包集成了封装好的函数、数据和示例。目前，R已经拥有2万多个工具包(其中一些已经停止更新，甚至废弃)，主要存储在CRAN网站，该网站上的R包都经过了严格的审查。另外，Bioconductor网站提供了生物信息学相关的R包，github等网站上也有一些未经官方审查的R包。\nR安装后，内置的base、methods、datasets、utils、grDevices、graphics和stats等7个基础工具包随R的启动而自动加载，能够实现基本的数据处理、可视化和统计建模功能。但R的很多专业功能都是由第三方R包提供的，因此,必须掌握R包的查询、安装、更新、卸载和删除等操作。\n\n2.1.2.1 查询R包\nCRAN网站提供了Task Views来归类众多的工具包，用户可从中查询相关的R包。例如有关空间数据分析的R包，可在Spatial和SpatioTemporal中查询。有关生态环境数据分析的R包，可在Environmetrics中查询。有关高性能计算和大数据处理的R包，可在HighPerformanceComputing中查询。有关机器学习的R包可在MachineLearning中查询。R包packagefinder提供了从R中控制台直接搜索CRAN网站上R包的功能。R包ctv提供了在本地安装任务分类视图和查询的功能。\n查询本地已经安装的R包，可使用installed.packages()函数，或者在RStudio工作界面点击“Packages”标签。查看当前工作空间已经加载的R包，可在控制台输入(.packages())。\n输入.packages(all.available = TRUE) 可列出默认工具包安装目录中所有R包的名称列表。\n\n\n2.1.2.2 安装和更新R包\n从CRAN网站安装R包使用函数install.packages()。该函数在安装R包时会自动分析并安装其依赖项(即其他R包)。例如安装tidyverse套件：\n\ninstall.packages(\"tidyverse\")\n\n该函数包含很多参数，重要的参数包括lib和repos，前者用于指定R包的安装目录(不指定即安装到默认的目录中)，后者用于指定包的来源。要查询本地安装包的目录，可以用.libPaths()函数。\n从github网站下载安装R包，需要先安装remotes包或devtools包，然后通过二者提供的install_github()函数。除此以外，这两个包也提供了多个从其他途径下载和安装R包的函数，如install_bioc()函数可从Bioconductor网站下载安装R包。 例如，从https://github.com/mlr-org下载安装mlr3包：\n\nremotes::install_github(\"mlr-org/mlr3\")  \n\n或\n\ndevtools::install_github(\"mlr-org/mlr3\") \n\n在包名后用::连接函数名的做法，是一种无需加载R包即可调用其中函数的方法。这在仅使用某个R包中的一个或少数函数时，可以避免将整个包都加载到工作空间，以减少资源占用。同时，这种调用函数的方法也能够避免工作空间中同名函数的冲突。\n可以将R包的安装包下载到本地进行安装，在RStudio中，点击“Tools”菜单中的“Install Packages…”，在弹出的对话框中，将“Install from”改为“Package Achive File”，然后点击“Browse”按钮找到已经下载到本地的R包压缩包，然后点击“Install”按钮即可。在安装Windows的电脑上，安装未编译的R包需要先安装Rtools软件，以提供本机编译功能。\n更新已安装到本地的R包的函数为update.packages()。该函数会将本地安装的包与CRAN仓库中的包进行版本比较，如果有新版本，则提示更新。在RStudio中更新所有本地安装的包，通过点击“Tools”菜单中的“Check for Package Updates…”即可。\n\n\n2.1.2.3 加载、卸载和删除R包\n加载R包使用library()函数或require()函数。二者都会在加载前检查是否已经安装了指定的包，如果没有，前者会报错，后者也会报错并返回逻辑值FALSE；如果有，则二者都执行加载，但后者还返回逻辑值TRUE。如加载ggplot2包：\n\nlibrary(ggplot2)\n\n或\n\nrequire(ggplot2)\n\n卸载R包是将指定的R包从当前工作空间中清除，而不是从磁盘中删除。卸载R包的方法是使用detach()函数。如卸载当前工作空间中的ggplot2包：\n\ndetach(package:ggplot2) \n\n注意package:前缀必须加上。该函数还可用于卸载当前工作空间中通过attach()函数加载的数据框对象。\n需要删除已经安装的R包，使用remove.packages()函数。\n\n练习 2.2 (1)在RStudio控制台用install.packages()函数完成tidyverse套件和remotes包的安装。\n(2)在RStudio控制台用remotes包中的install_github()函数完成mlr3包和mlr3verse套件的安装。\n(3)在RStudio控制台输入?airquality，查询数据集airquality的帮助信息，了解该数据集。\n(4)在RStudio控制台用library()函数加载ggplot2包，然后输入如下代码并观察结果：\n\nggplot(airquality, \n       aes(Solar.R, Ozone, color = Month)) + \n  geom_point()",
    "crumbs": [
      "基础知识",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>R语言编程基础</span>"
    ]
  },
  {
    "objectID": "CH2.html#基本语法",
    "href": "CH2.html#基本语法",
    "title": "2  R语言编程基础",
    "section": "2.2 基本语法",
    "text": "2.2 基本语法\n\n2.2.1 变量命名\nR语言中有效的变量名(也包括对象名)称由英文字母、数字以及点号.或下划线_组成，英文字母区分大小写，不能包含空格。变量名称只能以字母或点号.开头，以点号.开头时，后面只能直接跟英文字母和下划线_。变量名中不可有空格。如a、.a、.a1、a1、a.1、A_1、O3、o3_8h等都是合法R变量名。而a 1、1a、_a1、.1a等都是无效变量名称。变量的命名要能够表明变量意义，建议变量名只采用小写字母、数字和下划线_，尽可能不采用点号.。\n经常使用的常量建议以大写字母表示。无论是变量还是常量的命名，不应与R语言的保留关键字(如if、ifelse、while等)及常用函数名相同。\n此外，R中还有几个关键字具有特殊含义，有特殊用途，要特别注意。NA一般用于缺失值(占位)，NULL表示空值(不占位)，NaN表示不是一个数(例如0/0的结果)，Inf和-Inf分别表示正无穷大(1/0)和负无穷大(-1/0)。pi是R规定的一个系统常量，即圆周率的值。LETTERS和letters分别存储了26个大写和小写英文字母。month.name和month.abb分别存储了十二个月份的英文名和缩写。\n\n\n2.2.2 运算符号\n\n2.2.2.1 赋值符号\n左赋值：&lt;-(可用组合键 Alt  +  - 输入)或= 以及&lt;&lt;-，都是将右边的值赋给左边的变量。大多数情况下，=可以替代&lt;-，，例如o3 &lt;- 43或o3 = 43。但=多用于传值，而非赋值。此外，两者的作用域存在差异，例如：\n\nmean(x = 1:10)\n\n[1] 5.5\n\nx\n\nError in eval(expr, envir, enclos): object 'x' not found\n\nmean(x &lt;- 1:10)\n\n[1] 5.5\n\nx\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\n因此，在函数的参数值传递中都建议使用=，以避免产生更多对象而占用内存。\n&lt;&lt;-用于将变量写入上一层环境中。例如将自定义函数中的局部变量写入上一层变量中：\n\nmyplus &lt;- function(x){\n  a &lt;&lt;- 1 + x\n  b &lt;- 2 + x\n  x &lt;- x * 2        \n}\nx &lt;- 1\nmyplus(x)\nx\n\n[1] 1\n\na\n\n[1] 2\n\nb\n\nError in eval(expr, envir, enclos): object 'b' not found\n\n\n右赋值：-&gt;和-&gt;&gt;，即将左边的值赋给右边的变量，如43 -&gt; o3。-&gt;&gt;作用与&lt;&lt;-相同。\n赋值符号左右都应添加一个空格。有时为了对齐赋值符号，可以在赋值符号左边添加多个空格。\n\n\n2.2.2.2 数学运算符号\n+、-、*、/、%/%、%%、^分别为加法、减法、乘法、除法、整除、整除求余和乘方运算。混合运算可用( )来确定运算优先级。\n属于双目运算符(操作2个对象)的，都需要在其左右各添加一个空格，但^和所有括号(包括小括号、中括号和大括号)不需添加空格。所有单目运算符(操作1个对象)都不应添加空格，如作为正号和负号的+和-。\n\n3^2 + (2 - 1 * 4) / (-2) + 4 / 2 + 5 %% 2 - 5 %/% 2\n\n[1] 11\n\n\n\n\n2.2.2.3 关系运算符号\n&gt;、&gt;=、&lt;、&lt;=、==、!=分别表示大于、大于等于、小于、小于等于、等等于和不等于的关系运算。其返回值为逻辑值，即TRUE和FALSE，分别表示真或假。对于向量，逐元素(element-wise)进行比较，因此要求向量长度(即元素个数)相等。\n\nv &lt;- c(2,4,6,9)\nt &lt;- c(1,4,7,9)\nv &gt; t\n\n[1]  TRUE FALSE FALSE FALSE\n\n\n\n\n2.2.2.4 逻辑运算符号\n&表示逐元素逻辑与运算，同为TRUE返回TRUE，否则都返回FALSE。。\n|分别逐元素逻辑或运算，同为FALSE返回FALSE，否则都返回TRUE。\n!表示逐元素执行逻辑非运算，即返回与每个元素相反的逻辑值，元素值为TRUE返回FALSE，元素值为FALSE返回TRUE。\n显然，逻辑非运算符是单目运算符，而逻辑与和逻辑或运算符都是双目运算符。 关系运算和逻辑运算常常用于数据的条件过滤。\n\na &lt;-c(TRUE, FALSE, FALSE, TRUE)\nb &lt;- c(FALSE, TRUE, FALSE, TRUE)\na & b\n\n[1] FALSE FALSE FALSE  TRUE\n\na | b\n\n[1]  TRUE  TRUE FALSE  TRUE\n\n!a\n\n[1] FALSE  TRUE  TRUE FALSE\n\n\n注意，逻辑值TRUE和FALSE可以作为整数1和0参与数学运算。如果将数值转换为逻辑值，0为FALSE，其他所有实数都是TRUE。TRUE可缩写为T，FALSE可缩写为F。\n统计airquality数据集中Ozone变量值大于等于100的数目：\n\nsum(airquality$Ozone &gt;= 100, na.rm = TRUE)\n\n[1] 7\n\n\n\n\n2.2.2.5 其他运算符号\n:用于创建一系列间隔为1的等差数列向量，其左右不应留空格。\n\n-1:5\n\n[1] -1  0  1  2  3  4  5\n\n-1.5:2\n\n[1] -1.5 -0.5  0.5  1.5\n\n\n%in%用于判断左边元素是否包含在右边的向量中，返回逻辑值，如包含。返回TRUE，否则返回 FALSE。\n\na = c(\"A\", 2, \"C\")\nb = c(\"C\", \"D\", 1, 2)\na %in% b\n\n[1] FALSE  TRUE  TRUE\n\n\n此外，R自4.1版本开始提供了原生的管道符号|&gt;，和magrittr包提供的%&gt;%符号功能相同，即将左侧的输出结果传递给右侧命令，实现清晰的流程化数据操作，但原生管道符号占用资源更少。\n\n\n\n2.2.3 数据类型和数据结构\n\n2.2.3.1 数据类型\nR的数据类型有数值型(numeric)、整数型(integer)、复数型(complex)、字符型(character)、逻辑型(logical)、因子型(factor)、日期和日期时间型(date和datetime)、时间序列型(ts)等。\n\na = 2\nb = 1.23e3\nc = 2L;\nd = c(TRUE, FALSE, TRUE, TRUE)\ne = \"环境数据分析与机器学习\"\nf = factor(c(\"A\", \"B\", \"C\", \"B\", \"C\", \"A\"), \nlevels = c(\"A\", \"B\", \"C\"), labels = c(1, 2, 3))\ng = as.Date(\"2022/02/28\")\nh = as.POSIXct(\"2022-02-28 12:20:20\")\nk = as.POSIXlt(\"2022-02-28 12:20:20\")\nm = ts(1:10, frequency = 4, start=c(2020, 2))\n\na和b为数值型，c为整数型，d为逻辑型，e为字符型，f为因子型，g为日期型，h和k为日期时间型，m为时间序列型。在R中，一切皆为对象，这些数据类型都归属于某一类对象。class()和typeof()函数分别用于查看对象的类名和存储类型，str()函数则以紧凑的格式显示任何对象的结构。\n\n练习 2.3 \n创建上述变量，并分别用class()、typeof()和str()三个函数查看这些变量的类名、存储类型以及对象结构。\n\nPOSIXct和POSIXlt类型的日期时间数据的区别在于数据结构不同。前者以向量形式存储自1970年1月1日0:00:00(UTC时区，比北京时间早8小时)以来的秒数，后者则是以列表形式存储日期时间的各个要素)。可以在创建后用解除类属性的函数unclass()查看区别。\n数值型、整数型、字符型和逻辑型是基础数据类型，复数型、因子型、日期和日期时间型、时间序列型都是基础类型衍生出来的复合数据类型。\nR提供了is.~和as.~系列函数(~为numeric、integer、complex、character、logical、factor、ts等)，前者用来判断某个变量是否为某数据类型，后者将其他类型的数据强制转换为某类型数据，如果不能强制转换，则会给出警示信息。\n例如，判断并统计airquality数据集中Ozone变量中缺失值的数目：\n\nsum(is.na(airquality$Ozone))\n\n[1] 37\n\n\n\n\n2.2.3.2 数据结构\n这里的数据结构是指容纳数据的容器。R的基本数据结构有向量(vector)、矩阵(matrix)、数组(array)、列表(list)、数据框(data.frame)。\n向量和矩阵是数组的特例，即一维和二维数组。数组是容纳同一类型数据的容器，以数值型(包括整数型)最为常见。矩阵具有独特的数学运算，尤其是在机器学习建模中非常重要。列表是最具包容性的数据结构，可以容纳任何不同类型和不同维度的数据，非常灵活，R中很多对象的底层结构都是列表。数据框是矩阵与列表的融合，既具有矩阵的形式(矩形表格)，又能容纳不同类型的数据，但要求每一列数据为同一类型，不同列数据类型可以不同，同时要求每列数据的行数相同。数据框是机器学习和深度学习中非常重要的数据结构，绝大多数的机器学习和深度学习算法都可以接受数据框格式学习数据集。数据框的列对应不同变量，行对应一个观测或样例。\n\n\n\n\n\n\n\n\n图 2.2: R的数据结构\n\n\n\n\n\n(1)创建向量\n\na = c(1, 4, 5.5, 3:9) \nb = 1:10\nc = seq(1, 3, by = 0.5)\nd = seq(1, 3, length.out = 4)\ne = rep(c(1, 3, 5), each = 2)\nf = rep(c(1, 3, 5), times = 2)\n\nc()是连接(concatenate)函数，用于自由地将各种元素组合在一起，形成向量或列表。如果是创建向量，同时输入不同类型的数据，会强制转换为同一类型的数据。\n\n练习 2.4 \n在RStudio的控制台输入以上代码，然后分别输入各变量(在控制台输入变量名，然后回车)，查看结果，思考并理解以上创建向量方法之间的差异。\n\n向量的长度(即元素个数)可用length()函数获取。names()函数可以获取或者为向量中的元素提供名字(类似标签)。\n(2)创建矩阵\n\na = matrix(1:12, nrow = 3, ncol = 4, byrow = TRUE)\nb = matrix(1:12, nrow = 3, ncol = 4, byrow = FALSE)\nc = matrix(1, nrow = 3, ncol =4)\n\n\n练习 2.5 在RStudio的控制台输入以上代码，然后分别输入各变量，查看结果，思考并理解以上创建矩阵方法之间的差异。\n\n矩阵同样可以用length()获取长度，但通常用dim()获取矩阵的维数，用nrow()和ncol()分别获取矩阵的行数和列数，但不能用于向量。NROW()和NCOL()可以用于向量，这两个函数将向量视为一列的矩阵。矩阵可以有行名和列名，dimnames()可以同时查看和设置行名与列名， rownames()和colnames()分别用于查看和设置行名和列名。\n向量和矩阵可以进行逐元素的各种数学运算(加、减、乘、除、整除、整除求余、乘方，运算符号见 小节 2.2.2.1 )以及求总和sum()、求连乘积prod()以及累积求和cumsum()和累积乘积cumprod()。t()函数用于矩阵的转置。\n矩阵拥有专有的运算函数，如矩阵乘积%*%，内积运算corssprod()、外积运算%o%或outer()、Kronecker积%x%、特征值和特征向量求解eigen()、求逆和解矩阵方程solve()、矩阵行列式det()、QR分解并返回秩qr()、奇异值分解svd()、Cholesky因子分解chol()等，具体可参阅线性代数知识和R语言科学计算方面的书籍。\n\n练习 2.6 分别创建一个数值型向量和矩阵，然后分别执行向量和矩阵与标量的各种运算以及向量和矩阵与自身的各种数学运算，查看结果，思考并理解以上向量与矩阵的逐元素运算。\n\n不等长的向量之间以及矩阵与向量之间可以通过自动补齐的方式进行数学运算，但会显示警示信息。只有维数相同的矩阵与矩阵之间才可以进行数学运算。\n三维及三维以上的称为数组。图片数据的储存一般采用三维数组(行×列×页)形式来分别保存每一个像素点的R(红色)、G(绿色)、B(蓝色)的值。超过三维的数组在数据处理分析与建模中应用较少。\n(3)创建列表\n\na = 1:3\nb = c(\"Tom\", \"Jack\", \"Amy\", \"Kate\")\nc = list(a, b)\nd = list(A = a, B = b)\ne = c(A = 10, B = list(\"Tom\",\"Jack\")) \nf = c(A = 10, B = c(\"Tom\", \"Jack\"))  # 避免使用这种方式，其结果是字符向量\n\n建议尽量使用list()函数创建列表。可以用names()查看和设置列表中每一个项目的名称。\n\n练习 2.7 分别创建不同的长度的一个数值型向量、一个字符型向量和一个数值型矩阵，然后用list函数将三个向量合并组装成一个列表。\n\n(4)创建数据框\n\na = LETTERS[1:10]\nb = c(2, 4, 7, 9, 11, 15:19)\ndf = data.frame(type = a, mass = b)\nstr(df)\n\n'data.frame':   10 obs. of  2 variables:\n $ type: chr  \"A\" \"B\" \"C\" \"D\" ...\n $ mass: num  2 4 7 9 11 15 16 17 18 19\n\n\n\n练习 2.8 输入以上代码创建数据框df，然后用rownames()和colnames()查看和设置df的行名和列名，用str()查看df结构，用dim()查看df的维度。\n\n同样，R也提供了相应的is.~和as.~系列函数(~为vector、matrix、array、list、data.frame等)，前者用于判断对象是否属于该数据结构对象，后者用于强制转换数据结构，转换不成功会有警示信息。\ntidyverse套件中的tibble包提供了增强版的数据框对象：tibble。data.table包则提供了高效处理大数据的超强版数据框对象：data.table，可以读写和处理几十GB级的数据集。如果需要经常需要处理大数据，建议学习和掌握data.table包的使用。内存无法加载的超大数据集，建议使用工具包disk.frame，该包提供了大型数据框对象disk.frame，将数据存储在磁盘上，以处理无法加载至内存的大规模数据。\n\n\n2.2.3.3 访问数据和获取子集\n访问和获取向量、矩阵、数据框以及列表的元素和子集的方法有很多种，通用的方法是利用下标、元素对应的标签(名字)、列名和行名(矩阵和数据框)，以及利用subset()函数。\n\na = 1:12\nb = matrix(a, nrow = 3, ncol = 4)\nc = as.data.frame(b, row.names = letters[1:3], col.names = LETTERS[4:6])\nd = as.list(c)\n\n获取向量子集或元素：\n\na[1]; a[5:7]; a[5:2]; a[-3]\n\n[1] 1\n\n\n[1] 5 6 7\n\n\n[1] 5 4 3 2\n\n\n [1]  1  2  4  5  6  7  8  9 10 11 12\n\n\n获取矩阵子集或元素：\n\nb[, 1]; b[2, ]; b[1:3, 2:3]; b[-c(2:3), ]\n\n[1] 1 2 3\n\n\n[1]  2  5  8 11\n\n\n     [,1] [,2]\n[1,]    4    7\n[2,]    5    8\n[3,]    6    9\n\n\n[1]  1  4  7 10\n\n\n注意b[1]和b[, 1]的不同。\n获取数据框子集或元素：\n\nc[1]; c[, 1]; c[-2]; c[1:2, 3]; c$V1; c[\"V1\"]; c[, \"V1\"]\n\n  V1\na  1\nb  2\nc  3\n\n\n[1] 1 2 3\n\n\n  V1 V3 V4\na  1  7 10\nb  2  8 11\nc  3  9 12\n\n\n[1] 7 8\n\n\n[1] 1 2 3\n\n\n  V1\na  1\nb  2\nc  3\n\n\n[1] 1 2 3\n\n\n注意数据框获取子集方式的差异。\nsubset()函数可以通过条件过滤的方式获取子集：\n\nsubset(c, V1 &gt; 2, select = c(V2, V4)) \n\n  V2 V4\nc  6 12\n\nd[1]; d[\"V1\"]; d[[1]]; d$V1; d[1:2]; d[c(1,3)]\n\n$V1\n[1] 1 2 3\n\n\n$V1\n[1] 1 2 3\n\n\n[1] 1 2 3\n\n\n[1] 1 2 3\n\n\n$V1\n[1] 1 2 3\n\n$V2\n[1] 4 5 6\n\n\n$V1\n[1] 1 2 3\n\n$V3\n[1] 7 8 9\n\n\n注意列表子集查询和提取子集各方式的差异\n\n练习 2.9 \n在RStudio控制台输入以上代码，观察不同查询和获取子集方法结果的异同。",
    "crumbs": [
      "基础知识",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>R语言编程基础</span>"
    ]
  },
  {
    "objectID": "CH2.html#常用函数",
    "href": "CH2.html#常用函数",
    "title": "2  R语言编程基础",
    "section": "2.3 常用函数",
    "text": "2.3 常用函数\n对于开展数据处理、分析和建模而言，掌握R的一些常用函数的使用是非常必要的。R基础包中的所有函数的使用手册下载网址为https://cran.r-project.org/doc/manuals/r-release/fullrefman.pdf。\n\n2.3.1 常用数学函数\nBase-R提供的常用数学函数有：\n\nabs(): 计算绝对值\nsqrt(): 计算平方根\nlog()、log10()、log2(): 对数函数\nexp(): 指数函数\nsin()、cos()、tan(): 三角函数\nround(): 四舍五入指定保留位数\nceiling(): 向上取整\nfloor(): 向下取整\nderiv(): 求导数\nintegrate(): 求积分\n\n例如通过向量x计算y = sin(x) + log(x + 1)：\n\nx = 1:5\ny = sin(x) + log(x + 1)\n\n\n\n2.3.2 常用统计函数\nBase-R提供的常用统计函数有：\n\nsum(): 求和\nmean(): 计算算术平均值\nmedian(): 计算中位数\nsd(): 计算标准差\nvar(): 计算方差\nmad(): 计算绝对中位差\nmin()、max(): 计算最小值、最大值\nrange(): 计算值域，返回最小值和最大值\nquantile(): 计算百分位数\nscale(): 去中心化或标准化\ndiff(): 滞后差分\ndifftime(): 计算时间间隔\n\n\nset.seed(2024)\nx = rnorm(100, mean = 5, sd = 2)\nsum(x)\n\n[1] 483.0135\n\nmean(x)\n\n[1] 4.830135\n\nmedian(x)\n\n[1] 4.96555\n\nsd(x)\n\n[1] 2.044973\n\nvar(x)\n\n[1] 4.181914\n\nrange(x)\n\n[1] -1.548572  8.945637\n\nquantile(x, probs = c(0.1, 0.25, 0.50, 0.75, 0.90))\n\n     10%      25%      50%      75%      90% \n2.099641 3.560076 4.965550 6.343973 7.264369 \n\ny = 1:10\ndiff(y)\n\n[1] 1 1 1 1 1 1 1 1 1\n\ndiff(y, lag = 2)\n\n[1] 2 2 2 2 2 2 2 2\n\nscale(y, center = TRUE, scale = TRUE)\n\n            [,1]\n [1,] -1.4863011\n [2,] -1.1560120\n [3,] -0.8257228\n [4,] -0.4954337\n [5,] -0.1651446\n [6,]  0.1651446\n [7,]  0.4954337\n [8,]  0.8257228\n [9,]  1.1560120\n[10,]  1.4863011\nattr(,\"scaled:center\")\n[1] 5.5\nattr(,\"scaled:scale\")\n[1] 3.02765\n\n\nR提供了众多的概率分布函数，如表2-3所示。在这些概率分布的R命名前添加d、p、q、r前缀，分别表示概率密度函数(由x轴对应值计算分布曲线上对应的概率密度)、分布函数(由x轴对应值计算累积概率密度)、分位数函数(由累积概率密度计算x轴对应值)以及随机数生成函数(根据指定参数生成符合该分布类型的随机数集合)。\n\n\n\n\n表 2.1: R内置的概率分布函数\n\n\n\n\n\n\n分布类型\nR命名\n\n\n\n\n贝塔分布\nbeta\n\n\n二项分布\nbinom\n\n\n柯西分布\ncauchy\n\n\n卡方分布\nchisq\n\n\n指数分布\nexp\n\n\nF分布\nf\n\n\n伽马分布\ngamma\n\n\n几何分布\ngeom\n\n\n超几何分布\nhyper\n\n\n逻辑斯谛分布\nlogis\n\n\n对数正态分布\nlnorm\n\n\n负二项分布\nnbinom\n\n\n正态分布\nnorm\n\n\n泊松分布\npois\n\n\n学生t分布\nt\n\n\n均匀分布\nunif\n\n\n韦布尔分布\nweibull\n\n\n威尔考克斯分布\nwilcox\n\n\n\n\n\n\n\n\n例如生成均值为3、标准差为1的30个正态分布随机数：\n\nrnorm(30, mean = 3, sd = 1)\n\n [1] 1.624290 2.826074 5.711602 1.926553 1.951247 3.504240 4.445891 3.017523\n [9] 3.909281 3.671286 2.344481 2.746054 1.947971 3.636906 2.971663 3.883622\n[17] 1.081594 3.069935 1.597498 1.954728 4.311735 2.693475 1.078335 3.797105\n[25] 3.379924 1.464083 4.055746 4.033470 3.497045 2.649191\n\n\n计算分位数1.96对应的正态分布概率密度：\n\ndnorm(1.96)  \n\n[1] 0.05844094\n\n\n计算分位数1.96左侧的曲线下面积：\n\npnorm(1.96)  \n\n[1] 0.9750021\n\n\n计算分位数1.96右侧的曲线下面积：\n\n1 - pnorm(1.96)  \n\n[1] 0.0249979\n\n\n计算累积概率密度为0.975的分位数：\n\nqnorm (0.975) \n\n[1] 1.959964\n\n\n\n\n\n\n\n\n\n\n图 2.3: 标准正态分布各函数功能\n\n\n\n\n\n\n\n2.3.3 常用字符串处理函数\nBase-R提供的常用字符串处理函数如下：\n\nnchar(): 计算字符数量\nsubstr()、substring(): 提取或替代字符串\npaste()、paste0(): 连接字符串\nformat(): 把对象转换为字符串\n\\: 转义字符，如换行：\\n\nchartr(): 字符转换\nstrsplit(): 分割字符串\ngrep()、grepl(): 按正则表达式规则查找\nsub()、gsub(): 按正则表达式规则替换\ntolower()、toupper(): 小写和大写字母转换\nstrtrim(): 将字符串修建为指定宽度\n\n\nsubstr(\"abcdef\", 2, 4)\n\n[1] \"bcd\"\n\npaste(\"A\", 1:5, sep = \"-\")\n\n[1] \"A-1\" \"A-2\" \"A-3\" \"A-4\" \"A-5\"\n\npaste0(\"A\", 1:5)\n\n[1] \"A1\" \"A2\" \"A3\" \"A4\" \"A5\"\n\nstrsplit(\"a.b.c\", split = \".\")  # split参数为正则表达式\n\n[[1]]\n[1] \"\" \"\" \"\" \"\" \"\"\n\n# 上述操作不能实现按`.`分割字符串, 因为在正则表达式语法中，`.`表示任何字符\nstrsplit(\"a.b.c\", split = \"[.]\") \n\n[[1]]\n[1] \"a\" \"b\" \"c\"\n\n# `[.]`表示字符`.`\n\nstr &lt;- \"Now is the time      \"\nsub(\" +$\", \"\", str)  # 找到结尾多余空格并用\"\"替代，实质就是删除\n\n[1] \"Now is the time\"\n\nstr &lt;- \"Hello, World!\"\ngsub(\"World\", \"R\", str)\n\n[1] \"Hello, R!\"\n\n\ntidyverse套件中的stringr包是一个强大的字符串处理工具包，主要函数有：\n\nstr_length(): 获取字符串长度\nstr_c(): 拼接字符串\nstr_detect(): 字符包含检查\nstr_dup(): 字符多次重复\nstr_extract(): 提取第一个匹配的字符\nstr_extract_all(): 提取所有匹配的字符\nstr_match(): 返回第一个匹配的字符矩阵\nstr_match_all(): 返回所有匹配的字符矩阵\nstr_locate(): 提取第一个匹配字符的位置\nstr_locate_all(): 提取所有匹配字符的位置\nstr_pad(): 字符补齐\nstr_glue(): 字符创格式化输出\nstr_replace_na(): 用字符串”NA”替换NA\nstr_replace(): 替换第一个匹配的字符\nstr_replace_all(): 替换所有匹配的字符\nstr_which(): 返回匹配模式的字符位置\nstr_remove(): 删除第一个匹配的字符\nstr_remove_all(): 提取是所有匹配的字符\nstr_split(): 分割字符串\nstr_split_fixed(): 指定分割块数分割字符\nstr_sub(): 指定位置截取字符\nstr_subset(): 截取匹配模式的字符\nstr_sort(): 排序并返回字符向量\nstr_order(): 排序并返回字符索引\nstr_trim(): 删除字符串两边空格\nstr_squish(): 删除字符串中多余空格\nstr_to_lower(): 字符转小写\nstr_to_upper(): 字符转大写\nstr_to_title(): 字符串转标题\nstr_to_sentence(): 字符串转语句\n\n\nlibrary(stringr)\n\n# 拼接字符串\nstr_c(\"Hello\", \"World\")  # 默认无分隔符拼接\n\n[1] \"HelloWorld\"\n\nstr_c(\"Hello\", \"World\", sep = \" \")  # 指定分隔符拼接\n\n[1] \"Hello World\"\n\n# 分割字符串\nstr_split(\"one,two,three\", pattern = \",\") \n\n[[1]]\n[1] \"one\"   \"two\"   \"three\"\n\n# 提取字符串中的数字\nstr_extract(\"There are 42 apples\", \"\\\\d+\") \n\n[1] \"42\"\n\n# 替换字符串中的数字为#\nstr_replace(\"There are 42 apples\", \"\\\\d+\", \"#\") \n\n[1] \"There are # apples\"\n\n# 检测字符串是否包含数字\nstr_detect(\"There are 42 apples\", \"\\\\d+\") \n\n[1] TRUE\n\n# 返回匹配字符的位置\nstr_locate(\"banana\", \"a\") \n\n     start end\n[1,]     2   2\n\n# 补齐字符串\nstr_pad(\"Jack\", width = 7, side = \"right\", pad = \"S\") \n\n[1] \"JackSSS\"\n\n# 删除部分字符\nstr_remove(\"banana\", \"a\") \n\n[1] \"bnana\"\n\n\n\n\n2.3.4 常用数据处理函数\n\nlength(): 返回长度\nsort()、order(): 排序，返回排序结果或索引\ndim(): 返回数组和数据框的维度\nrev(): 逆序，返回逆序后的结果\nsubset(): 提取子集\nunique(): 提取唯一元素\nnrow()、ncol(): 返回数组的行数和列数\nNROW()、NCOL(): 返回包括0长度和1维向量的行数和列数\nrep() 生成重复的序列\nseq()、seq.POSIXt()、seq.Date()、seq_len()、seq_along(): 生成数值、时间和日期的等差序列\ncolnames()、rownames(): 查询和设置矩阵、数据框的列名和行名\n\ncbind()、rbind(): 对矩阵、数据框进行列合并和行合并\nnames() 查询和设置对象的名称\n\n\nx = c(1:4, -3:0, 2:4)\n\nlength(x)\n\n[1] 11\n\nsort(x)\n\n [1] -3 -2 -1  0  1  2  2  3  3  4  4\n\nsort(x, decreasing = TRUE)\n\n [1]  4  4  3  3  2  2  1  0 -1 -2 -3\n\norder(x)  # 在排序位置上给出相应元素原先的位置序号\n\n [1]  5  6  7  8  1  2  9  3 10  4 11\n\nunique(x)\n\n[1]  1  2  3  4 -3 -2 -1  0\n\ny = matrix(1:12, nrow = 3, byrow = TRUE)\ndim(y)\n\n[1] 3 4\n\nnrow(y)\n\n[1] 3\n\nncol(y)\n\n[1] 4\n\nrep(1:3, times = 2)\n\n[1] 1 2 3 1 2 3\n\nrep(1:3, each = 2)\n\n[1] 1 1 2 2 3 3\n\nrep(1:3, length.out = 5)\n\n[1] 1 2 3 1 2\n\nseq(from = 0, to = 2, by = 0.5)\n\n[1] 0.0 0.5 1.0 1.5 2.0\n\nseq(from = 0, to = 2, length.out = 4)\n\n[1] 0.0000000 0.6666667 1.3333333 2.0000000\n\nseq_along(x)\n\n [1]  1  2  3  4  5  6  7  8  9 10 11\n\nseq_along(y)\n\n [1]  1  2  3  4  5  6  7  8  9 10 11 12\n\ncolnames(y) = LETTERS[1:4]\ncolnames(y)\n\n[1] \"A\" \"B\" \"C\" \"D\"\n\nnames(x) = \"vec\"\nnames(x)\n\n [1] \"vec\" NA    NA    NA    NA    NA    NA    NA    NA    NA    NA   \n\nnames(x) = paste0(\"V\", 1:length(x))\nnames(x)\n\n [1] \"V1\"  \"V2\"  \"V3\"  \"V4\"  \"V5\"  \"V6\"  \"V7\"  \"V8\"  \"V9\"  \"V10\" \"V11\"\n\n\n\n\n2.3.5 常用日期时间处理函数\n时间序列数据中，日期和时间是关键变量。Base-R提供了日期与时间数据的处理函数，但lubridate包更高效和专业，并且使用广泛，主要操作函数如下：\n\n2.3.5.1 生成日期时间对象函数\n\nlibrary(lubridate)\n\nymd(\"2024-03-09\")\n\n[1] \"2024-03-09\"\n\nmdy(\"March 9, 2024\")\n\n[1] \"2024-03-09\"\n\ndmy(\"09/03/2024\")\n\n[1] \"2024-03-09\"\n\nymd_hms(\"2024-03-09 12:15:00\")\n\n[1] \"2024-03-09 12:15:00 UTC\"\n\n\n\n\n2.3.5.2 提取日期时间数据中的要素\n\nyear(ymd(\"2024-02-29\"))  # 提取年\n\n[1] 2024\n\nmonth(ymd(\"2024-02-29\"))  # 提取月\n\n[1] 2\n\nday(ymd(\"2024-02-29\"))  # 提取日\n\n[1] 29\n\nhour(ymd_hms(\"2024-02-29 11:15:30\"))  # 提取时\n\n[1] 11\n\nminute(ymd_hms(\"2024-02-29 11:15:30\"))  # 提取分\n\n[1] 15\n\nsecond(ymd_hms(\"2024-02-29 11:15:30\"))  # 提取秒\n\n[1] 30\n\nyday(ymd(\"2024-02-29\"))  # 提取年日, 1~365\n\n[1] 60\n\nmday(ymd(\"2024-02-29\"))  # 提取月日，1~31\n\n[1] 29\n\nwday(ymd(\"2024-02-29\"))  # 提取周日，1~7\n\n[1] 5\n\n\n\n\n2.3.5.3 获取当前日期和时间\n\ntoday()  # 获取当前日期\n\n[1] \"2024-04-06\"\n\nnow()  # 获取当前日期时间\n\n[1] \"2024-04-06 14:12:07 CST\"\n\n\n\n\n2.3.5.4 创建和操作时间间隔\n\nduration(1, \"days\")  # 创建以秒单位的Duration对象\n\n[1] \"86400s (~1 days)\"\n\nperiod(1, \"days\")  # 创建或解析Period对象\n\n[1] \"1d 0H 0M 0S\"\n\ninterval(ymd(\"2024-03-09\"), ymd(\"2024-03-10\"))  # 创建Interval对象\n\n[1] 2024-03-09 UTC--2024-03-10 UTC\n\n# 日期时间数据运算\nymd_hms(\"2024-01-01 00:00:00\") - years(1) + months(2) + days(5) + hours(2) \n\n[1] \"2023-03-06 02:00:00 UTC\"\n\n\n\n\n2.3.5.5 时区操作函数\nforce_tz()：强制改变日期时间对象的时区，而不改变时间。\n\ndt &lt;- ymd_hms(\"2024-02-29 11:15:30\")\ndt\n\n[1] \"2024-02-29 11:15:30 UTC\"\n\nforce_tz(dt, \"Asia/Shanghai\")\n\n[1] \"2024-02-29 11:15:30 CST\"\n\n\nwith_tz()：将日期时间对象转换为另一个时区，同时改变时间。\n\nwith_tz(dt, \"Asia/Shanghai\")\n\n[1] \"2024-02-29 19:15:30 CST\"\n\n\ntz()：获取或设置日期时间对象的时区。\n\ntz(dt)\n\n[1] \"UTC\"\n\ntz(dt) &lt;- \"Asia/Shanghai\"\ntz(dt)\n\n[1] \"Asia/Shanghai\"\n\n\n\n\n\n2.3.6 因子型数据处理函数\nR的因子型数据对象可以非常便利地处理分类变量和实验数据。R中因子型数据实际保存为整数，因此对因子进行字符型操作时，可能导致错误。安全的做法是在用字符型数据处理函数操作前，用as.character()函数将因子转换为字符。\n\nfactor()、ordered(): 创建无序和有序因子数据\ntable(): 统计因子数据各水平的频数\nlevels(): 查询因子数据的水平\ncut(): 分割连续变量并转换成因子\n\n\nss = substring(\"environment\", 1:11, 1:11)\nss\n\n [1] \"e\" \"n\" \"v\" \"i\" \"r\" \"o\" \"n\" \"m\" \"e\" \"n\" \"t\"\n\nff = factor(ss, levels = letters)\nff\n\n [1] e n v i r o n m e n t\nLevels: a b c d e f g h i j k l m n o p q r s t u v w x y z\n\ntable(ff)\n\nff\na b c d e f g h i j k l m n o p q r s t u v w x y z \n0 0 0 0 2 0 0 0 1 0 0 0 1 3 1 0 0 1 0 1 0 1 0 0 0 0 \n\nffo = ordered(ff)\nffo\n\n [1] e n v i r o n m e n t\nLevels: e &lt; i &lt; m &lt; n &lt; o &lt; r &lt; t &lt; v\n\n\n\nset.seed(2024)  # 设定随机数字发生器状态，以实现可重复性\nx = sample(1L:10L, 100, replace = TRUE)\ntable(x)\n\nx\n 1  2  3  4  5  6  7  8  9 10 \n13 13 11 13  9  6  5  4 16 10 \n\nclass(x)\n\n[1] \"integer\"\n\ntable(y &lt;- cut(x, breaks = c(0, 3, 7, 10)))\n\n\n (0,3]  (3,7] (7,10] \n    37     33     30 \n\nclass(y)\n\n[1] \"factor\"\n\nlevels(y)\n\n[1] \"(0,3]\"  \"(3,7]\"  \"(7,10]\"\n\n\n在因子水平较多时，往往需要对因子水平进行排序、合并等操作，forcats包提供了一些处理复杂因子型数据的函数，可在安装后利用help(package = forcats)查看帮助信息。\n\n\n2.3.7 其他常用函数\n\nset.seed(): 设置随机数字发生器状态\ncat(): 以连接的方式输出信息\nprint(): 打印对象，泛型函数\nls(): 列出当前环境中的对象\ngetwd()、setwd(): 获取和设置当前工作目录\ndir()、list.files(): 列出指定目录中的子目录和文件\nrm()、rm(list=ls()): 删除环境中指定或所有变量\nfile.exists()、file.rename(): 检查文件是否存在和文件改名\ndir.exsits()、dir.create(): 检查目录是否存在和创建目录\nfile.copy()、file.remove(): 文件复制和文件删除\nfile.path()、file.size(): 返回文件所在目录和大小\n\n\n# 用print()输出\nx = sin(pi/2)\ny = cos(pi/3)\n# 用print()输出x\nprint(x)\n\n[1] 1\n\nprint(y)\n\n[1] 0.5\n\n# 用cat()拼接输出\ncat(\"π/2的正弦函数值为\", x, \"\\n\") # 必须添加\"\\n\"以强制换行\n\nπ/2的正弦函数值为 1 \n\ncat(\"π/3的余弦函数值为\", y, \"\\n\")  \n\nπ/3的余弦函数值为 0.5",
    "crumbs": [
      "基础知识",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>R语言编程基础</span>"
    ]
  },
  {
    "objectID": "CH2.html#数据读写",
    "href": "CH2.html#数据读写",
    "title": "2  R语言编程基础",
    "section": "2.4 数据读写",
    "text": "2.4 数据读写\n少量或小规模的数据可以在R中用键盘直接输入，如a = c(1,2,4)。也可利用函数scan()在控制台从键盘接收数值和字符输入，或从文件导入数据；如a = scan()``，b = scan(what = \"\")。输入空行时(不输入数据直接按 Enter 键)结束输入。readlines()和readLines()函数可在控制台从键盘接收单行和多行字符串。后者还可以从一个连接(connection)对象(文件、网络链接、压缩文件等)接收文本。readLines()函数在控制台的结束需要按 Esc 键。\n当数据以各种格式的文件形式存在时，R及相关的包提供了丰富的接口来读写各种格式的数据，如文本数据(常见的如csv格式文件)、EXCEL表格数据、各种关系型数据库、NetCDF格式数据、XML格式数据、JSON格式数据、Matlab格式数据、统计软件SPSS、SAS以及stata格式的数据等。 R拥有自己的数据文件格式：rds和RData(或rda)，能够将数据更快地加载到R的工作空间，rds格式只能存储一个变量，而rda或RData格式可以保存不同类型的多个数据对象乃至整个工作空间中的内容。\n\n2.4.1 纯文本数据读写\n纯文本数据是最常见的一种数据格式，最常见的是以空格或逗号隔开。R提供read.table()函数及其各种变体函数(如read.csv()等)来读入纯文本数据，通过write.table()函数及各种变体函数(如write.csv()等)来保存纯文本数据。tidyverse套件中的readr包提供了丰富且效率更高的数据读写函数，其命名一般为read_*和write_*形式，这里的*代表不同类型数据的格式，如csv、tsv、rds等。这里介绍read_csv()和write_csv()两个函数的使用。\n读取csv格式数据文件的函数：\n\nread_csv(\n  file,  # 文件名，包括具体路径，需要用英文引号\n  col_names = TRUE,  # 指定第一行是否为列名，如果赋值字符串向量则指定列名\n  col_types = NULL,  # 指定列的数据类型\n  col_select = NULL,  # 选择读入的列\n  id = NULL,  # 指定存储数据文件路径的列名\n  locale = default_locale(),  # 区域设置，关系到时区、编码、小数点标记等\n  na = c(\"\", \"NA\"),  # 指定为缺失值的字符\n  comment = \"\",  # 指定为注释的字符串，其后文本将被忽略\n  trim_ws = TRUE,  # 解析数据前是否消除前导和尾随的空格\n  skip = 0,  # 指定跳过读取的行数\n  n_max = Inf,  # 指定读取的最大行数\n  guess_max = min(1000, n_max),  # 指定猜测列类型的最大行数\n  name_repair = \"unique\",  # 修复列名，默认是确保列名的唯一性\n  num_threads = readr_threads(),  # 指定初始解析和延迟读取数据的线程数\n  progress = show_progress(),  # 是否显示进度条\n  show_col_types = should_show_types(),  # 是否显示猜测的列名\n  skip_empty_rows = TRUE,  # 是否忽略空行\n  lazy = should_read_lazy()  # 是否延迟读取\n)\n\n以上为read_csv()函数及其参数。其他类似函数的用法与此大同小异，具体可以查阅帮助信息。 例如，读取当前目录下data子目录中的so2-2021-hf.csv数据文件：\n\ndf = readr::read_csv(\"./data/so2-2021-hf.csv\")\n\n注意，R中输入文件目录中的“”具有转义功能，所以要用“/”或“\\”替代。上面的代码将该文件读入到df变量中。 保存csv格式数据文件的函数：\n\nwrite_csv(\n  x,  # 指定要存储的变量\n  file,  # 指定存储的文件名，可包含路径\n  na = \"NA\",  # 指定缺失值使用的字符串，默认是NA\n  append = FALSE,  # 指定数据添加到文件的方式，默认是覆盖，如为True则追加到现有文件中\n  col_names = !append,  # 指定是否保存列名\n  quote = c(\"needed\", \"all\", \"none\"),  # 指定处理包含需要引号的字符的字段的方式\n  escape = c(\"double\", \"backslash\", \"none\"),  # 当数据中有引号时的转义方式\n  eol = \"\\n\",  # 指定行结束字符\n  num_threads = readr_threads(),  # 同read_csv\n  progress = show_progress()  # 同read_csv\n)\n\n以上为write_csv()函数及其参数，其他类似函数的用法与此基本相同，具体可以查阅帮助信息。 例如，将内置数据集airquality存储到当前子目录data中：\n\nwrite_csv(airquality, \"./data/aqdata.csv\")\n\n\n\n2.4.2 Excel数据读写\ntidyverse套件中的readxl包提供了读取Excel格式数据文件的函数： read_excel()、read_xls()和read_xlsx()。其中read_excel()函数的形式及其参数如下所示：\n\nread_excel(\n  path,  # 指定文件名，包含路径\n  sheet = NULL,  # 指定要读取的工作表(Excel文件可包含多个工作表)\n  range = NULL,  # 指定工作表的读取范围\n  col_names = TRUE,  # 指定第一行是否为列名\n  col_types = NULL,  # 指定各列数据类型\n  na = \"\",  # 指定解释为缺失值的字符\n  trim_ws = TRUE,  # 是否消除前导和尾随空格\n  skip = 0,  # 指定跳过读取的行数\n  n_max = Inf,  # 指定读取的最大行数\n  guess_max = min(1000, n_max),  # 指定猜测列类型的最大行数\n  progress = readxl_progress(),  # 是否显示进度条\n  .name_repair = \"unique\"  # 修复列名，默认是确保列名的唯一性\n)\n\n其他两个函数的用法与read_excel()基本相同。 例如读取当前目录下子目录data中的pollutants-2021-hf.xlsx数据文件中第二个表格：\n\ndf = readxl::read_xlsx(\"./data/pollutants-2021-hf.xlsx\",\n                       sheet = 2, col_names = TRUE)\n\n如果要将R的数据框变量写为Excel格式文件，可利用xlsx包中的write.xlsx()函数。\n\n\n2.4.3 R格式数据读写\n读写包含单个变量的rds格式数据：\n\na = 1:12\nsaveRDS(a, \"a.rds\")    # 将a的内容保存到a.rds文件中\nrm(a)    # 从当前空间删除a\nb = readRDS(\"a.rds\")    # 从a.rds文件读入其中保存的数据，并赋值给变量b\n\n读写包含多个变量的RData(或rda)类型数据文件的函数为load()和save()。save.image()是save()的变体，用于将当前环境所有变量都写入RData文件。R和RStudio在退出时默认将当前工作环境所有变量保存到当前目录下的.RData文件中。为了区别以及方便，用户自己保存的变量建议用rda作为扩展名。\n\na = 1:5; b = letters[1:6]; c = c(\"PM10\", \"PM2.5\", \"O3\")\nsave(a, b, c, file = \"air.rda\")    # 将三个变量写入文件，\"file =\" 不可省略\nrm(list = ls())    # 删除环境中所有变量\nload(\"air.rda\")    # 读取数据文件，其中变量全部加载入当前工作空间\n\n\n\n2.4.4 其他格式数据读写\nNetCDF(Network Common Data Form)是美国大学大气研究协会(UCAR)的Unidata项目科学家针对科学数据的特点开发的网络通用数据格式，是一种面向数组型、跨平台并适于网络共享的数据的描述和编码标准。NetCDF已经广泛应用于大气科学、水文、海洋学、环境模拟、地球物理等诸多领域。 在R中导入NetCDF格式数据(扩展名为nc)需要借助ncdf4包，具体操作可参考该包的帮助信息。\nXML是可扩展标记语言(eXtensible Markup Language)，被设计用于传输和存储数据。利用XML包，可以读取和处理XML数据文件。\nJSON是JavaScript Object Notation的缩写，剂JavaScript对象表示法，是一种存储和交换文本信息的语法。JSON类似XML，但比XML更小、更快，更易解析。jsonlite和rjson是R语言处理json数据的最常用的两个工具包。\nR的foreign包中提供了读取SPSS、SAS、Stata、Minitab、S、Weka等多个统计软件的数据文件的函数以及保存为相关格式数据文件的函数。R.matlab包提供了读写mat格式数据文件并从R内部调用Matlab的功能。imager包提供了多种图像处理的函数，帮助R对图像数据进行分析。\n在RStudio的“File”菜单中，有一个“Import Dataset”菜单项，其中不仅有导入文本和Excel数据的选项，也有导入SPSS、SAS和Stata格式数据的选项(通过haven包实现)。在RStudio的“Environment”窗口的工具栏上也可以找到这些选项。\nR拥有多种面向关系型数据库管理系统(DBMS)的接口(包)，包括SQL Server、Access、MySQL、Oracle、PostgreSQL、DB2、Sybase、Teradata、以及SQLite。其中一些包通过原生的数据库驱动提供访问功能，另一些则是通过ODBC或JDBC来实现访问的。\n\n\n2.4.5 大数据文件读写\n在处理较大规模的数据文件时(文件大小1GB以上)，data.table包提供的fread()和fwrite()函数的读写速度具有显著的优势。同时该包还提供了更高效的各种操作数据的函数。结合bigmemory包的内存管理，可以实现大数据的高效处理。在CRAN网站的“Task View”中的HighPerformanceComputing任务分类中可以查阅大数据读写、处理与分析相关的R包。其中fst包使用多线程的方式读写fst格式的数据文件。大数据保存为csv格式会占用很大的磁盘空间，可利用fst包中的write_fst()函数将导入的csv格式数据另存为fst格式文件，再用read_fst()读取fst文件，既可减少数据磁盘占用，又能加快读取速度。ff、disk.frame等R包提供了对太大而无法加载到内存中的数据集的访问功能以及许多更高级别的功能。\n更大规模的海量数据通常采用分布式文件系统、NoSQL数据库、云数据库等方式存储。Spark和Hadoop是两个著名的大数据框架，Spark处理速度快且易于使用，而Hadoop安全性高且成本低廉。R包sparklyr提供了Spark的R接口，该包支持连接到本地和远程的Spark集群，提供了一个兼容dplyr包的后端，并提供使用Spark内置机器学习算法的接口。有关R与Spark结合来处理大数据的内容，可参阅R interface to Apache Spark提供的信息。",
    "crumbs": [
      "基础知识",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>R语言编程基础</span>"
    ]
  },
  {
    "objectID": "CH2.html#控制结构和自定义函数",
    "href": "CH2.html#控制结构和自定义函数",
    "title": "2  R语言编程基础",
    "section": "2.5 控制结构和自定义函数",
    "text": "2.5 控制结构和自定义函数\n\n2.5.1 表达式\nR的任何一个语句都可以看成是一个表达式。R允许一行可以有多个表达式，但必须以分号隔开。如：\n\na = 5; b = rep(1:4, each = 3)\nc = mean(b)\n\n为了提高代码的可读性和格式规范性，通常不建议一行有多个表达式。当表达式长度超过行宽时， 表达式会自动续行。 如果上一行不是完整的表达式，如末尾是加减乘除等运算符，或有未配对的括号，则下一行为上一行的继续。若干个表达式可以组成一个复合表达式，复合表达式的值为最后一个表达式的值， 组合用大括号表示, 如：\n\n{\n  a = 1\n  b = 3\n  c = a^2 + b^2\n  c\n}\n\n[1] 10\n\n\n复合表达式常常用于分支结构、循环结构以及自定义函数。\n\n\n2.5.2 分支结构\n\n2.5.2.1 ifelse\nifelse是一个结构紧凑的判断语句，其形式为：\n\nifelse(test, yes, no)\n\n参数test是一个返回逻辑值的对象或表达式，如果返回TRUE，则选择yes项，否则选择no项。如：\n\nx = -2:2\ny = ifelse(x &gt;= 0, sqrt(x), NA)\n\n上面的代码先对x中的元素执行≥0的比较，其返回值为TRUE或FALSE，如果为TRUE，则计算并返回该元素的平方根，如果为FALSE，则返回NA。\n\n\n2.5.2.2 if和if…else…\nif语句执行一个条件判断，当返回值为TRUE时执行其后的表达式，否则不执行。如\n\nx &lt;- 30L\nif(is.integer(x)) {\n   print(\"x是一个整数\")\n}\n\n[1] \"x是一个整数\"\n\n\nif…else…对条件判断返回的TRUE和FALSE都进行响应。如：\n\nx = c(\"tmall\", \"jingdong\", \"taobao\")\na = \"tmall\"\nif(a %in% x) {\n   print(\"x包含a\")\n} else {\n   print(\"不包含a\")\n}\n\n[1] \"x包含a\"\n\n\nif…else…还可以嵌套，以对更多判断做出响应，如：\n\nx = c(0.05, 0.6, 0.3, 0.9)\n\nfor(i in seq(along = x)){\n  if(x[i] &lt;= 0.2){\n    cat(\"Small\\n\")\n  } else if(x[i] &lt;= 0.8){\n    cat(\"Medium\\n\")\n  } else {\n    cat(\"Large\\n\")\n  }\n}\n\nSmall\nMedium\nMedium\nLarge\n\n\nR是向量化语言，应尽可能少用标量运算，而采用向量化运算。例如，有一个向量x，现在要通过x来计算得到另一个等长向量y， 计算规则为y中元素当且仅当x的对应元素为正数时等于1， 否则等于0。 下面的代码是错误的：\n\nx = rnorm(10, 0, 2)  # 随机生成10个符合均值为0、标准差为2的正态分布数据\nif(x &gt; 0) y = 1 else y = 0\n\nError in if (x &gt; 0) y = 1 else y = 0: the condition has length &gt; 1\n\n\n原因是x &gt; 0返回的是一个包含多个值的逻辑向量，而后续的赋值操作传递的是一个标量。 正确的向量化运算的代码为：\n\nx = rnorm(10, 0, 3)\ny = integer(length(x))    # 生成一个与x等长的元素均为0的整数向量\ny[x &gt; 0] = 1\ny\n\n [1] 0 0 0 0 0 1 0 0 0 0\n\n\n采用向量的下标(或称索引)来实现判断是一种良好的编程习惯。x &gt; 0返回一个与x等长度的逻辑型向量，同时对应y的每一个下标，值为TRUE的下标位置上的y元素被赋值1。 R中的switch()函数也可以建立多分支结构，但嵌套的if…else…结构更容易理解，逻辑也更清晰。因此，尽可能避免使用swtich()创建多分支结构。\n\nx = \"air\"\nres &lt;- switch(x,\n              water = \"COD\",\n              air   = \"SO2\",\n              \"未知选项\")\nprint(res)\n\n[1] \"SO2\"\n\nx = \"soil\"\nres &lt;- switch(x,\n              water = \"COD\",\n              air   = \"SO2\",\n              \"未知选项\")\nprint(res)\n\n[1] \"未知选项\"\n\n\n\n\n\n2.5.3 循环结构\n\n2.5.3.1 for循环\nfor循环可以实现基本的计数循环，也能对容器对象中的各个元素逐一进行操作。例如，前面向量计算的问题也可以采用for循环来解决：\n\nx = rnorm(10, 0, 3)\ny = integer(length(x))\nfor(i in seq_along(x)){  # seq_along()函数生成与x长度相等的从1开始的整数序列\n  if(x[i] &gt; 0) y[i] = 1 else y[i] = 0\n}\n\n显然，for循环很容易理解，但执行效率低，而且没有向量化运算简洁和优雅。\n虽然R可以直接遍历向量中的元素，但有时会导致向量属性丢失，如日期时间类型的向量。此时，应采用下标访问的方法。此外，在循环中可以通过next命令(而非函数)直接进入下一轮循环，用break命令跳出循环。\n\n# 创建日期类型向量\ndate_vec = as.POSIXct(c(\"2024-01-01\", \"2024-02-01\",\n                        \"2024-03-01\", \"2024-04-01\"))\n# 使用下标遍历向量\nfor(i in seq_along(date_vec)) {\n  # 如果日期在3月和4月，就跳出整个循环\n  if (format(date_vec[i], \"%m\") %in% c(\"03\", \"04\")) {\n    break\n  }  \n  # 如果日期在2月，就跳过本轮循环，不打印日期\n  if (format(date_vec[i], \"%m\") == \"02\") {\n    next\n  }\n  # 打印日期\n  print(date_vec[i])\n}\n\n[1] \"2024-01-01 CST\"\n\n\n\n\n2.5.3.2 while和repeat循环\nwhile循环在条件为TRUE时才进行循环，如果第一次条件判断返回FALSE就一次也不执行循环内的语句。而repeat循环是无条件循环，其循环内的语句中一般采用if语句和break命令结合，当if判断条件为TRUE时跳出循环，所以repeat语句至少需要执行一次循环内的语句。\n\n# while循环\ni = 1\nwhile (i &lt;= 3) {\n  print(paste(\"这是第\", i, \"次while循环\"))\n  i = i + 1\n}\n\n[1] \"这是第 1 次while循环\"\n[1] \"这是第 2 次while循环\"\n[1] \"这是第 3 次while循环\"\n\n# repeat循环\ni = 1\nrepeat {\n  print(paste(\"这是第\", i, \"次repeat循环\"))\n  i = i + 1\n  if (i &gt; 3) {\n    break\n  }\n}\n\n[1] \"这是第 1 次repeat循环\"\n[1] \"这是第 2 次repeat循环\"\n[1] \"这是第 3 次repeat循环\"\n\n\n在R中应尽量避免使用循环，因为其速度比向量化运算慢一个数量级以上，处理大数据时，运算效率低的劣势更为明显。\n\n\n2.5.3.3 循环遍历的替代方法\n对矩阵以及数据框类型的大数据集，采用循环遍历其中每个元素的方法，其时间成本非常大。为此，R提供了apply系列函数来替代循环语句，实现高效运算。\n\napply(): 按行或列对数组元素应用函数，返回向量\nlapply(): 对列表或向量元素应用函数，返回列表\nsapply(): 与lapply()相同，但简化输出\ntapply(): 按因子水平分类并应用函数\nmapply(): sapply()的多元版本\nrapply(): lapply()的递归版本\neapply(): 运用函数遍历“环境”的具名元素\nvapply(): 类似sapply()，可设定返回值的类型\n\napply系列函数在遍历不同对象中元素的同时，按指定维度或范围应用指定的函数来进行运算，然后返回运算结果。apply函数形式如下：\n\napply(X,           # 数据对象\n  MARGIN,          # 1表示按**行**运算，2表示按**列**运算\n  FUN,             # 执行运算的函数名，可以是自定义的函数\n  ...,             # 其他参数\n  simplify = TRUE  # 输出结果是否简化\n)\n\n例如，对R内置的鸢尾花分类数据集iris的前4列执行求均值、标准差运算：\n\napply(iris[, 1:4], 2, mean)\n\nSepal.Length  Sepal.Width Petal.Length  Petal.Width \n    5.843333     3.057333     3.758000     1.199333 \n\napply(iris[, 1:4], 2, sd)\n\nSepal.Length  Sepal.Width Petal.Length  Petal.Width \n   0.8280661    0.4358663    1.7652982    0.7622377 \n\n\ntapply函数对数据按指定的因子水平分组后再应用指定的函数。例如对iris数据集按鸢尾花类别名称(三类，即三个水平)计算Sepal.length列的平均值：\n\ntapply(iris$Sepal.Length, iris$Species, mean)\n\n    setosa versicolor  virginica \n     5.006      5.936      6.588 \n\n\n可通过查阅帮助信息以进一步学习和掌握这些函数的用法。注意不同apply函数返回结果的类型可能不同。\nR还提供了rowSums()、colSums()、rowMeans()、colMeans()四个函数以替代循环计算矩阵和数据框的行和、列和、行均值、列均值。\ntidyverse中的purrr包提供了泛函式编程功能，可以实现更优雅的循环迭代。函数的函数称为泛函，在编程中表示函数作用在函数上，或者说函数包含其它函数作为参数。其中map()和map_*()系列函数依次将指定函数应用于一个向量或列表中的每个元素上，map2()和map2_*()系列函数依次将指定函数应用于两个向量中的每对元素上，pmap()和pmap_*()系列函数将指定函数应用于多个向量(封装成列表对象或数据框对象)中的每组元素上，imap()和imap_*()系列函数将指定函数应用于向量或列表的元素及其索引上。map()、map2()及pmap()默认返回列表，imap()返回向量。添加后缀_int、_dbl、_lgl、_chr、_df则返回相应的类型，添加_df、_dfr、_dfc则返回数据框类型，其中后二者可按行或列合并。这些函数输入的向量应为原子向量类型，即向量中的元素是同类型的，包括6种类型：logical、integer、double、character、complex、raw(原始字节)，其中 integer 和 double 统称为numeric。如果需要有条件的应用map()，可使用map_if()和map_at()函数。\n例如，用map函数对iris数据集前4列求平均值的代码如下：\n\nlibrary(purrr)\nmap(iris[, 1:4], mean)\n\n$Sepal.Length\n[1] 5.843333\n\n$Sepal.Width\n[1] 3.057333\n\n$Petal.Length\n[1] 3.758\n\n$Petal.Width\n[1] 1.199333\n\n\npurrr还提供了walk()、walk2()、pwalk()和iwalk()函数，用于执行副作用，即不返回任何结果。有时仅需要遍历一个数据结构调用函数进行一些显示、绘图等无需返回值的操作，这称为函数的副作用，不需要返回结果。\n\nlist1 = list(a = 1, b = 2, c = 3)\nlist2 = list(x = 10, y = 20, z = 30)\nwalk2(list1, list2, ~ cat(.x, .y, \"\\n\"))\n\n1 10 \n2 20 \n3 30 \n\n\ntidyverse套件都支持管道操作符，最常用的是%&gt;%，还有%T&gt;%(传递上一步结果给下一步操作后，再将上一步原始结果继续向后传递)、%&lt;&gt;%(用下一步结果对上一步结果进行更新)和%$%(将上一步结果中的名字传递给下一步)，使用频率较低。%&gt;%的功能最简单，就是将数据经上一个函数处理或操作后的结果传递给下一个函数，从而将数据整理、分析与建模的流程整合在一个链条上，既简化了代码，使得代码逻辑关系更加清晰，又能省去中间变量的输出，减少内存占用。这些管道操作符依赖于magrittr包。R自4.1版开始增加的原生管道符|&gt;，可以替代%&gt;%，从而不再依赖magrittr包，减少资源占用，运算效率更高。但原生管道符|&gt;目前还不支持占位符，当函数的第一个参数不是上一步骤的结果时，需要构造匿名函数来解决。\n\nlibrary(magrittr)\n# 使用%&gt;%\nrnorm(10000, mean = 10, sd = 1) %&gt;%          # 生成10000个正态分布随机数\n  sample(size = 100, replace = FALSE) %&gt;%    # 从中随机抽取100\n  abs %&gt;%                                    # 求绝对值\n  log %&gt;%                                    # 求自然对数值\n  diff %&gt;%                                   # 一次差分\n  plot(col = \"red\", type = \"l\")              # 绘制图形\n\n\n\n\n\n\n\n\n从上述代码可见，利用管道操作符整合数据处理流程，代码简洁优雅，逻辑清晰。当函数的第一个形参就是上一个函数处理后的对象且无需设置其他形参时，可省略()，只使用函数名。 如果流程中调用的第一个输入参数并非上一步结果的函数，如lm()函数，则%&gt;%和|&gt;会有区别：\n\nairquality %&gt;% \n  lm(Ozone ~ Temp, data = .)  \n\n\nCall:\nlm(formula = Ozone ~ Temp, data = .)\n\nCoefficients:\n(Intercept)         Temp  \n   -146.995        2.429  \n\n\n%&gt;%提供了占位符.来解决这个问题，而|&gt;则需要构造一个匿名函数来解决这个问题：\n\nairquality |&gt;\n  (\\(x) lm(Ozone ~ Temp, data = x))()\n\n\nCall:\nlm(formula = Ozone ~ Temp, data = x)\n\nCoefficients:\n(Intercept)         Temp  \n   -146.995        2.429  \n\n\n在R与RStudio中可使用Ctrl+Shift+MCtrl+Shift+M组合键输入管道符号，默认为%&gt;%，如果需要改为|&gt;，可依次点击菜单“Tools”→“Global options…”，打开“Options”对话框，点击其中左侧“Code”，在右侧“Editing”标签下，勾选“Use native pipe operator, |&gt; (requires 4.1+)”选项。\n\n\n\n2.5.4 自定义函数\nR语言以及R包的主要功能都是通过函数实现的。除了直接利用R语言和R包内置的各种函数以外，用户还可以根据数据处理任务的需求自己编写函数。自定义函数的形式如下：\n\n函数名 = function(arg1, arg2, ...) {\n  函数体\n  return(返回值)\n}\n\narg1、arg2是自定义函数的形参，可以没有，也可以是多个，甚至用...作为形参以表示可以有任意个参数，另外也可以为形参指定默认值。函数体用花括号包围，return()并非必需(但有返回值时，建议加上，以提高代码可读性)，因为R自定义函数默认将最后一行的值作为返回值。 下面给出一个自定义函数的例子：\n\nmyf = function(n, mean = 5, sd = 2){\n  z = rnorm(n, mean = mean, sd = sd)\n  avg = mean(z)\n  mx = max(z)\n  mn = min(z)\n  std = sd(z)\n  barplot(z)\n  return(list(avg = avg, min = mn, max = mx, sd = std))\n} \n\n调用该函数时，可以一次传入三个参数的值，但也可以只传递n的参数值，mean和sd参数采用默认值。调用该函数后，会根据传入的参数生成一个正态分布的数据集，接着计算这个数据集的平均值、最大最小值以及标准差，并绘制该数据集的条形图，最后用列表形式返回计算的四个值。\n\nmyf(100, 10, 2)\n\n\n\n\n\n\n\n\n$avg\n[1] 10.00962\n\n$min\n[1] 5.654718\n\n$max\n[1] 14.38687\n\n$sd\n[1] 1.797214\n\n\n\n练习 2.10 \n利用readr包编写R代码，将airquality数据集命名为aqdata.csv保存到磁盘上。然后再将其读入到R的工作空间。利用readxl包编写R代码，将airquality数据集命名为aqdata.xlsx保存到磁盘上。然后再将其读入到R的工作空间。\n\n\n练习 2.11 \n选择使用purrr包中map系列函数，计算R内置数据集iris前四列的均值和标准差。\n\n\n练习 2.12 \n编写R代码，识别airquality数据集中的缺失值，并将其删除。\n\n\n练习 2.13 \n(4)大气污染物高架连续点源地面浓度理论计算方程为：\n\n\\rho(x,y,0)=\\frac{Q}{\\pi\\bar{u}\\sigma_{y}\\sigma_{z}}\\exp\\left(-\\frac{y^{2}}{2\\sigma_{y}^{2}}\\right)\\exp\\left(-\\frac{H^{2}}{2\\sigma_{z}^{2}}\\right)\n\n式中：\\rho为地面任一点污染物浓度，g/m3；x(正向为平均风向)和y(垂直于x，正向在x轴左侧)为地面任一点坐标；Q为源强，g/s；u̅为平均风速，m/s；H为有效源高，m；\\sigma_{y}和；\\sigma_{z}分别为距原点x处烟流中污染物在y和z方向上分布的标注差，m。请为该方程编写一个R函数。",
    "crumbs": [
      "基础知识",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>R语言编程基础</span>"
    ]
  },
  {
    "objectID": "CH4.html",
    "href": "CH4.html",
    "title": "4  环境数据统计分析",
    "section": "",
    "text": "4.1 描述性统计\n描述性统计分析是以一种简单而有意义的方式来描述、显示和概括原始数据，亦可称之为探索性数据分析(Exploratory Data Analysis，EDA)，主要包括频数分析、集中趋势分析、离散程度分析、数据分布特征以及一些基本的统计图表。",
    "crumbs": [
      "进阶知识",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>环境数据统计分析</span>"
    ]
  },
  {
    "objectID": "CH4.html#描述性统计",
    "href": "CH4.html#描述性统计",
    "title": "4  环境数据统计分析",
    "section": "",
    "text": "4.1.1 频数分析\n频数分析是按某个分类变量分组或按某个数值变量分区间对数据进行频数统计，以了解数据在这些有意义的组或区间上的具体分布。在频数的基础上，可以进一步计算频率、累积频率等指标。直方图是最常用的频数分析的可视化方式。R提供的table()函数可对数据集中某个分类变量进行频数统计(结果为频数分布表，也称为一维列联表)，xtabs()函数根据两个分类变量创建交叉频数统计(结果为二维列联表)，prop.table()函数在频数统计的结果上计算频率，addmargins()函数为频数或频率统计结果计算行列之和。在R Markdown和Quarto文档中，可利用knitr包中的kable()函数美化频数统计表的输出。\n\n例 4.1 下面是一个对河流水质数据进行的频数分析。\n\n\nlibrary(tidyverse)\n\nset.seed(2022)\n#随机构造数据：rn为河流名称，wq为水质等级，wt为河流类型\ndata &lt;- tibble(\n  rn = paste(\"河流\", 1:50),\n  wq = sample(c(\"I类\",\"II类\",\"III类\",\"IV类\",\"V类\",\"劣V类\"),\n              50, replace = TRUE),\n  wt = sample(c(\"小型河流\",\"中型河流\",\"大型河流\"),\n              50, replace = TRUE)) %&gt;% \n  mutate(wq = factor(wq,levels = c(\"I类\",\"II类\",\"III类\",\n                                   \"IV类\",\"V类\",\"劣V类\"),\n                 ordered = TRUE),\n         wt = factor(wt,levels = c(\"小型河流\",\"中型河流\",\n                                   \"大型河流\"),\n                 ordered = TRUE))\ndata\n\n# A tibble: 50 × 3\n   rn      wq    wt      \n   &lt;chr&gt;   &lt;ord&gt; &lt;ord&gt;   \n 1 河流 1  IV类  大型河流\n 2 河流 2  III类 中型河流\n 3 河流 3  劣V类 小型河流\n 4 河流 4  III类 大型河流\n 5 河流 5  IV类  中型河流\n 6 河流 6  劣V类 中型河流\n 7 河流 7  劣V类 大型河流\n 8 河流 8  IV类  大型河流\n 9 河流 9  III类 大型河流\n10 河流 10 I类   小型河流\n# ℹ 40 more rows\n\nt1 &lt;- data$wq %&gt;% table() \nt1 %&gt;% knitr::kable()\n\n\n\n\n.\nFreq\n\n\n\n\nI类\n7\n\n\nII类\n7\n\n\nIII类\n10\n\n\nIV类\n10\n\n\nV类\n7\n\n\n劣V类\n9\n\n\n\n\nt2 &lt;- data %&gt;% xtabs(~ wq + wt, .)\nt2 %&gt;% knitr::kable()\n\n\n\n\n\n小型河流\n中型河流\n大型河流\n\n\n\n\nI类\n4\n2\n1\n\n\nII类\n1\n3\n3\n\n\nIII类\n2\n4\n4\n\n\nIV类\n3\n3\n4\n\n\nV类\n3\n1\n3\n\n\n劣V类\n2\n3\n4\n\n\n\n\nt2 %&gt;% \n  addmargins() %&gt;% \n  knitr::kable()\n\n\n\n\n\n小型河流\n中型河流\n大型河流\nSum\n\n\n\n\nI类\n4\n2\n1\n7\n\n\nII类\n1\n3\n3\n7\n\n\nIII类\n2\n4\n4\n10\n\n\nIV类\n3\n3\n4\n10\n\n\nV类\n3\n1\n3\n7\n\n\n劣V类\n2\n3\n4\n9\n\n\nSum\n15\n16\n19\n50\n\n\n\n\nt2 %&gt;% \n  prop.table() %&gt;% \n  addmargins() %&gt;% \n  knitr::kable()\n\n\n\n\n\n小型河流\n中型河流\n大型河流\nSum\n\n\n\n\nI类\n0.08\n0.04\n0.02\n0.14\n\n\nII类\n0.02\n0.06\n0.06\n0.14\n\n\nIII类\n0.04\n0.08\n0.08\n0.20\n\n\nIV类\n0.06\n0.06\n0.08\n0.20\n\n\nV类\n0.06\n0.02\n0.06\n0.14\n\n\n劣V类\n0.04\n0.06\n0.08\n0.18\n\n\nSum\n0.30\n0.32\n0.38\n1.00\n\n\n\n\n\ncumsum()函数用于计算累积频数或频率：\n\ndf &lt;- t1 %&gt;% \n  prop.table() %&gt;% \n  as.data.frame() %&gt;% \n  mutate(CumFreq = cumsum(Freq))\ndf\n\n      . Freq CumFreq\n1   I类 0.14    0.14\n2  II类 0.14    0.28\n3 III类 0.20    0.48\n4  IV类 0.20    0.68\n5   V类 0.14    0.82\n6 劣V类 0.18    1.00\n\n\n频数分布表可以用as.data.frmae()函数转换为数据框类型。\n对数值变量做分组频数统计，需要先确定分组区间界值及相应的标签，再用cut()函数根据区间界值对数据分组，然后用table()和prop.table()函数进行频数和频率统计。\n\n例 4.2 现有某市2014年冬季(2014年12月-2015年2月)的每日AQI指数，共90个数据(如下表所示)，对其做频数分析。\n\n\n\n\n\n\n\n\n月份\nAQI\n\n\n\n\n12月\n86 62 75 71 78 107 130 137 86 119 134 81 76 92 132 70 55 79 79 108 78 71 99 150 117 65 95 105 103 94 73\n\n\n1月\n64 79 94 310 194 185 91 98 99 165 142 97 76 72 86 183 161 127 98 99 199 141 154 185 257 264 118 53 56 142 135\n\n\n2月\n67 99 147 217 121 80 103 72 62 70 95 120 240 164 74 196 212 139 77 42 55 130 132 96 70 144 54 29\n\n\n\n根据《环境空气质量评价技术规范(试行)》(HJ 663—2013)的规定，AQI(空气质量指数)分级如下：0~50为优(一级)，51~100为良(二级)，101~150为轻度污染(三级)，151~200为中度污染(四级)，201~300为重度污染(五级)，&gt;300为严重污染(六级)。 先通过电子表格软件如WPS表格或Office Excel，输入数据，第一列列名为Month，第二列列名为Day，第三列列名为AQI，最后命名保存为aqi-data.csv数据文件。\n\nlibrary(tidyverse)\n\ndf &lt;- read_csv(\"./data/aqi-data.csv\")\nlabels &lt;- c(\"优\", \"良\", \"轻度污染\", \"中度污染\", \"重度污染\", \"严重污染\")\nbreaks &lt;- c(0, 51, 101, 151, 201, 301, Inf)\naqi &lt;- cut(df$AQI, breaks = breaks, labels = labels,\n           include.lowest = TRUE)\n# include.lowest参数为TRUE时表示区间包含界值下限，为FALSE时表示包含界值上限\nft &lt;- as.data.frame(table(Group = aqi))\nlibrary(dplyr)\ndft &lt;- ft %&gt;% \n  mutate(CumFreq = cumsum(Freq),\n         FreqRate = prop.table(Freq)) %&gt;% \n  mutate(CumFreqRate = cumsum(FreqRate)) %&gt;% \n  mutate(FreqRate = round(FreqRate * 100, 2),\n         CumFreqRate = round(CumFreqRate * 100, 2))\ndft\n\n     Group Freq CumFreq FreqRate CumFreqRate\n1       优    2       2     2.22        2.22\n2       良   47      49    52.22       54.44\n3 轻度污染   25      74    27.78       82.22\n4 中度污染   10      84    11.11       93.33\n5 重度污染    5      89     5.56       98.89\n6 严重污染    1      90     1.11      100.00\n\n\n由以上频数分析结果可见，该城市冬季AQI为“优”和“良”的累积比例超过54%。\n\ndft$Group &lt;- ordered(dft$Group,                     \n                     levels = c(\"优\",\"良\",\"轻度污染\",\"中度污染\",\n                                \"重度污染\",\"严重污\"))\nggplot(dft, aes(x = Group, y = Freq)) + \n  geom_col() +   \n  geom_text(aes(label = Freq), vjust = -0.5) +  \n  ylim(c(0,55)) +   \n  labs(x = \"空气质量等级\", y = \"累计天数\") +  \n  theme_bw()\n\n\n\n\n\n\n\n图 4.1: 频数分布条形图\n\n\n\n\n\n在除尘系统设计中，也需要对颗粒物粒径进行类似的频数统计，从而更好地了解尘粒的粒径分布以计算分级除尘效率。\n\n\n4.1.2 集中趋势\n集中趋势反映一组变量值的集中位置，用于描述数据的一般水平。集中趋势用平均数表示，包括算术平均数、几何平均数、调和平均数、中位数、百分位数和众数，前三者为数值平均数，后三者为位置平均数。数值平均数综合了所有数据，受极值影响较大，且不适用于定类和定序数据；位置平均数不受极值的影响，适用于定序数据。要注意，众数不具有唯一性，有时无法计算得到，只有对大量数据计算众数才有意义。\n计算数值平均数的方法：\n\n计算算术平均数: mean()。\n\n计算加权算术平均数: weighted.mean()。\n\n计算几何平均数: psych::geometric.mean()，或pracma::geomean()，或exp(mean(log(x)))。几何平均数受极值的影响较算术平均数小，适用于等比或近似等比关系的数据，计算时要注意负数的影响。\n\n计算调和平均数(倒数的算术平均数的倒数)：psych::harmonic.mean()，或pracma::harmmean()。调和平均数实际应用范围较小，一般用于因缺乏总体单位数而不能直接计算算术平均数的场合，计算时要注意0值的影响。\n\n对于同一组数据，调和平均数≤几何平均数≤算术平均数。\n计算位置平均数的方法：\n\n中位数: median()。中位数是数据排序后居于中间位置的数值，不受两端极值影响，在非正态分布数据中较算术平均数更有代表性。\n\n百分位数: quantile()。百分位数是中位数的扩展，有些数据需要了解其1%、5%、10%、25%、75%、90%、95%、99%、99.99%等位置上的数值，如环境毒理学研究、除尘技术研发等；其中居于25%和75%位置上的数据分别称为下四分位数(Q1)和上四分位数(Q3)。\n\n众数: 众数是数据集中出现频次最高的数值。计算众数首先要排除无法计算的情况(如所有数值出现的频数相等)，然后先计算频数：tbl = as.data.frame(table(x))，再找出最大频数：max_freq = max(tbl[ , 2])，最后找出最大频数对应的数值：mode = tbl[tbl[ , 2] == max_freq, 1]\n\n通过算术平均数、中位数以及众数的关系可以判断数据的大致分布类型(如 图 4.2 所示)：对于理想的对称分布，算术平均数 = 中位数 = 众数；在右偏分布的资料中，算术平均数＞中位数＞众数；在左偏分布的资料中，算术平均数＜中位数＜众数。\n\n\n\n\n\n\n\n\n图 4.2: 通过算术平均数、中位数和众数的关系判断数据分布类型\n\n\n\n\n\n\n例 4.3 计算数值平均数。\n\n\n# 算术平均数\n# 计算废水中铬浓度10次测定结果的算术平均数\ncr_conc &lt;- c(0.92, 0.83, 0.82, 0.91, 1.06, 0.88, 1.07, 0.84, 1.01, 0.96)\nmean(cr_conc)\n\n[1] 0.93\n\n# 分组频数资料加权计算算术平均数\nso2_conc &lt;- c(0.44, 0.48, 0.52, 0.56, 0.60)    # 二氧化硫分组组中值\nf &lt;- c(3, 5, 9, 3, 5)    # 对应频数\nwm &lt;- weighted.mean(so2_conc, f)\nround(wm, 2)\n\n[1] 0.52\n\n# 几何平均数\n# 8个样本中某污物染浓度测定值(近似等比资料)\np_conc &lt;- c(10, 10, 20, 20, 40, 80, 160, 326)\n# 利用对数法计算\nlogpc &lt;- log10(p_conc)\nn &lt;- length(p_conc)\ngm &lt;- 10^(sum(logpc) / n)    # 对数法\nround(gm, 2)\n\n[1] 40.09\n\n# 利用psych包计算\nround(psych::geometric.mean(p_conc), 2)\n\n[1] 40.09\n\n# 调和平均数\n# 4个化工园区废水处理量和处理率，求平均处理率\nwv  &lt;- c(30000, 50000, 25000, 18000)    # 废水处理量\npt &lt;- c(65, 75, 95, 55)    # 废水处理率\nhm &lt;- sum(wv) / sum(wv / (pt / 100))\nround(hm * 100, 2)\n\n[1] 71.57\n\n\n\n例 4.4 计算位置平均数。\n\n\n# 中位数\n# 10个废水BOD浓度测定值\nbod &lt;- c(50, 53.5, 51, 53.5, 52, 55, 58, 53, 56, 53.5)\nmedian(bod)\n\n[1] 53.5\n\n# 计算1%、5%、95% 99%分位数\n# 随机生成颗粒物粒径数据(利用对数正态分布函数)\nset.seed(2022)\np_size &lt;- rlnorm(1000, meanlog = 3, sdlog = 1)\nqp &lt;- quantile(p_size, probs = c(0.01, 0.05, 0.95, 0.99))\nround(qp, 2)\n\n    1%     5%    95%    99% \n  2.16   4.23  99.19 232.50 \n\n# 计算众数\nset.seed(2022)\nbod &lt;- c(50, 53.5, 51, 53.5, 52, 55, 58, 53, 56, 53.5)\ntbl &lt;- as.data.frame(table(bod))    # 统计频数并转换成数据框类型\nmax_freq &lt;- max(tbl[ , 2])    # 找到最大的频数\ntbl[tbl[ , 2] == max_freq, 1]    # 频数最大的数值即众数\n\n[1] 53.5\nLevels: 50 51 52 53 53.5 55 56 58\n\n\n计算总和可用sum()函数，对于矩阵还可用rowSums()、colSums()、rowMeans()和colMeans()分别求其行和列的和与算术平均数，也可利用apply()函数与sum()或mean()结合来计算行与列的和或算术平均数。\n\n例 4.5 计算矩阵行列总和与平均数。\n\n\nmat &lt;- matrix(1:24, nrow = 4, byrow = TRUE)\nmat\n\n     [,1] [,2] [,3] [,4] [,5] [,6]\n[1,]    1    2    3    4    5    6\n[2,]    7    8    9   10   11   12\n[3,]   13   14   15   16   17   18\n[4,]   19   20   21   22   23   24\n\nrowSums(mat)\n\n[1]  21  57  93 129\n\ncolSums(mat)\n\n[1] 40 44 48 52 56 60\n\nrowMeans(mat)\n\n[1]  3.5  9.5 15.5 21.5\n\ncolMeans(mat)\n\n[1] 10 11 12 13 14 15\n\napply(mat, 1, sum)\n\n[1]  21  57  93 129\n\napply(mat, 2, sum)\n\n[1] 40 44 48 52 56 60\n\n\n\n\n4.1.3 离散程度\n离散程度反映数据偏离中心位置的程度，用于描述数据的变异大小。离散程度用变异数表征，变异指标包括极差(亦称全距，range)、四分位间距(interquartile range，IQR)、标准差(standard deviation，SD)、方差(variance，VAR)、标准误(standard error，SE)以及变异系数(coefficient of variation，CV)，其中方差和标准差最为常用。\n\n极差: max(x) – min(x)，即为数据中最大值与最小值之差。仅使用了数据中两个极端观测值，稳定性差。\n\n四分位间距: IQR(x)，即上、下四分位数之差。与极差相比，IQR不受极值影响，具有较好的稳定性，但仍然只使用了数据集中2个观测值。\n\n方差: 总体方差用sum((x - mean(x))^2) / length(x)计算，样本方差用var(x)计算。方差综合考虑了数据中所有观测值的变异，是更有代表性也被经常使用的离散程度指标。\n\n标准差: 由于方差的量纲与原数据不同，故引入标准差，即方差的平方根。总体标准差用sqrt()计算总体方差的平方根，样本标准差直接用sd()计算。\n\n标准误: sd(x) / sqrt(length(x))，即样本均值的标准误差，用于衡量样本均值和总体均值的偏差大小。\n\n变异系数: 相对变异指标，消除了量纲和平均数差异的影响，便于比较不同组数据之间的离散程度。常用的变异系数为标准差变异系数，计算方法为sd(x) / abs(mean(x)) * 100，或直接使用raster::cv()函数。\n\n\n例 4.6 计算各种变异指标。\n\n\nlibrary(readr)\n\n# 读入aqi数据\ndf &lt;- read_csv(\"./data/aqi-data.csv\")\nx &lt;- df$AQI\n\n# 计算极差\nmax(x) - min(x)\n\n[1] 281\n\n# 计算四分位间距\nIQR(x)\n\n[1] 62.5\n\n# 计算样本方差\nvar(x)\n\n[1] 2821.243\n\n# 计算样本标准差\nsd(x)\n\n[1] 53.11537\n\n# 计算标准误\nsd(x) / sqrt(length(x))\n\n[1] 5.598852\n\n# 计算变异系数\nraster::cv(x)\n\n[1] 46.69255\n\nsd(x) / abs(mean(x)) * 100\n\n[1] 46.69255\n\n\n\n\n4.1.4 数据分布特征\n环境数据的统计分析，通常需要对样本的分布做出假设，绝大多数的统计分析方法要求样本分布(近似)为正态分布。样本的分布特征可以用偏度(skewness)和峰度(kurtosis)来描述。\n偏度用于描述数据分布偏斜方向和程度，如果是对称分布(如 图 4.2 中图所示)，则偏度为0；如果是左偏分布(亦称负偏态，分布高峰偏右，如 图 4.2 左图所示)，偏度为负值；如果是右偏分布(亦称正偏态，分布高峰偏左，如 图 4.2 右图所示)，偏度为正值。\n峰度用于表征概率密度曲线在平均值处的峰值高低，也反映了尾部的厚度。当数据分布接近正态分布时，峰度接近于0；当数据分布两侧的极端数据更多时，峰度为负值；当数据分布两侧的极端数据更少时，峰度为正值。\n有很多R包提供了计算偏度和峰度的函数，此处介绍e1071包中偏度和峰度的计算函数skewness()和kurtosis()。e1071包中偏度和峰度的计算均有三个不同的公式，在函数中以type参数指定，参数值为1、2和3，默认type=3。在资料为正态分布时，这三个公式计算的偏度都是无偏估计值，而峰度只有第二个公式的计算结果为无偏估计值。psych包中计算偏度和峰度函数为skew()和kurtosi()，与e1071包的计算方法一致。\n\n例 4.7 计算偏度和峰度。\n\n\ndf &lt;- read_csv(\"./data/aqi-data.csv\")\nx &lt;- df$AQI\n\ne1071::skewness(x, type = 1)\n\n[1] 1.312296\n\ne1071::skewness(x, type = 2)\n\n[1] 1.334644\n\ne1071::skewness(x, type = 3)\n\n[1] 1.290485\n\ne1071::kurtosis(x, type = 1)\n\n[1] 1.765053\n\ne1071::kurtosis(x, type = 2)\n\n[1] 1.936933\n\ne1071::kurtosis(x, type = 3)\n\n[1] 1.659751\n\n# 生成1000个正态数据(样本均值为0，标准差为1)\nset.seed(2022)\ny &lt;- rnorm(1000)\nskewness(y)\n\n[1] 0.07010706\n\nkurtosis(y)\n\n[1] 0.08095659\n\n\n从 例 4.7 的计算结果可以发现，标准正态分布数据的偏度和峰度均接近于0。\nQ-Q图(x轴为理论分位数，y轴为样本数据分位数；符合正态分布时，二者应呈直线关系)和经验分布函数图(比较样本经验分布和理论分布曲线的重合程度)可用来直观地判断数据分布是否符合或近似正态分布。R中qqnorm()和qqline()函数分别用于绘制Q-Q图及其相应直线，ecdf()函数用于计算样本数据的经验分布，再通过plot()绘制出图形，与理论分布函数曲线(pnorm()函数计算理论概率并用lines()函数绘制曲线)进行比较。\n\n例 4.8 绘制 例 4.7 中x和y的Q-Q图(结果如 图 4.3 所示)。\n\n# 偏态数据\ndf &lt;- read_csv(\"./data/aqi-data.csv\")\nx &lt;- df$AQI\nqqnorm(x)\nqqline(x, col = \"red\")\n\n# 正态数据\nset.seed(2022)\ny &lt;- rnorm(100)\nqqnorm(y)\nqqline(y, col = \"red\")\n\n\n\n\n\n\n\n\n\n\n\n(a) 偏态数据的Q-Q图\n\n\n\n\n\n\n\n\n\n\n\n(b) 正态数据的Q-Q图\n\n\n\n\n\n\n\n图 4.3: Q-Q图\n\n\n\n\nlibrary(ggplot2)\nggplot(df, aes(sample = AQI)) + \n  geom_qq(size = 0.6) +\n  geom_qq_line(color = \"red\") +\n  theme_bw()\n\n\n\n\n\n\n\nset.seed(2022)\ndfn &lt;- data.frame(\n  val = rnorm(100)\n)\nggplot(dfn, aes(sample = val)) + \n  geom_qq(size = 0.6) +\n  geom_qq_line(color = \"red\") +\n  theme_bw()\n\n\n\n\n\n\n\n\n由 图 4.3 可见，AQI数据的分位数与正态分布的理论分位数在两端的差异非常明显，表明其分布不符合正态分布；而构造的标准正态数据的样本分位数与理论分位数呈明显的直线关系，表明其符合正态分布。\n\n例 4.9 绘制经验累积分布函数曲线图与理论累积分布函数曲线(结果如 图 4.4 所示，其中红色平滑曲线为理论累积分布密度函数曲线，蓝色为经验累积分布密度函数曲线，即数据实际的累积分布密度)。\n\n# 偏态数据\ndf &lt;- read_csv(\"./data/aqi-data.csv\")\nx &lt;- df$AQI\nplot(ecdf(x), verticals = TRUE, do.p = FALSE, col = \"blue\")\nm &lt;- c(min(x):max(x))\nlines(m, pnorm(m, mean(x), sd(x)), col = \"red\")    # 理论分布\n\n# 正态数据\nset.seed(2022)\ny &lt;- round(rnorm(100, 20, 5))\nplot(ecdf(y), verticals = TRUE, do.p = FALSE, col = \"blue\")\nm &lt;- c(min(y):max(y))\nlines(m, pnorm(m, mean(y), sd(y)), col = \"red\")    # 理论分布\n\n\n\n\n\n\n\n\n\n\n\n(a) 偏态数据\n\n\n\n\n\n\n\n\n\n\n\n(b) 正态数据\n\n\n\n\n\n\n\n图 4.4: 经验累积分布函数曲线\n\n\n\n由 图 4.4 (b) 可见，根据正态分布生成的数据的样本经验分布函数与理论分布函数的曲线基本重合。\n\n\n4.1.5 数据摘要\nR中summary()函数可以提供数据的基本摘要信息，对数值变量，可显示其最小值、最大值、算术平均数、中位数、Q1和Q3以及缺失数。与之相比，psych包提供的describe()函数提供的摘要信息更为全面，对数值变量，除了summary()函数提供的信息外，还包括截断均值、标准差、标准误、极差、四分位间距、绝对中位数偏差、偏度和峰度等信息。skimr包提供的skim()函数强化了对缺失数据的分析，并对数值变量给出频数分布直方图。\n\n例 4.10 比较summary()、describe()和skim()三个数据摘要函数的差异。\n\n\nsummary(airquality)\n\n     Ozone           Solar.R           Wind             Temp      \n Min.   :  1.00   Min.   :  7.0   Min.   : 1.700   Min.   :56.00  \n 1st Qu.: 18.00   1st Qu.:115.8   1st Qu.: 7.400   1st Qu.:72.00  \n Median : 31.50   Median :205.0   Median : 9.700   Median :79.00  \n Mean   : 42.13   Mean   :185.9   Mean   : 9.958   Mean   :77.88  \n 3rd Qu.: 63.25   3rd Qu.:258.8   3rd Qu.:11.500   3rd Qu.:85.00  \n Max.   :168.00   Max.   :334.0   Max.   :20.700   Max.   :97.00  \n NA's   :37       NA's   :7                                       \n     Month            Day      \n Min.   :5.000   Min.   : 1.0  \n 1st Qu.:6.000   1st Qu.: 8.0  \n Median :7.000   Median :16.0  \n Mean   :6.993   Mean   :15.8  \n 3rd Qu.:8.000   3rd Qu.:23.0  \n Max.   :9.000   Max.   :31.0  \n                               \n\npsych::describe(airquality, IQR = TRUE, quant = c(0.25, 0.75))\n\n        vars   n   mean    sd median trimmed   mad  min   max range  skew\nOzone      1 116  42.13 32.99   31.5   37.80 25.95  1.0 168.0   167  1.21\nSolar.R    2 146 185.93 90.06  205.0  190.34 98.59  7.0 334.0   327 -0.42\nWind       3 153   9.96  3.52    9.7    9.87  3.41  1.7  20.7    19  0.34\nTemp       4 153  77.88  9.47   79.0   78.28  8.90 56.0  97.0    41 -0.37\nMonth      5 153   6.99  1.42    7.0    6.99  1.48  5.0   9.0     4  0.00\nDay        6 153  15.80  8.86   16.0   15.80 11.86  1.0  31.0    30  0.00\n        kurtosis   se    IQR  Q0.25  Q0.75\nOzone       1.11 3.06  45.25  18.00  63.25\nSolar.R    -1.00 7.45 143.00 115.75 258.75\nWind        0.03 0.28   4.10   7.40  11.50\nTemp       -0.46 0.77  13.00  72.00  85.00\nMonth      -1.32 0.11   2.00   6.00   8.00\nDay        -1.22 0.72  15.00   8.00  23.00\n\nskimr::skim(airquality)\n\n\nData summary\n\n\nName\nairquality\n\n\nNumber of rows\n153\n\n\nNumber of columns\n6\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n6\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nOzone\n37\n0.76\n42.13\n32.99\n1.0\n18.00\n31.5\n63.25\n168.0\n▇▃▂▁▁\n\n\nSolar.R\n7\n0.95\n185.93\n90.06\n7.0\n115.75\n205.0\n258.75\n334.0\n▅▃▅▇▅\n\n\nWind\n0\n1.00\n9.96\n3.52\n1.7\n7.40\n9.7\n11.50\n20.7\n▂▇▇▃▁\n\n\nTemp\n0\n1.00\n77.88\n9.47\n56.0\n72.00\n79.0\n85.00\n97.0\n▂▃▇▇▃\n\n\nMonth\n0\n1.00\n6.99\n1.42\n5.0\n6.00\n7.0\n8.00\n9.0\n▇▇▇▇▇\n\n\nDay\n0\n1.00\n15.80\n8.86\n1.0\n8.00\n16.0\n23.00\n31.0\n▇▇▇▇▆\n\n\n\n\n\n显然，describe()的摘要信息比summary()更全面，而skim()突出了缺失数据的摘要信息，并提供了简易的变量分布直方图。\n\n练习 4.1 \n(1)用rlnorm()函数生成1000个数据点，用ggplot2绘制频数分布直方图(geom_histogram())，并叠加密度曲线(geom_density())。\n(2)参考 例 4.2 ，将上述数据合理划分为10个组，计算出如 例 4.2 的频数分布表。\n(3)基于上述数据计算各种集中趋势指标和离散趋势指标以及峰度和偏度。\n(4)参考 例 4.10 ，对上述数据进行数据摘要。",
    "crumbs": [
      "进阶知识",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>环境数据统计分析</span>"
    ]
  },
  {
    "objectID": "CH4.html#推断性统计",
    "href": "CH4.html#推断性统计",
    "title": "4  环境数据统计分析",
    "section": "4.2 推断性统计",
    "text": "4.2 推断性统计\n在现实中，收集总体中所有个体信息的成本往往过高甚至无法实现，大多数时候都是采用无偏和随机的科学抽样方法来获取总体的一个子集(样本数据)。推断性统计(亦称抽样推断)则是从样本数据推断总体特征的方法，主要包括参数估计、假设检验和方差分析(ANOVA)等。\n推断性统计由于随机抽样必然导致抽样误差，利用抽样误差的分布规律就可以保障推断性统计结果的可靠性。影响抽样误差的因素包括：样本容量大小、总体变异大小以及抽样组织形式。其他条件相同的情况下，样本容量越大，抽样误差越小；总体变异越大，抽样误差越大；回置抽样比不回置抽样的抽样误差更大。\n\n4.2.1 随机抽样方法\n\n4.2.1.1 简单随机抽样\n简单随机抽样是指对全部个体不做任何分组，从中完全随机地抽取个体组成样本的抽样技术，一般应用于小规模总体。R中sample()函数可以执行回置和不回置的简单随机抽样。sampling是一个更专业的实现各种随机抽样方法的R包，其中srswor()和srswr()函数分别用于执行不回置和回置简单随机抽样。\n\n例 4.11 简单随机抽样示例。\n\n\ndf = data.frame(x = 1:9, y = seq(2, 6, by = 0.5))  # 构造的待抽样总体\nn = 6    # 抽取样本容量\n\nset.seed(2022)\ns1 = sample(1:nrow(df), n, replace = FALSE)  # 不回置抽样\ndf1 = df[s1, ]    # 根据抽取的行号获取数据\ndf1\n\n  x   y\n4 4 3.5\n3 3 3.0\n6 6 4.5\n8 8 5.5\n9 9 6.0\n2 2 2.5\n\nset.seed(2022)\ns2 = sample(1:nrow(df), n, replace = TRUE)  # 回置抽样 \ndf2 = df[s2, ]    # 根据抽取的行号获取数据\ndf2\n\n    x   y\n4   4 3.5\n3   3 3.0\n7   7 5.0\n4.1 4 3.5\n6   6 4.5\n9   9 6.0\n\nlibrary(sampling)\n\nset.seed(2022)\ns3 = srswor(n, nrow(df))  # 不回置抽样\ndf3 = getdata(df, s3)  # 根据抽取的行号获取数据\ndf3\n\n  ID_unit x   y\n2       2 2 2.5\n3       3 3 3.0\n4       4 4 3.5\n6       6 6 4.5\n8       8 8 5.5\n9       9 9 6.0\n\nset.seed(2022)\ns4 = srswr(n, nrow(df))  # 回置抽样\ndf4 = getdata(df, s4)  # 根据抽取的行号获取抽样数据\ndf4\n\n    x   y\n1   1 2.0\n2   2 2.5\n4   4 3.5\n6   6 4.5\n9   9 6.0\n9.1 9 6.0\n\n\n\n\n4.2.1.2 系统抽样\n系统抽样也称等距抽样、机械抽样，是指先将全部个体按一定顺序排列，并根据样本容量确定抽样间隔，然后随机确定抽样起点，再按确定的间隔抽取个体组成样本的抽样技术，主要应用于分布均匀的小规模总体。sampling包中的UPrandomsystmatic()函数用于执行系统抽样。\n\n例 4.12 系统抽样示例。\n\n\ndf = data.frame(x = 1:100, y = round(rnorm(100) * 5 + 2))\nn = 10\nset.seed(2022)\npik = rep(1 / n, nrow(df) / n)    # 概率向量,用于确定抽样概率和计算抽样间隔 \ns5 = UPrandomsystematic(pik)\ndf5 = getdata(df, s5)\ndf5[ , -1]\n\n    x  y\n4   4  3\n14 14 -3\n24 24  4\n34 34  3\n44 44  1\n54 54  8\n64 64  7\n74 74  9\n84 84  9\n94 94  6\n\n\n\n\n4.2.1.3 分层抽样\n分层抽样也称类型抽样，是指将全部总体单位按某些标志分成若干层(类型)，然后在各层中按规定的比例随机抽取单位组成样本的抽样技术。各层抽取的比例(或概率)可以相等也可以不等。分层抽样的优点是抽取样本的代表性比较好，抽样误差比较小，在复杂总体抽样调查中经常被使用。sampling包中的strata()函数用于执行分层抽样。\n\n例 4.13 分层抽样示例。\n\n\ndf = iris\ns6 = strata(df, stratanames = \"Species\", \n             size = c(3, 3, 4), method = \"srswor\")\ndf6 = getdata(df, s6)\ndf6\n\n    Sepal.Length Sepal.Width Petal.Length Petal.Width    Species ID_unit Prob\n7            4.6         3.4          1.4         0.3     setosa       7 0.06\n14           4.3         3.0          1.1         0.1     setosa      14 0.06\n41           5.0         3.5          1.3         0.3     setosa      41 0.06\n72           6.1         2.8          4.0         1.3 versicolor      72 0.06\n79           6.0         2.9          4.5         1.5 versicolor      79 0.06\n98           6.2         2.9          4.3         1.3 versicolor      98 0.06\n101          6.3         3.3          6.0         2.5  virginica     101 0.08\n105          6.5         3.0          5.8         2.2  virginica     105 0.08\n132          7.9         3.8          6.4         2.0  virginica     132 0.08\n149          6.2         3.4          5.4         2.3  virginica     149 0.08\n    Stratum\n7         1\n14        1\n41        1\n72        2\n79        2\n98        2\n101       3\n105       3\n132       3\n149       3\n\n\n以上代码按iris数据集中“Species”变量进行分层(共3层)，并用size参数指定每层抽取的单位数，用method参数指定抽取方法。\n实际操作中，分层抽样方法可以通过人为调整各层抽样比例，来确保单位数较少的层也能被抽出足够数量的单位以组成更有代表性的样本。\n\n\n4.2.1.4 整群抽样和多阶段抽样\n整群抽样是指将全部总体单位按某个标志分群，然后随机抽取群并将群内所有单位组成样本的抽样技术该方法，此即简单整群抽样。如果在抽取的群中再采用各种随机抽样方法来抽取单位组成样本，则称为二阶段整群抽样。整群抽样特别适用于大规模总体。实际工作中，经常采用多阶段抽样技术，综合运用整群抽样、分层抽样以及简单随机抽样等多种抽样技术，以抽取所需要的单位。sampling包中的cluster()函数用于执行整群抽样，mstage()函数用于执行二阶段和多阶段抽样。\n\n例 4.14 简单整群抽样示例。\n\n\n# 简单整群抽样\nset.seed(2022)\ns7 = cluster(df, clustername = \"Species\", size = 1,\n              method = \"srswor\", description = TRUE) \n\nNumber of selected clusters: 1 \nNumber of units in the population and number of selected units: 150 50 \n\ndf7 = getdata(df, s7)\nhead(df7)\n\n    Sepal.Length Sepal.Width Petal.Length Petal.Width   Species ID_unit\n101          6.3         3.3          6.0         2.5 virginica     101\n102          5.8         2.7          5.1         1.9 virginica     102\n103          7.1         3.0          5.9         2.1 virginica     103\n104          6.3         2.9          5.6         1.8 virginica     104\n105          6.5         3.0          5.8         2.2 virginica     105\n106          7.6         3.0          6.6         2.1 virginica     106\n         Prob\n101 0.3333333\n102 0.3333333\n103 0.3333333\n104 0.3333333\n105 0.3333333\n106 0.3333333\n\n\n以上代码根据Species变量从iris数据集中将某一个品种的所有观测值作为一个群而抽出，参数size指定抽取群数，参数method指定抽样方法(“srswor”、“srswr”、“poisson”和”systematic”)；参数description设置抽样时是否显示各群基本信息。如果从该群中再随机抽取10个观测值作为最终的样本，即为二阶段整群抽样：\n\n例 4.15 二阶段抽样示例。\n\n\n# 二阶段整群抽样\ns8 = srswor(10, nrow(df7))  \ndf8 = getdata(df7, s8)\ndf8\n\n    Sepal.Length Sepal.Width Petal.Length Petal.Width   Species ID_unit\n104          6.3         2.9          5.6         1.8 virginica     104\n106          7.6         3.0          6.6         2.1 virginica     106\n107          4.9         2.5          4.5         1.7 virginica     107\n111          6.5         3.2          5.1         2.0 virginica     111\n114          5.7         2.5          5.0         2.0 virginica     114\n128          6.1         3.0          4.9         1.8 virginica     128\n129          6.4         2.8          5.6         2.1 virginica     129\n141          6.7         3.1          5.6         2.4 virginica     141\n146          6.7         3.0          5.2         2.3 virginica     146\n150          5.9         3.0          5.1         1.8 virginica     150\n         Prob\n104 0.3333333\n106 0.3333333\n107 0.3333333\n111 0.3333333\n114 0.3333333\n128 0.3333333\n129 0.3333333\n141 0.3333333\n146 0.3333333\n150 0.3333333\n\n\n对存在多个分类变量的复杂总体，可以直接采用mstage()函数进行多阶段随机整群抽样：\n\n例 4.16 多阶段抽样示例。\n\n\n# 人为构造样本\ndf &lt;- rbind(matrix(rep(\"PA\", 100), 100, 1),\n            matrix(rep(\"PB\", 120), 120, 1),\n            matrix(rep(\"PC\", 150), 150, 1))\ndf = cbind.data.frame(df, c(rep(paste0(\"ca\", 1:5), 20),\n                            rep(paste0(\"cb\", 1:5), 24),\n                            rep(paste0(\"cc\", 1:5), 30)),\n                      runif(370) * 100)\nnames(df) &lt;- c(\"Province\", \"City\", \"SO2\")\ntable(df$Province, df$City)\n\n    \n     ca1 ca2 ca3 ca4 ca5 cb1 cb2 cb3 cb4 cb5 cc1 cc2 cc3 cc4 cc5\n  PA  20  20  20  20  20   0   0   0   0   0   0   0   0   0   0\n  PB   0   0   0   0   0  24  24  24  24  24   0   0   0   0   0\n  PC   0   0   0   0   0   0   0   0   0   0  30  30  30  30  30\n\nset.seed(2022)\nattach(df)\n# 分别对前2个变量进行二阶段随机整群抽样\n# 先从Province中随机抽出2个群，然后从2个群的City中再各自随机抽出2个子群\ns9 = mstage(df, stage = list(\"cluster\", \"cluster\"),\n            varnames = list(\"Province\", \"City\"),\n            size = list(2, c(2, 2)), method = c(\"srswor\", \"srswor\"))\ndetach(df)\ndf9 = getdata(df, s9)[[2]]\ntable(df9$Province, df9$City)\n\n    \n     cb3 cb4 cc3 cc4\n  PB  24  24   0   0\n  PC   0   0  30  30\n\n\n\n\n\n4.2.2 参数估计\n针对总体分布类型已知而参数未知的情况，可以通过样本统计量来估算总体参数，这称之为参数估计。参数估计包括点估计和区间估计。点估计无法反映抽样误差的大小，而区间估计则给出总体参数的一个置信区间(CI)来体现抽样误差的大小，综合考虑了点估计和抽样误差。\n参数估计的原则如下：\n\n无偏性：如果某估计量的所有可能值的平均值，即估计量的数学期望等于相应的总体参数值，则该估计量就是相应总体参数的无偏估计量。\n\n有效性：如果某估计量是若干个无偏估计量中方差最小的，则该估计量就是相应总体参数的更有效的估计量。\n\n一致性：如果某估计量随着样本容量的增大，越来越接近总体参数的真值，则该估计量就是相应总体参数的一致估计量。\n\n充分性：如果某估计量充分利用了样本中有关总体的所有可能信息，则该估计量就是相应总体参数的充分估计量。\n\n区间估计需要给出置信水平，即概率，大多数情况下这个概率取0.95，即保证总体参数落入区间的概率为95%。置信水平一般以1－\\alpha表示，\\alpha称为显著性水平。在给定置信水平下，就可以基于中心极限定理和抽样分布来建立置信区间。\n\n4.2.2.1 点估计\n点估计方法有矩法(moment method)、极大似然法(maximum likelihood method)、最小二乘法(least square method)、贝叶斯法(Bayes method)等，其中矩法和极大似然法最为常用。\n(1)矩法\n矩法估计是基于大数定律的参数估计方法。对于随机变量X，其n阶原点矩定义为E(X^n)，其n阶中心矩定义为E{[X - (X)]^n}。样本x的k阶原点矩A_k = (\\sum{x_i^k})/n是总体k阶原点矩的无偏估计量。对于正态分布，样本均值(1阶原点矩)是总体均值的无偏估计量。但k阶样本中心矩B_k = [\\sum(x_i-\\bar{x})^k]/n是总体k阶中心矩的有偏估计量，B_k^{'} = \\sum(x_i-\\bar{x})^k/(n-1)才是总体k阶中心矩的无偏估计量，在n趋近于无穷大时，B_k与B_k^{'}等价。方差计算基于二阶中心矩，偏度基于三阶中心矩，峰度基于四阶中心矩。 矩法估计时，低阶矩估计的结果更精确。对于参数为λ的指数分布，其均值为1/λ，方差为1/λ^2，则估计λ时，应采用1/\\bar{x}，而不是1/s。\n\n例 4.17 矩法估计指数分布的参数\\lambda。\n\n\nset.seed(2022)\nx = rexp(50, rate = 5)    # 生成50个服从λ=5的指数分布的数据\n\n(ld1 = 1/mean(x))    # 根据1/x ̅估算参数λ\n\n[1] 5.287561\n\n(ld2 = 1/sd(x))    # 根据1/s估算参数λ\n\n[1] 4.527402\n\n\n上述代码中两个估算参数的表达式最外层的英文括号()用于显示表达式计算结果，相当于print()的功能。计算结果表明，基于1/\\bar{x}估算的结果更接近总体参数\\lambda。矩法估计参数时，小样本估计的结果不唯一，波动很大，甚至会跳出参数空间的范围。因此，矩法在足够样本量的情况下，估计结果才能更准确和可靠。\n(2)极大似然法\n极大似然法基于极大似然原理，即概率大的事件在一次观测中更容易发生。极大似然法针对总体分布已知的情况，通过求解待估参数的似然函数得到优良且唯一的估计量。在小样本情况下，极大似然法估计结果比矩法估计结果波动小，相对稳定。 下面以均匀分布来比较矩法和极大似然法估计参数的稳定性。对于均匀分布U(0, \\theta)(\\theta＞0)，其概率密度函数如下：\n\nf(x;\\theta)=\\begin{cases}\\dfrac{1}{\\theta},&0\\le x\\le\\theta\\\\0,&\\text{其他}\\end{cases}\n\\tag{4.1}\n矩法估计参数\\theta的公式为\\hat{\\theta}=2\\bar{x}，而根据似然函数推导出的参数\\theta的极大似然估计公式为\\hat{\\theta}=\\text{max}(x_i)。下面的代码模拟了从\\theta=6的U(0, \\theta)分布中抽取大小为30的100个样本，分别计算矩法和极大似然估计的结果，并绘制图形(如 图 4.5 所示)，以直观比较。\n\n例 4.18 矩法和极大似然法利用小样本估计均匀分布参数\\theta的稳定性。\n\n\nn = 30; theta = 6; ul = 8; dl = 4; times = 100\nme = mle = rep(0, times)\nfor(i in 1:times){\n  x = runif(n, 0, theta)    # 随机生成样本\n  me[i] = 2 * mean(x)\n  mle[i] = max(x)\n}\n# 绘制图形\nplot(1:times, me, type = \"l\", xlab = \"样本序号\", ylab = \"参数估计值\",\n     ylim = c(4, 8), lty = 3, col = \"blue\")\nlines(1:times, mle, lty = 1, col = \"red\")\nabline(h = 6, lwd = 2)\nabline(h = 5, lty = 3)\nabline(h = 7, lty = 3)\n\n\n\n\n\n\n\n图 4.5: 比较矩法估计和极大似然估计\n\n\n\n\n\n从 图 4.5 可见，小样本情况下，极大似然法估计值比矩法更稳定，但存在低估的情况。\n\n\n4.2.2.2 区间估计\n区间估计是在点估计的基础上，根据抽样分布原理，按一定的概率估计总体参数落在哪个范围，这个范围称为总体参数的置信区间。对总体参数\\theta估计其取值范围，对于给定的小概率\\alpha，有P(\\theta_1&lt;\\theta&lt;\\theta_2)=1-\\alpha，(\\theta_1,\\theta_2)是参数\\theta的置信区间，\\theta_1、\\theta_2分别是参数的置信下限和上限，置信区间两端的区域称为舍弃域，\\alpha为显著性水平，1-\\alpha为区间估计的置信概率，也称为置信水平。置信水平的含义是指由全部样本统计量所确定的所有置信区间中，有100(1-\\alpha)%的置信区间包括了总体参数\\theta，另外有100\\alpha%的置信区间没有包括总体参数\\theta。在实际应用中，\\alpha一般取0.01或0.05，如无特别指出，\\alpha一般取0.05。\n\n\n\n\n\n\n\n\n图 4.6: 置信区间与舍弃域\n\n\n\n\n\n置信区间的宽度(\\theta_2-\\theta_1)反映了参数估计的不确定性，越宽则不确定性越大，精确度越小，但参数落入该区间的几率越大。置信水平越大，置信区间越宽；样本容量增大，置信区间收窄。\n(1)单个正态总体均值\\mu的区间估计\n当总体\\sigma^2已知时，基于正态分布统计量估计其均值的双侧和单侧置信区间的计算公式如下：\n\n  \\begin{gathered}\n  \\left(\\bar{x}-\\frac\\sigma{\\sqrt{n}}|z_{\\alpha/2}|,\\bar{x}+\\frac\\sigma{\\sqrt{n}}|z_{\\alpha/2}|\\right)\\\\\n  \\left(-\\infty,\\bar{x}+\\frac\\sigma{\\sqrt{n}}|z_{\\alpha}|\\right)\\\\\n  \\left(\\bar{x}-\\frac\\sigma{\\sqrt{n}}|z_{\\alpha}|,+\\infty\\right)\n  \\end{gathered}\n\\tag{4.2}\n利用qnorm()函数计算统计量z的值，如求累积概率为0.025处的z值：qnorm(0.025)。R包BSDA中的z.test()函数可直接给出该情况下的区间估计和假设检验结果。\n当总体\\sigma^2未知时，基于t分布统计量估计其均值的双侧和单侧置信区间计算公式如下：\n\n\\begin{gathered}\n  (\\bar{x}-\\frac{s}{\\sqrt{n}}|t_{\\alpha/2,n-1}|,\\bar{x}+\\frac{s}{\\sqrt{n}}|t_{\\alpha/2,n-1}|\\\\\n  (-\\infty,\\bar{x}+\\frac{s}{\\sqrt{n}}|t_{\\alpha,n-1}|) \\\\\n  (\\bar{x}-\\frac{s}{\\sqrt{n}}|t_{\\alpha,n-1}|,+\\infty)\n\\end{gathered}\n\\tag{4.3}\n利用qt()函数计算统计量t的值，如求自由度为9、累积概率为0.025处的t值：qt(0.025, 9)。t.test()函数可以同时给出区间估计和假设检验结果。\n\n例 4.19 为测定某湖泊水Hg污染情况，从该湖中随机取9条鱼龄相近的鱼，测得鱼胸肌中Hg含量分别1.921、2.024、2.046、2.032、1.982、1.889、2.136、1.983、2.146(mg/kg)，试估计该湖泊鱼胸肌中Hg含量的95%和99%置信区间。\n\n解：n = 9，小样本，按t分布计算；先计算t_{0.05/2,8}和t_{0.01/2,8}，然后按公式计算CI：\n\nt1 = qt(0.975,8); t2 = qt(0.995,8)\nx = c(1.921,2.024,2.046,2.032,1.982,1.889,2.136,1.983,2.146)\nxt1 = t1 * sd(x) / sqrt(length(x))\nxt2 = t2 * sd(x) / sqrt(length(x))\ncat(\"95%CI: \", mean(x) - xt1, mean(x) + xt1, '\\n')\n\n95%CI:  1.951157 2.084176 \n\ncat(\"99%CI: \", mean(x) - xt2, mean(x) + xt2, '\\n')\n\n99%CI:  1.920891 2.114443 \n\n\n直接采用t.test()函数计算：\n\nt.test(x, conf.level = 0.95)$conf.int\n\n[1] 1.951157 2.084176\nattr(,\"conf.level\")\n[1] 0.95\n\nt.test(x, conf.level = 0.99)$conf.int\n\n[1] 1.920891 2.114443\nattr(,\"conf.level\")\n[1] 0.99\n\n\n(2)单个正态总体方差\\sigma^2的区间估计\n当总体均值\\mu已知时，基于\\chi^2分布统计量估计其方差\\sigma^2的双侧和单侧置信区间：\n\\begin{gathered}\n  (\\frac{\\sum_{i=1}^{n}(x_{i}-\\mu)^{2}}{\\chi_{\\alpha/2,n}^{2}},\\frac{\\sum_{i=1}^{n}(x_{i}-\\mu)^2}{\\chi_{1-\\alpha/2,n}^2}\\\\\n  (0,\\frac{\\sum_{i=1}^{n}(x_{i}-\\mu)^{2}}{\\chi_{1-\\alpha,n}^2}) \\\\\n  (\\frac{\\sum_{i=1}^{n}(x_{i}-\\mu)^2}{\\chi_{\\alpha/2,n}^2},+\\infty)\n\\end{gathered}\n\\tag{4.4}\n当总体均值\\mu未知时，基于\\chi^2分布统计量估计其方差\\sigma^2的双侧和单侧置信区间：\n\n  \\left(\\frac{(n-1)s^2}{\\chi_{(\\alpha/2,n-1)}^2},\\frac{(n-1)s^2}{\\chi_{(1-\\alpha/2,n-1)}^2}\\right)\\\\\n  \\left(0,\\frac{(n-1)s^2}{\\chi_{(1-\\alpha/2,n-1)}^2}\\right)\\\\\n  \\left(\\frac{(n-1)s^2}{\\chi_{(\\alpha/2,n-1)}^2},+\\infty\\right)\\\\\n\\tag{4.5}\nR利用qchisq()函数计算统计量\\sigma^2的值，如求自由度为9、累积概率为0.025出的\\sigma^2值：\n\nqchisq(0.025, 9)\n\n[1] 2.700389\n\n\n(3)两个正态总体均值差的区间估计\n当两个总体的方差\\sigma_1^2、\\sigma_2^2均已知时，基于正态分布统计量估计均值差的双侧和单侧置信区间：\n\n\\left(\\bar{x}_1-\\bar{x}_2-\\left|z_{\\alpha/2}\\right|\\sqrt{\\frac{\\sigma_1^2}{n_1}+\\frac{\\sigma_2^2}{n_2}},\\bar{x}_1-\\bar{x}_2+\\left|z_{\\alpha/2}\\right|\\sqrt{\\frac{\\sigma_1^2}{n_1}+\\frac{\\sigma_2^2}{n_2}}\\right)\\\\\n\\left(-\\infty,\\bar{x}_1-\\bar{x}_2+\\left|z_{\\alpha}\\right|\\sqrt{\\frac{\\sigma_1^2}{n_1}+\\frac{\\sigma_2^2}{n_2}}\\right)\\\\\n\\left(\\bar{x}_1-\\bar{x}_2-\\left|z_{\\alpha}\\right|\\sqrt{\\frac{\\sigma_1^2}{n_1}+\\frac{\\sigma_2^2}{n_2}},+\\infty\\right)\\\\\n\\tag{4.6}\n当两个总体的方差\\sigma_1^2、\\sigma_2^2均未知且n_1 \\neq n_2但n_1 、n_2均足够大(≥50)时，基于正态分布统计量估计均值差的双侧和单侧置信区间：\n\n\\begin{gathered}\n  (\\bar{x}_1-\\bar{x}_2-|z_{\\alpha/2}|\\sqrt{\\frac{s_1^2}{n_1}+\\frac{s_2^2}{n_2}},\\bar{x}_1-\\bar{x}_2+|z_{\\alpha/2}|\\sqrt{\\frac{s_1^2}{n_1}+\\frac{s_2^2}{n_2}}) \\\\\n  (-\\infty,\\bar{x}_1-\\bar{x}_2+|z_\\alpha|\\sqrt{\\frac{s_1^2}{n_1}+\\frac{s_2^2}{n_2}}) \\\\\n  (\\bar{x}_{1}-\\bar{x}_{2}-|z_{\\alpha}|\\sqrt{\\frac{s_1^2}{n_1}+\\frac{s_2^2}{n_2}},+\\infty)\n\\end{gathered}\n\\tag{4.7}\n以上两种情况下的总体均值差的区间估计可直接使用R包BSDA中的z.test()函数。\n当两个总体的方差\\sigma_1^2、\\sigma_2^2均未知但相等时，基于t分布统计量估计均值差的双侧和单侧置信区间：\n\n\\begin{gathered}\n  (\\bar{x}_1-\\bar{x}_2-s_w|t_{\\alpha/2,n_1+n_2-2}|\\sqrt{\\frac{1}{n_1}+\\frac{1}{n_2}},\\bar{x}_1-\\bar{x}_2+s_{w}|t_{\\alpha/2,n_1+n_2-2}|\\sqrt{\\frac{1}{n_1}+\\frac{1}{n_2}}\\\\\n  (-\\infty,\\bar{x}_1-\\bar{x}_2+s_w|t_{(\\alpha,n_1+n_2-2)}|\\sqrt{\\frac{1}{n_1}+\\frac{1}{n_2}}\\\\\n  (\\bar{x}_1-\\bar{x}_2-s_w|t_{(a,n_1+n_2-2)}|\\sqrt{\\frac{1}{n_1}+\\frac{1}{n_2}},+\\infty\\\\\n\\end{gathered}\n\\tag{4.8}\n其中s_w的计算公式为：\n\ns_w=\\sqrt{\\frac{(n_1-1)s_1^2+(n_2-1)s_2^2}{n_1+n_2-2}}\n\\tag{4.9}\n以上此情况下的均值差区间估计和假设检验，可执行t.test(x, y, var.equal = TRUE)。\n当两个总体的方差\\sigma_1^2、\\sigma_2^2均未知但n_1=n_2=n时，基于t分布统计量估计均值差的双侧和单侧置信区间：\n\n\\begin{gathered}\n  (\\bar{x}_1-\\bar{x}_2-|t_{\\alpha/2,n-1}|\\frac{s_z}{\\sqrt{n}},\\bar{x}_1-\\bar{x}_2+|t_{\\alpha/2,n-1}|\\frac{s_z}{\\sqrt{n}})\\\\\n  (-\\infty,\\bar{x_1}-\\bar{x_2}+|t_{\\alpha,n-1}|\\frac{s_z}{\\sqrt{n}})\\\\\n  (\\bar{x}_1-\\bar{x}_2-|t_{\\alpha,n-1}|\\frac{s_z}{\\sqrt{n}},+\\infty)\n\\end{gathered}\n\\tag{4.10}\n其中s_z的计算公式为:\n\n\\begin{gathered}\n  s_{z}=\\sqrt{\\frac{\\sum_{i=1}^{n}[(x_{1,i}-\\bar{x}_1)-(x_{2,i}-\\bar{x}_2)]^2}{n-1}}\n\\end{gathered}\n\\tag{4.11}\n以上情况下的均值差区间估计和假设检验，可执行t.test(x, y, paired = TRUE)。\n当两个总体的方差\\sigma_1^2、\\sigma_2^2均未知且不相等时，基于t分布统计量估计均值差的双侧和单侧置信区间：\n\n\\begin{gathered}\n  \\left(\\bar{x}_{1}-\\bar{x}_{2}-|t_{(\\alpha/2,\\nu)}|\\sqrt{\\frac{s_{1}^{2}}{n_{1}}+\\frac{s_{2}^{2}}{n_{2}}},\\bar{x}_{1}-\\bar{x}_{2}+|t_{(\\alpha/2,\\nu)}|\\sqrt{\\frac{s_{1}^{2}}{n_{1}}+\\frac{s_{2}^{2}}{n_{2}}}\\right) \\\\\n  \\left(-\\infty\\text{,}\\bar{x}_{1}-\\bar{x}_{2}+|t_{(\\alpha,\\nu)}|\\sqrt{\\frac{s_{1}^{2}}{n_{1}}+\\frac{s_{2}^{2}}{n_{2}}}\\right) \\\\\n  \\left(\\bar{x}_{1}-\\bar{x}_{2}-|t_{(\\alpha,\\nu)}|\\sqrt{\\frac{s_{1}^{2}}{n_{1}}+\\frac{s_{2}^{2}}{n_{2}}},+\\infty\\right)\n\\end{gathered}\n\\tag{4.12}\n其中\\nu的计算公式为:\n\n\\begin{gathered}\n  \\nu=\\frac{({s_1^2}/{n_1}+{s_2^2}/{n_2})^2}{{s_1^4}/[{n_1^2(n_1-1)}]+{s_2^4}/[{n_2^2(n_2-1)}]}\n\\end{gathered}\n\\tag{4.13}\n以上情况下的均值差区间估计和假设检验，直接执行t.test(x, y)。\n(4)两个正态总体方差比的区间估计\n当两个总体的均值\\mu_1、\\mu_2已知时，基于F分布估计二者方差比\\sigma_1^2/\\sigma_2^2的双侧和单侧置信区间如下：\n\n\\begin{gathered}\n  \\left(\\frac{n_2\\sum_{i=1}^{n_1}(x_{1,i}-\\mu_1)^2}{n_1\\sum_{i=1}^{n_2}(x_{2,i}-\\mu_2)^2}F_{1-\\alpha/2,n,m},\\frac{n_2\\sum_{i=1}^{n_1}(x_{1,i}-\\mu_1)^2}{n_1\\sum_{i=1}^{n_2}(x_{2,i}-\\mu_2)^2}F_{\\alpha/2,n,m}\\right)\\\\\n  \\left(0,\\frac{n_2\\sum_{i=1}^{n_1}(x_{1,i}-\\mu_1)^2}{n_1\\sum_{i=1}^{n_2}(x_{2,i}-\\mu_2)^2}F_{\\alpha,n,m}\\right)\\\\\n  \\left(\\frac{n_2\\sum_{i=1}^{n_1}(x_{1,i}-\\mu_1)^2}{n_1\\sum_{i=1}^{n_2}(x_{2,i}-\\mu_2)^2}F_{1-\\alpha,n,m},+\\infty\\right)\n\\end{gathered}\n\\tag{4.14}\n当两个总体的均值\\mu_1、\\mu_2未知时，基于F分布估计二者方差比\\sigma_1^2/\\sigma_2^2的双侧和单侧置信区间如下：\n\n\\begin{gathered}\n\\left(\\frac{s_1^2}{s_2^2}F_{(1-\\alpha/2,n-1,m-1)},\\frac{s_1^2}{s_2^2}F_{(\\alpha/2,n-1,m-1)}\\right) \\\\\n\\left(0\\text{,}\\frac{s_1^2}{s_2^2}F_{(\\alpha,n-1,m-1)}\\right) \\\\\n\\left(\\frac{s_1^2}{s_2^2}F_{(1-\\alpha,n-1,m-1)},+\\infty\\right)\n\\end{gathered}\n\\tag{4.15}\n此情况下，直接使用var.test()函数估计方差比的置信区间并进行假设检验。\n(5)总体比率的区间估计\n当样本量n较大(np＞25)时，可基于正态分布统计量估计单个总体比率p的置信区间：\n\n\\begin{gathered}\n  \\left(p-|z_{\\alpha/2}|\\sqrt{\\frac{p(1-p)}n},p+|z_{\\alpha/2}|\\sqrt{\\frac{p(1-p)}n}\\right) \\\\\n  \\left(0,p+|z_\\alpha|\\sqrt{\\frac{p(1-p)}n}\\right) \\\\\n  \\left(p-|z_\\alpha|\\sqrt{\\frac{p(1-p)}n},1\\right)\n\\end{gathered}\n\\tag{4.16}\n当样本量n较小时，可基于F分布统计量估计单个总体比率p的置信区间：\n\n\\begin{gathered}\n  \\left(\\frac{2npF_{1-\\alpha/2,2np,2n-2np+2}}{2n-2np+2+2npF_{1-\\alpha/2,2np,2n-2np+2}},\\frac{(2np+2)F_{\\alpha/2,2np+2,2n-2np}}{2n-2np+(2np+2)F_{\\alpha/2,2np+2,2n-2np}}\\right)\\\\\n  \\left(0,\\frac{(2np+2)F_{\\alpha/2,2np+2,2n-2np}}{2n-2np+(2np+2)F_{\\alpha,2np+2,2n-2np}}\\right)\\\\\n  \\left(\\frac{2npF_{1-\\alpha/2,2np,2n-2np+2}}{2n-2np+2+2npF_{1-\\alpha,2np,2n-2np+2}},1\\right)\n\\end{gathered}\n\\tag{4.17}\n函数binom.test()可用于进行精确估计总体比率的置信区间，也可用prop.test()函数(默认进行连续性修正，即参数correct = TRUE)进行近似估计。\n\n例 4.20 测定某工厂废水样本63个，发现其中3个样本污染物A的浓度超标。试估计污染物A总体超标率的90%置信区间。\n\n\n# 公式计算\nn = 63\nnp = 3\na = 0.1\nnu1 = 2 * np\nnu2 = 2 * n - 2 * np + 2\nnu3 = 2 * np + 2\nnu4 = 2 * n - 2 * np\nq1 = qf(a / 2, nu1, nu2)\nq2 = qf(1 - a / 2, nu3, nu4)\nci = c(nu1 * q1 / (nu2 + nu1 * q1), nu3 * q2 / (nu4 + nu3 * q2))\nci\n\n[1] 0.01310334 0.11849878\n\n# binom.test函数计算\nbinom.test(x = np, n = n, conf.level = 0.90)\n\n\n    Exact binomial test\n\ndata:  np and n\nnumber of successes = 3, number of trials = 63, p-value = 9.048e-15\nalternative hypothesis: true probability of success is not equal to 0.5\n90 percent confidence interval:\n 0.01310334 0.11849878\nsample estimates:\nprobability of success \n            0.04761905 \n\n# prop.test函数计算\nprop.test(x = np, n = n, conf.level = 0.90)\n\n\n    1-sample proportions test with continuity correction\n\ndata:  np out of n, null probability 0.5\nX-squared = 49.778, df = 1, p-value = 1.722e-12\nalternative hypothesis: true p is not equal to 0.5\n90 percent confidence interval:\n 0.01472307 0.12381063\nsample estimates:\n         p \n0.04761905 \n\n # 不使用连续性修正\nprop.test(x = np, n = n, conf.level = 0.90, correct = FALSE)   \n\n\n    1-sample proportions test without continuity correction\n\ndata:  np out of n, null probability 0.5\nX-squared = 51.571, df = 1, p-value = 6.904e-13\nalternative hypothesis: true p is not equal to 0.5\n90 percent confidence interval:\n 0.01918907 0.11330422\nsample estimates:\n         p \n0.04761905 \n\n\n\n\n4.2.2.3 自举采样估计\n当总体分布未知，仅有一个样本且样本量n较小时，可通过自举采样(bootstrap sampling)来估计总体参数的置信区间。在机器学习建模中，自举采样法也有重要的应用，例如集成学习算法bagging的核心思想就是自举采样方法。\n自举采样是现代统计学中一种针对小样本的重采样技术，即以已有的小样本为“总体”，从中反复回置抽取等量的自举样本。“自举采样的样本统计量围绕原始样本统计量的变化”是“原始样本统计量围绕总体参数的变化”的一个很好的近似。与传统参数估计方法需要总体为正态分布的假设相比，自举采样法仅仅通过已有样本数据而不对总体的分布做任何假设。\n自举采样法的特点是：采取回置抽样，每次抽样均以相同概率抽取单位；自举样本容量与原始样本容量相同。\n利用自举抽样方法估计总体参数的步骤如下：\n(1)通过回置抽样从已知样本抽取N个与已知样本容量大小n相同的自举样本；\n(2)计算每个自举样本的待估参数的函数值；\n(3)根据置信概率α求取N个函数值的上下分位数。\n\n例 4.21 利用自举抽样法求解 例 4.19 。\n\n\nx = c(1.921, 2.024, 2.046, 2.032, 1.982, 1.889, 2.136, 1.983, 2.146)\nN = 5000; n = length(x)\nmat = rep(NA, N)\nfor(i in 1:N){\n  mat[i] = mean(sample(x, n, replace = TRUE))\n}\n# 95%置信区间\nquantile(mat, probs = c(0.025, 0.975))\n\n    2.5%    97.5% \n1.963994 2.070000 \n\n# 99%置信区间\nquantile(mat, probs = c(0.005, 0.995))\n\n    0.5%    99.5% \n1.950218 2.084115 \n\n\nR包boot中的boot.ci()函数提供了自举抽样法估计参数置信区间的更多功能。\n\n例 4.22 利用boot.ci()函数估计 例 4.19 的置信区间：\n\n\nlibrary(boot)\nx = data.frame(conc = c(1.921, 2.024, 2.046, 2.032, 1.982,\n                        1.889, 2.136, 1.983, 2.146))\nmeanx = function(x, ind) mean(x[ind, ])\nx.boot = boot(x, meanx, R = 5000, stype = \"i\")\nboot.ci(x.boot, conf = c(0.95, 0.99), type = c(\"basic\", \"norm\", \"perc\", \"bca\"))\n\nBOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS\nBased on 5000 bootstrap replicates\n\nCALL : \nboot.ci(boot.out = x.boot, conf = c(0.95, 0.99), type = c(\"basic\", \n    \"norm\", \"perc\", \"bca\"))\n\nIntervals : \nLevel      Normal              Basic         \n95%   ( 1.965,  2.071 )   ( 1.966,  2.071 )   \n99%   ( 1.949,  2.088 )   ( 1.950,  2.087 )  \n\nLevel     Percentile            BCa          \n95%   ( 1.964,  2.069 )   ( 1.966,  2.071 )   \n99%   ( 1.949,  2.085 )   ( 1.951,  2.087 )  \nCalculations and Intervals on Original Scale\n\n\nboot.ci()函数可生成5种不同类型的双侧等尾非参数置信区间：一阶正态近似(Normal)、基本自举区间(Basic)、学生化自举区间(Studentized)、自举百分位数区间(Percentile)和校正自举百分位数区间(BCa)，可通过type参数指定置信区间估算方法。\n\n练习 4.2 \n(1)重复10次测定某废水中CODMn含量分别为436.5、430.3、442.8、438.9、429.5、440.7、439.6、431.4、433.2、435.4mg/L，基于t分布和自举抽样法估计其均值的95%和99%置信区间。\n(2)基于 例 4.19 ，按同样方法计算95%置信区间。\n\n\n\n\n4.2.3 假设检验\n假设检验是用来判断造成样本与样本、样本与总体之间差异的原因是抽样误差还是本质差别的一种统计推断方法。显著性检验是最常用的一种假设检验方法，其基本原理是先对总体参数做出某种假设，然后基于反证法思想，利用样本信息对假设做出接受或拒绝的决策。假设检验是一种重要的推断性统计分析方法，基于“小概率事件”原理，即小概率事件在一次试验或观测中基本上不会发生。常用的假设检验方法分为参数检验和非参数检验，前者需要总体分布已知，而后者不需要知道总体分布类型。\n\n4.2.3.1 假设检验的步骤\n(1)根据问题提出假设\n根据实际问题，一般提出两个对立的假设：检验假设(null hypothesis，亦称零假设或原假设)和备择假设(alternative hypothesis)。检验假设通常是统计者想要拒绝的假设，用H0表示，双侧检验用=，右侧检验用≥，左侧检验用≤。备择假设通常是统计者想要接受的假设，用H1表示，双侧检验用≠，右侧检验用&gt;，左侧检验用&lt;。\n假设检验有可能发生两种错误：弃真错误(第I类错误)和取伪错误(第II类错误)。弃真错误是H0为真但被拒绝，其概率记为\\alpha，亦即检验水准或显著性水平，人为设定从而可以控制其大小。取伪错误是H0为假但被接受，其概率记为\\beta，在显著性检验(仅控制弃真错误的概率)中不受控制。显然，应当将犯错后果最严重的假设作为检验假设，因为通过控制α可以来降低犯弃真错误的概率。\n(2)确定检验水准\n检验水准\\alpha是指当检验假设为真时，检验统计量落在拒绝域的概率，亦即犯弃真错误的概率。\\alpha的取值一般为0.01、0.05、0.1等。在样本容量n一定时，若减小弃真错误的概率，则会增大取伪错误的概率，增大样本容量可以同时降低两类错误的概率。\n拒绝域是指由显著性水平围成的区域，也称否定域。拒绝域之外称为接受域。如果检验统计量落入拒绝域，则拒绝检验假设，否则接受检验假设。\n左侧检验、双侧检验和右侧检验的拒绝域和接受域如 图 4.7 所示。\n\n\n\n\n\n\n\n\n图 4.7: 三种检验方式下的拒绝域和接受域\n\n\n\n\n\n(3)选择检验方法并计算相应检验统计量\n对不同类型的资料和不同的分析目的，选用适当的检验方法，并计算相应的检验统计量。\n(4)确定界值或计算p值\n根据设定的检验水准，计算拒绝域界值，或根据检验统计量计算或查表确定其对应的p值。\n(5)做出推断结论\n比较\\alpha对应的界值和检验统计量对应的界值或p值与\\alpha的关系(参照 图 4.7 )来判断样本是否落入拒绝域，从而做出拒绝或接受H0的决策。\nR中假设检验结果的显著性水平可根据星号(*)的有无和个数来判断：三个星号(***)表示p ≤ 0.001，两个星号(**)表示0.001＜ p ≤ 0.01，一个星号(*)表示0.01＜ p ≤ 0.05。\n\n\n4.2.3.2 检验功效及其计算\n检验功效或统计功效(statistical power)，也称为敏感性(sensitivity)，是指备择假设为真时拒绝检验假设的概率(1－\\beta)，即甄别本质差异的能力(如 图 4.8 所示)。在样本容量一定的前提下，\\alpha越大，\\beta越小，1－\\beta越大；样本容量n越大，方差s^2越小，则\\beta越小，1－\\beta越大。\n\n\n\n\n\n\n\n\n图 4.8: α、1－α、β和1－β的关系\n\n\n\n\n\n在环境调查和实验中，重视检验水准\\alpha而忽视检验功效1－\\beta是最容易犯的错误，这会导致获取的样本量不足以甄别本质差异，从而增大犯取伪错误的概率。power.t.test()函数可计算用于t检验的样本特征：样本容量、效应值、检验水准、检验功效和总体标准差，提供其中任意四个参数值以推断余下一个参数值。\n\n例 4.23 鱼肉PCB安全水平 ≤ 0.05mg/kg。如果真正浓度接近0.2 mg/kg，此时需要以较高概率甄别这种差异。如果现在有12个样本用于分析PCB浓度，如何估计识别平均浓度在危险水平0.2 mg/kg上的检验功效？反之，如果设定一个较高的检验功效(如0.85)，如何计算所需的样本容量？\n\n\n解：假设\\sigma=0.5，\\alpha=0.05，计算检验功效1－\\beta：\n\nn = 12\nd = 0.15\na = 0.05\ns = 0.5\npower.t.test(n = n, delta = d, sd = s, \n             sig.level = a, \n             type = \"one.sample\",\n             alternative = \"one.sided\")\n\n\n     One-sample t test power calculation \n\n              n = 12\n          delta = 0.15\n             sd = 0.5\n      sig.level = 0.05\n          power = 0.2517193\n    alternative = one.sided\n\n\n结果表明检验功效只有0.25。\n若希望检验功效达到0.85，计算结果显示样本容量需要82个：\n\npower.t.test(power = 0.85, delta = d, sd = s, \n             sig.level = a,\n             type = \"one.sample\", \n             alternative = \"one.sided\")\n\n\n     One-sample t test power calculation \n\n              n = 81.25206\n          delta = 0.15\n             sd = 0.5\n      sig.level = 0.05\n          power = 0.85\n    alternative = one.sided\n\n\npower.prop.test()函数用于双样本比例假设检验的检验功效计算，power.anova.test()函数用于平衡单因素方差分析的检验功效计算。R包pwr提供了更多的功效分析功能(如 表 4.1 所示)。\n\n\n\n\n表 4.1: pwr包提供的功效分析函数\n\n\n\n\n\n\n\n\n\n\n函数\n功能\n\n\n\n\npwr.2p.test\n计算n相等的两比率检验功效(与power.prop.test相同)\n\n\npwr.2p2n.test\n计算n不相等的两比率检验功效\n\n\npwr.anova.test\n计算平衡单因素方差分析的检验功效(与power.anova.test相同)\n\n\npwr.chisq.test\n计算χ2检验功效\n\n\npwr.f2.test\n计算广义线性模型的检验功效\n\n\npwr.norm.test\n计算方差已知的正态分布均值的检验功效\n\n\npwr.p.test\n计算单样本比率的检验功效\n\n\npwr.r.test\n计算相关系数的检验功效\n\n\npwr.t.test\n计算均值t检验的功效(单样本、两样本、配对样本)\n\n\npwr.t2n.test\n计算n不等的双样本均值t检验的功效\n\n\n\n\n\n\n\n\n\n\n4.2.3.3 单正态总体均值与方差的假设检验\n(1)单正态总体均值的假设检验\n\n当总体方差\\sigma^2已知时,根据以下公式计算z统计量:\n\n  z=\\frac{\\bar{x}-\\mu_0}{\\sigma/\\sqrt{n}}\n\\tag{4.18}\n此种情况下，直接用BSDA包中z.test()函数检验样本均值与总体均值的差异显著性，或采用qnorm()函数计算拒绝域界值，再与统计量z进行比较以做出判断。双侧检验的拒绝域为|z|≥z_{\\alpha/2}，左侧检验的拒绝域为z≥-z_{\\alpha}，右侧检验的拒绝域为z≤z_{\\alpha}。\n当总体方差\\sigma^2未知时,根据以下公式计算t统计量:\n\n  t=\\frac{\\bar{x}-\\mu_0}{s/\\sqrt{n}}\n\\tag{4.19}\n此种情况下，直接利用t.test()函数检验样本均值与总体均值的差异显著性，或采用qt()函数计算拒绝域界值，再与统计量t进行比较以做出判断。双侧检验的拒绝域为|t|≥t_{\\alpha/2,n-1}，左侧检验的拒绝域为t≥-t_{\\alpha,n-1}，右侧检验的拒绝域为t≤t_{\\alpha,n-1}。\n(2)单正态总体方差的假设检验\n当总体均值\\mu已知时,根据以下公式计算c统计量:\n\n  c=\\frac{\\sum_{i=1}^{n}(x_i-\\mu)^2}{\\sigma_0^2}\n\\tag{4.20}\n此时，先用qchisq()函数计算拒绝域界值，再与统计量c进行比较以做出判断。双侧检验的拒绝域为c≤\\chi_{\\alpha/2,n}^2或c≥\\chi_{1-\\alpha/2,n}^2，左侧检验的拒绝域为c≥\\chi_{1-\\alpha,n}^2，右侧检验的拒绝域为c≤\\chi_{\\alpha,n}^2。\n当总体均值\\mu未知时,根据以下公式计算c统计量:\n\n  c=\\frac{\\sum_{i=1}^{n}(x_i-\\bar{x})^2}{\\sigma_0^2}\n\\tag{4.21}\n此时，先用qchisq()函数计算拒绝域界值，再与统计量c进行比较以做出判断。双侧检验的拒绝域为c≤\\chi_{\\alpha/2,n-1}^2或c≥\\chi_{1-\\alpha/2,n-1}^2，左侧检验的拒绝域为c≥\\chi_{1-\\alpha,n-1}^2，右侧检验的拒绝域为c≤\\chi_{\\alpha,n-1}^2。\n\n例 4.24 在新建金属冶炼厂周围随机取10个土样，测镉含量平均值为0.212mg/kg，标准差为0.130mg/kg。已知该地以往土壤中镉背景值为0.057mg/kg。试分析目前土壤中镉含量是否较以往为高？\n\n解：建立假设：H0：\\mu≤0.057，H1：\\mu＞0.057；\n右侧检验，总体方差未知，n&lt;30，根据t分布进行检验；计算统计量：\n\n\nn = 10; x = 0.212; s = 0.130; m = 0.057\n(t = (x - m) / (s / sqrt(n)))\n\n[1] 3.770408\n\n\n确定p值：自由度\\nu=n-1=9，用qt()函数计算1-\\alpha=0.95、1-\\alpha=0.99时的界值：\n\n\nqt(0.95, df = n - 1)\n\n[1] 1.833113\n\nqt(0.99, df = n - 1) \n\n[1] 2.821438\n\n\n得出结论：比较t统计量与拒绝域界值，拒绝H0，差异具有极显著性，认为目前土壤中镉含量比以往高。\n如果给出了样本具体的观测值，则可用t.test()或BSDA包中的z.test()直接检验。\n\nt.test(x, y = NULL,\n       alternative = c(\"two.sided\", \"less\", \"greater\"),\n       mu = 0, paired = FALSE, var.equal = FALSE, conf.level = 0.95, ...)\nz.test(x, y = NULL,\n       alternative = c(\"two.sided\", \"less\", \"greater\"),\n       mu = 0, sigma.x = NULL, sigma.y = NULL, conf.level = 0.95\n\n其中：x和y为两个正态样本，默认为单样本检验；alternative为检验类型，分别是”two.side”(双侧)、“greater”(右侧)、“less”(右侧)；mu为总体均值；paired表示是否为配对样本t检验；var.equal表示双样本检验中是否等方差假设；conf.level为(1-\\alpha)，即置信水平。在总体标准差已知时选用z.test()，未知时选用t.test()；在总体分布近似正态分布但样本量n＜30时，选用t.test()，在n≥30时选用z.test()。\n\n例 4.25 根据某地环保规定，倾入河流废水中某有毒化学物质的平均含量不得超过3ppm。该地环境监测机构对沿河某工厂进行检查，测定20日入河废水中该物质的含量(ppm)为：\n3.1, 3.2, 3.3, 2.9, 3.5, 3.4, 2.5, 4.3, 3.0, 3.4, 2.9, 3.6, 3.2, 3.0, 2.7, 3.5, 2.9, 3.3, 3.3, 3.1\n假定废水中该有毒物含量服从正态分布，试在检验水准\\alpha=0.05时判断该工厂是否符合当地环保规定。\n\n\n解：总体方差未知，n＜30，采用t检验。直接采用t.test()函数:\n\nx = c(3.1,3.2,3.3,2.9,3.5,3.4,2.5,4.3,3.0,3.4,2.9,3.6,\n      3.2,3.0,2.7,3.5,2.9,3.3,3.3,3.1)\nt.test(x, alternative = \"greater\", mu = 3, conf.level = 0.95)\n\n\n    One Sample t-test\n\ndata:  x\nt = 2.4013, df = 19, p-value = 0.01337\nalternative hypothesis: true mean is greater than 3\n95 percent confidence interval:\n 3.057383      Inf\nsample estimates:\nmean of x \n    3.205 \n\n\n检验结果表明在\\alpha=0.05时，拒绝检验假设，认为该工厂不符合当地环保规定。\n\n例 4.26 对某检测仪器检测水体中污染物A的精度要求是方差不超过1.5ppm。现用该仪器对污染物A浓度为100ppm的标准溶液检测10次，结果为104.8, 99.4, 102.1, 100.1, 102.2, 99.3, 98.6, 101.2, 101.0, 102.1。试问该仪器检测精度是否达到了预定要求(\\alpha=0.01)。\n\n\n解：建立假设：H0为\\sigma^2≤1.5，H1为\\sigma^2＞1.5;\n基于\\chi^2分布进行检验，根据前面的公式计算统计量：\n\nmu = 100\ns = 1.5\na = 0.01\nx = c(104.8, 99.4, 102.1, 100.1, 102.2, 99.3, 98.6, 101.2, 101.0, 102.1)\nn = length(x)\nchi = sum((x - mu)^2) / s^2\nchi\n\n[1] 18.64889\n\n\n计算拒绝域界值：\n\nqchisq(a / 2, n)\n\n[1] 2.155856\n\nqchisq(1 - a / 2, n)\n\n[1] 25.18818\n\n\n根据界值判断，检验统计量落入拒绝域。因此，该仪器检测精度未达预定的方差要求。\n\n\n4.2.3.4 双正态总体均值与方差的假设检验\n对于分别来自总体N(\\mu_1,\\sigma_1^2)和总体N(\\mu_2,\\sigma_2^2)的两个独立样本X_1(n_1个观测值)和X_2(n_2个观测值)，其均值差异和方差差异的显著性检验方法分别如下：\n(1)双正态总体均值的显著性检验\n当两个总体方差已知时,根据以下公式计算z统计量:\n\n  z=\\frac{\\bar{x_1}-\\bar{x_2}}{\\sqrt{{\\sigma_1^2}/{n_1}+{\\sigma_2^2}/{n_2}}}\n\\tag{4.22}\n当两个总体方差未知但n_1和n_2均≥50时,根据以下公式计算z统计量:\n\n  z=\\frac{\\bar{x_1}-\\bar{x_2}}{\\sqrt{{s_1^2}/{n_1}+{s_2^2}/{n_2}}}\n\\tag{4.23}\n上述两种情况下，均可利用BSDA::z.test()函数直接检验，或利用qnorm()函数计算拒绝 域界值，再与统计量z进行比较以做出判断。双侧检验的拒绝域为|z|≥z_{\\alpha/2}，左侧检验的拒绝域为z≥-z_{\\alpha}，右侧检验的拒绝域为z≤z_{\\alpha}。\n当两个总体方差未知但\\sigma_1^2=\\sigma_2^2时,根据以下公式计算t统计量:\n\n  t=\\frac{\\bar{x_1}-\\bar{x_2}}{s_w\\sqrt{1/{n_1}+1/{n_2}}}\n\\tag{4.24}\n式中s_w见 式 4.9 。\n此时直接用t.test(x1, x2, var.equal = TRUE)进行检验。如果两个样本的观测值未知，则需要借助qt()函数计算拒绝域界值，与统计量t进行比较以做出判断。双侧检验的拒绝域为|t|≥t_{\\alpha/2,n_1+n_2-2}，左侧检验的拒绝域为t≥-t_{\\alpha,n_1+n_2-2}，右侧检验的拒绝域为t≤t_{\\alpha,n_1+n_2-2}。\n当两个总体方差未知但n_1=n_2=n时,根据以下公式计算t统计量:\n\n  t=\\frac{\\bar{x_1}-\\bar{x_2}}{s_z/\\sqrt{n}}\n\\tag{4.25}\n式中s_w见 式 4.11 。\n此时直接用t.test(x1, x2, paired = TRUE)进行检验。如果两个样本的观测值未知，则需要借助qt()函数计算拒绝域界值，与统计量t进行比较以做出判断。双侧检验的拒绝域为|t|≥t_{\\alpha/2,n-1}，左侧检验的拒绝域为t≥-t_{\\alpha,n-1}，右侧检验的拒绝域为t≤t_{\\alpha,n-1}。\n当两个总体方差未知但\\sigma_1^2≠\\sigma_2^2时,根据以下公式计算t统计量:\n\n  t=\\frac{\\bar{x_1}-\\bar{x_2}}{\\sqrt{s_1^2/{n_1}+s_2^2/{n_2}}}\n\\tag{4.26}\n此时直接用t.test(x1, x2)进行检验。如果两个样本的观测值未知，则需要借助qt()函数计算拒绝域界值，与统计量t进行比较以做出判断。双侧检验的拒绝域为|t|≥t_{\\alpha/2,\\nu}，左侧检验的拒绝域为t≥-t_{\\alpha,\\nu}，右侧检验的拒绝域为t≤t_{\\alpha,\\nu}。其中自由度\\nu的计算见 式 4.13 。\n\n\n例 4.27 测得某河流原水(x)和清水(y)含砷量的结果如下所示，试问两组均值之间的差异有无显著性。\nx：0.020, 0.016, 0.016, 0.016, 0.016, 0.200, 0.135, 0.220, 0.220, 0.160, 0.220, 0.220\ny：0.008, 0.008, 0.008, 0.006, 0.006, 0.006, 0.005, 0.005, 0.007, 0.007, 0.006, 0.006\n\n\n解：\n\nx = c(0.020, 0.016, 0.016, 0.016, 0.016, 0.200, 0.135, 0.220,\n    0.220, 0.160, 0.220, 0.220)  \ny = c(0.008, 0.008, 0.008, 0.006, 0.006, 0.006, 0.005, 0.005,\n    0.007, 0.007, 0.006, 0.006)\nt.test(x, y)\n\n\n    Welch Two Sample t-test\n\ndata:  x and y\nt = 4.1507, df = 11.003, p-value = 0.001614\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n 0.0540609 0.1761058\nsample estimates:\nmean of x mean of y \n0.1215833 0.0065000 \n\n\n结果拒绝H0，即认为两个样本代表的总体均值具有差异显著性。\n(2)双正态总体方差的显著性检验\n当两个总体均值已知时，按以下公式计算统计量：\n\n  F=\\frac{n_1\\sum_{i=1}^{n_1}(x_{1,i}-\\mu_1^2)^2}{n_2\\sum_{i=1}^{n_2}(x_{2,i}-\\mu_2^2)^2}\n\\tag{4.27}\n此时，双侧检验的拒绝域为F≤F_{\\alpha/2,n_1,n_2}或F≥F_{1-\\alpha/2,n_1,n_2}，左侧检验的拒绝域为F≥F_{1-\\alpha,n_1,n_2}，右侧检验的拒绝域为F≤F_{\\alpha,n_1,n_2}。\n当两个总体均值未知时，按以下公式计算统计量：\n\n  F=\\frac{s_1^2}{s_2^2}\n\\tag{4.28}\n此时，双侧检验的拒绝域为F≤F_{\\alpha/2,n_1-1,n_2-1}或F≥F_{1-\\alpha/2,n_1-1,n_2-1}，左侧检验的拒绝域为F≥F_{1-\\alpha,n_1-1,n_2-1}，右侧检验的拒绝域为F≤F_{\\alpha,n_1-1,n_2-1}。\n以上两种情况，均可利用var.test()函数直接检验，亦可通过qf()函数计算拒绝域界值，再与统计量F比较以做出判断。\n\n例 4.28 x和y(均服从正态分布)分别为两种检测方法对某污染物标准溶液的重复检测结果：\nx: 20.5, 19.8, 19.0, 19.7, 20.1, 20.4, 20.0, 19.9\ny: 19.8, 20.7, 19.5, 20.8, 19.6, 20.4, 20.2\n试问两种检测方法的精度(方差)有无差异(\\alpha=0.05)。\n\n解：直接采用var.test()函数计算：\n\nx = c(20.5, 19.8, 19.0, 19.7, 20.1, 20.4, 20.0, 19.9)\ny = c(19.8, 20.7, 19.5, 20.8, 19.6, 20.4, 20.2)\nvar.test(x, y)\n\n\n    F test to compare two variances\n\ndata:  x and y\nF = 0.79319, num df = 7, denom df = 6, p-value = 0.7608\nalternative hypothesis: true ratio of variances is not equal to 1\n95 percent confidence interval:\n 0.1392675 4.0600387\nsample estimates:\nratio of variances \n         0.7931937 \n\n\n检验结果接受H0，即两样本总体方差比等于1。\n采用公式计算：\n\nnx = length(x); ny = length(y)\nssx = var(x); ssy = var(y)\n# 计算方差比统计量\n(f = ssx / ssy)\n\n[1] 0.7931937\n\na = 0.05\n# 计算右侧界值\nqf(0.025, nx - 1, ny - 1)\n\n[1] 0.195366\n\n# 计算左侧界值\nqf(0.975, nx - 1, ny - 1)\n\n[1] 5.69547\n\n\n统计量f的值落在接受域，与var.test()直接检验的结果一致，表明这两种检测方法的精度没有差异显著性。\n\n\n4.2.3.5 总体比率的假设检验\n总体比率p可视为伯努利分布(亦即0-1分布)B(1,p)的参数。对于来自伯努利分布B(1,p)的一个样本X(x_i, i=1~n)，当np与n(1-p)均＞5时，其总体比率p与已知总体比率p_0的假设检验可用prop.test()进行检验；当np或n(1-p)≤5时，则使用binom.test()进行检验。prop.test()基于近似计算，而binom.test()执行精确计算。对于来自伯努利分布B(1,p)的两个独立样本X_1 (x_{1,i},i=1\\sim n_1)和X_2 (x_{2,i},i=1\\sim n_2)，其总体比率p_1和p_2的假设检验可使用prop.test()进行检验。\n\n例 4.29 按规定，某工厂排放的污水超标率不超过8%为合格。对该厂排放污水取样210次检测，结果有23次超标，问抽样检测结果是否达到合格规定要求？\n\n解：H0为p≤0.08，H1为p＞0.08\n采用prop.test()检验：\n\nn = 210\nx = 23\np0 = 0.08 \nprop.test(x, n, p = p0, alternative = \"greater\", conf.level = 0.95)\n\n\n    1-sample proportions test with continuity correction\n\ndata:  x out of n, null probability p0\nX-squared = 2.1021, df = 1, p-value = 0.07355\nalternative hypothesis: true p is greater than 0.08\n95 percent confidence interval:\n 0.07690104 1.00000000\nsample estimates:\n        p \n0.1095238 \n\n\n采用`binom.test()检验：\n\nbinom.test(x, n, p = p0, alternative = \"greater\", conf.level = 0.95)\n\n\n    Exact binomial test\n\ndata:  x and n\nnumber of successes = 23, number of trials = 210, p-value = 0.07812\nalternative hypothesis: true probability of success is greater than 0.08\n95 percent confidence interval:\n 0.07602032 1.00000000\nsample estimates:\nprobability of success \n             0.1095238 \n\n\n以上两种检验方法的结果均未拒绝H0，因此认为检测结果达到合格规定。\n\n例 4.30 2019年某单位为了调查工业三废对某河流水质的污染情况，对该河流枯水期和丰水期水体中汞含量进行了抽样检查：枯水期取样356个，超标15个；丰水期取样360个，超标62个。试问枯水期和丰水期汞含量超标情况是否相同？\n\n\n解：H0为p_1=p_2，H1为p_1≠p_2\n\nx = c(15, 62)\nn = c(356, 360)\nprop.test(x, n)\n\n\n    2-sample test for equality of proportions with continuity correction\n\ndata:  x out of n\nX-squared = 30.22, df = 1, p-value = 3.858e-08\nalternative hypothesis: two.sided\n95 percent confidence interval:\n -0.17711583 -0.08305896\nsample estimates:\n    prop 1     prop 2 \n0.04213483 0.17222222 \n\n\n检验结果拒绝H0，因此认为枯水期和丰水期水体中汞含量超标情况不同，丰水期更严重。\n\n\n4.2.3.6 基于列联表资料的假设检验\n列联表(contigency table)也称交叉表，用于整理离散型变量的分组频数，连续型变量可通过分区间统计频数实现离散化。其中一维列联表就是单变量的频数分布表，二维列联表是两个变量的交叉频数分布表，而两个二分类变量的2行×2列的列联表又称为四格表(如 表 4.2 所示，表中a、b、c、d为实际频数，记为A；括号中为对应的理论频数，记为T)，因为其中四个单元格中的频数是核心数据。依此类推，三个及以上变量对应的为三维及更高维的列联表，也称为行列表。很多实际问题都可以转化为列联表形式的频数资料来进行分析。\n\n\n\n\n表 4.2: 四格表资料形式\n\n\n\n\n\n\n\n\n\n\n\n\n\n列联表资料的检验是通过样本的实际频数A与理论频数T计算偏离程度来研究变量是否独立(独立性检验)、是否一致(齐性检验)等问题，常用的检验方法有Pearson\\chi^2检验、Fisher精确检验、McNemar检验等。一般而言，当样本量n≥40且所有理论频数T≥5时，采用Pearson\\chi^2检验(chisq.test()函数，设置参数correct=FALSE)；当样本量n≥40但1≤T＜5时，用带Yates连续性修正的\\chi^2检验(chisq.test()函数，默认参数correct=TRUE)；当样本量n＜40，或理论频数T＜1时，或\\chi^2检验检验结果表明p≈\\alpha(检验水准)，应采用Fisher精确检验(fisher.test()函数)；配对的二维列联表资料采用McNemar检验(mcnemar.test()函数)；三维列联表资料采用Cochran-Mantel-Haenszel\\chi^2检验(mantelhaen.test()函数)。\n(1)列联表资料的齐性检验\n齐性检验是检验多个总体在某个变量的各个类别上的分布特征是否相同，例如不同季节的湖水总氮等污染物含量是否一致，不同功能区的土壤重金属含量是否一致等。多总体比率的差异显著性检验，可以转化为列联表资料，再应用Pearson\\chi^2检验方法执行齐性检验。\n\n例 4.31 将 例 4.30 的数据改成四格表资料，如下所示：\n\n\n\n\n\n\n\n\n\n\n解：H0为p_1=p_2，H1为p_1≠p_2；\n\nmt = matrix(c(15, 62, 341, 298), nrow = 2,\n            dimnames = list(\"分组\" = c(\"枯水期\", \"丰水期\"),\n                            \"频数\" = c(\"超标\", \"不超标\")))\nchisq.test(mt, correct = FALSE)\n\n\n    Pearson's Chi-squared test\n\ndata:  mt\nX-squared = 31.561, df = 1, p-value = 1.933e-08\n\n\n检验结果与 例 4.30 一致，拒绝H0。\n采用带Yates连续性修正的\\chi^2检验，得到的p值更大，即检验结果更保守，不易犯弃真错误。Fisher精确经验法基于超几何分布，特别适用于单元频数小的四格表和行列表资料。\n\n例 4.32 某市工业区和居民区土壤铬含量超标和不超标结果如下表所示，试问两个区域的超标率差异有无显著性意义？\n\n\n\n\n\n\n\n\n\n\n解：H0为p_1=p_2，H1为p_1≠p_2\n由于有实际频数和理论频数小于5，先采用带Yates连续性修正的\\chi^2检验：\n\nmt = matrix(c(22, 10, 2, 6), nrow = 2,\n            dimnames = list(\"分类\" = c(\"工业区\", \"居民区\"),\n                            \"超标\" = c(\"超标\", \"不超标\")))\nchisq.test(mt)\n\nWarning in chisq.test(mt): Chi-squared approximation may be incorrect\n\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  mt\nX-squared = 3.444, df = 1, p-value = 0.06348\n\n\n检验结果发现接受H0，但p接近检验水准\\alpha，改用Fisher精确检验：\n\nfisher.test(mt)\n\n\n    Fisher's Exact Test for Count Data\n\ndata:  mt\np-value = 0.04204\nalternative hypothesis: true odds ratio is not equal to 1\n95 percent confidence interval:\n  0.9160508 73.9370368\nsample estimates:\nodds ratio \n  6.266107 \n\n\nFisher精确检验结果结果拒绝H0。\n(2)列联表资料的独立性检验\n列联表资料的独立性检验是检验两个变量是否独立，亦即是否存在相关性，R中采用\\chi^2检验方法，即chisq.test()函数。由于该函数亦可用于方差齐性检验，这让人容易混淆对独立性检验与方差齐性检验的理解，二者实际上存在本质区别：齐性检验是对不同总体分别抽样，而独立性检验是对同一总体进行抽样再分类；齐性检验基于各总体在某个变量不同分类上是否具有相同概率的假设，而独立性检验是基于事件独立的假设；齐性检验中的变量关系不对等，可分为自变量和因变量，而独立性检验中的变量关系平等。\n\n例 4.33 某地水源受氟污染，为掌握氟污染对人体健康的危害，有关机构在2010年对该地区1176人进行了调查，发现802人患有釉斑症，各级发病程度与饮用水氟含量的频数资料如下表所示。试问饮用水氟含量与釉斑症的发病程度的有无相关性(独立性检验)？\n\n\n\n\n\n\n\n\n\n\n解：H0为饮用水氟含量与釉斑病发病程度无关，H1为饮用水氟含量与釉斑病发病程度有关；\n\nmt = matrix(c(39, 87, 87, 33, 58, 75, 157, 66, 14, 28, 69, 89),\n            nrow = 4, dimnames = list(\"氟含量\" = paste0(\"C\", 1:4),\n                                \"发病程度\"=c(\"初显症\",\"1度\",\"2~3度\")))\nchisq.test(mt)\n\n\n    Pearson's Chi-squared test\n\ndata:  mt\nX-squared = 88.424, df = 6, p-value &lt; 2.2e-16\n\n\n检验结果拒绝H0，表明饮用水氟含量与釉斑病发病程度有关。\n(3)配对列联表资料的McNemar检验\n对于配对的二维列联表资料，如每一对实验对象给予不同的处理，或同一实验对象先后给予不同的处理，McNemar检验通过频数变化来检验一致性，R中函数为mcnemar.test()。\n\n例 4.34 将28份微生物样品按相同条件分别接种于甲乙两种培养基，“+”表示能够生长，“-”表示不能生长，最终接种培养结果整理汇总于下表中。试问两种培养基效果是否一致？\n\n\n\n\n\n\n\n\n\n\n解：H0为两种培养基效果一致，H1为两种培养基效果不一致；\n\nmt = matrix(c(11, 1, 9, 7), nrow=2,\n            dimnames = list(\"乙培养基\" = c(\"阳性\", \"阴性\"),\n                            \"甲培养基\" = c(\"阳性\", \"阴性\")))\nmcnemar.test(mt, correct = TRUE)\n\n\n    McNemar's Chi-squared test with continuity correction\n\ndata:  mt\nMcNemar's chi-squared = 4.9, df = 1, p-value = 0.02686\n\n\n结果拒绝H0，表明两种培养基效果不一致，乙培养基的效果优于甲培养基。\n\n\n4.2.3.7 分布函数的拟合优度检验\n拟合优度(goodness-of-fit)检验用于检验样本所在总体的分布与某一理论分布是否一致。令某总体的分布函数为F(x)且未知，X为来自该总体的一个样本，x_i(i=1\\sim n)为观测值。则该类问题的假设为：H0为F(x)=F_0(x,\\theta)，H1为F(x)≠F_0(x,\\theta)，其中\\theta是分布的未知参数，在检验之前，需要对其进行估计。\n(1)Shapiro-Wilk正态性检验\n该方法也称W检验，用于检验样本是否来自于正态总体，对异常值敏感，适用于小样本。R中执行该检验的函数为shapiro.test()。\n\n例 4.35 Shapiro-Wilk正态性检验示例。\n\n\nx = rnorm(100, mean = 6, sd = 2)    # 构造正态分布样本\nshapiro.test(x)\n\n\n    Shapiro-Wilk normality test\n\ndata:  x\nW = 0.99086, p-value = 0.7339\n\ny = runif(100, min = 4, max = 8)    # 构造均匀分布样本\nshapiro.test(y)\n\n\n    Shapiro-Wilk normality test\n\ndata:  y\nW = 0.97107, p-value = 0.0267\n\n\n以上代码演示了shapiro.test()函数对不同分布样本的检验，使用简单，检验结果正确。\n(2)Kolmogorov-Smirnov检验\n该方法简称K-S检验，用于检验样本是否来自某一理论分布，适用于连续型随机变量分布的检验问题。该方法相对稳健，对异常值具有较好的抗干扰能力。R中执行该检验的函数为ks.test()。\n\n例 4.36 Kolmogorov-Smirnov检验示例。\n\n\nx = rnorm(100, mean = 6, sd = 2)    # 构造正态分布样本\ny = runif(80, min = 2, max = 8)    # 构造均匀分布样本\nks.test(x, \"pnorm\", 6, 2)\n\n\n    Asymptotic one-sample Kolmogorov-Smirnov test\n\ndata:  x\nD = 0.057828, p-value = 0.8917\nalternative hypothesis: two-sided\n\n\n检验结果接受H0，即x来自均值为6、方差为2的正态总体。\n\nks.test(x, \"pnorm\", 5, 2)\n\n\n    Asymptotic one-sample Kolmogorov-Smirnov test\n\ndata:  x\nD = 0.23281, p-value = 3.92e-05\nalternative hypothesis: two-sided\n\n\n检验结果拒绝H0，即x不是来自均值为5、方差为2的正态总体。\n\nks.test(x, y)\n\n\n    Exact two-sample Kolmogorov-Smirnov test\n\ndata:  x and y\nD = 0.2125, p-value = 0.03114\nalternative hypothesis: two-sided\n\n\n检验结果表明x和y不是来自同一连续型随机变量分布。\n(3)总体分布的Pearson\\chi^2检验\nPearson\\chi^2检验是最常用的适用于所有理论分布的拟合优度检验方法。可用chisq.test()函数执行该检验，使用时需指定与样本(参数x不能包含负数)等长度的概率向量(参数p，总和必须为1)。\n\n例 4.37 总体分布的Pearson χ2检验示例。\n\n\nx = runif(10, min = 2, max = 8)    # 构造均匀分布样本\np0 = rep(1 / length(x), length(x))\nchisq.test(x, p = p0)\n\n\n    Chi-squared test for given probabilities\n\ndata:  x\nX-squared = 6.2747, df = 9, p-value = 0.7121\n\n\n检验结果表明样本x来自均匀分布总体。\n\n\n4.2.3.8 其他检验方法\n(1)游程检验\n游程检验亦称“连贯检验”，是一种非参数检验方法，用于单个样本的随机性或两个总体的分布是否相同。R包tseries中的runs.test()函数可用于二分类或二进制序列的随机性检验。\n(2)白噪声检验\n白噪声检验也称纯随机性检验，用于检验序列数据是否为纯随机序列，常用于时间序列分析、模型预测残差分析等。R中Box.test()函数用于执行白噪声检验。\n(3)Wilcoxon秩和检验和Wilcoxon符号秩检验\n秩(rank)是样本中变量观测值从小到达排序后的次序号。Wilcoxon秩和检验和Wilcoxon符号秩检验都是利用样本数据的秩信息进行检验，属于非参数检验方法，用于不能满足t检验的场景，如总体分布不是正态分布，或总体分布类型未知。R中wilcox.test()函数执行这两种检验，其中参数paired默认为FALSE，执行Wilcoxon秩和检验；当paired设置为TRUE，则执行配对资料的Wilcoxon符号秩检验。pairwise.wilcox.test()函数执行两两比较的秩和检验和符号秩检验。\n(4)Kruskal-Wallis秩和检验(K-W检验)\nK-W检验是用于三个及三个以上独立样本是否来自相同分布的非参数检验方法，前提要求总体是连续型分布，除位置参数不同外，分布相似。R中kruskal.test()函数执行该检验。\n(5)Friedman检验\nFriedman检验也是检验三个及三个以上独立样本是否来自相同分布的非参数检验方法，但要求样本为配对资料。R中friedman.test()函数执行该检验。\n(6)Bartlett、Levene及Fligner-Killeen方差齐性检验\n三者都是多样本方差齐性检验的方法，Bartlett方法为参数检验方法，用于正态分布数据，R中函数为bartlett.test()；Levene方法是非参数检验方法，可用于非正态分布数据，R中函数为car包中的leveneTest()函数；Fligner-Killeen方法也是非参数检验方法，R中函数为fligner.test()。\nR及很多R包提供了针对各种场景下的假设检验函数，具体使用可根据需要查询相关函数或R包的使用方法。\n\n\n4.2.3.9 方差分析\n在环境科学与工程研究工作中，经常需要同时对三个及三个以上的样本均值进行比较，如比较不同工艺去除污染物的效果，比较不同污染物的毒性等，比较不同季节大气中某种污染物的含量等。这里研究的工艺、污染物种类、污染物、季节即为因素(也称为控制变量，)，因素所处的状态称为水平，而研究者感兴趣的污染物去除效果、毒性、含量称为观测变量或研究变量。应用t检验分析多样本均值差异的显著性需要执行很多次，且容易增大分析误差，而应用方差分析(F检验法)可简化分析步骤。除此以外，方差分析还有很多用途，如分析多个因素之间的交互作用、回归模型的显著性检验等。\n方差分析的前提要求是样本必须满足独立性、正态性和方差齐性。独立性是指各样本来自相互独立的随机总体，正态性是指各样本均来自正态总体，方差齐性是指各样本对应总体的方差相同。方差分析的基本思想是把全部观测值的总变异，按因素分组进行分析，计算每个因素对总变异的贡献，从而判定因素的重要性。例如，在单因素的完全随机设计的实验资料中，总变异可分为组内变异和组间变异；在配伍组设计的资料中，总变异可分为处理组间变异、配伍组间变异及误差三部分；在2×2析因设计资料中，总变异可分为两个因素的两个组间变异、两因素交互作用及误差四部分。\nR中执行方差分析的函数为aov()，该函数的形式如下：\n\naov(formula, data = NULL, projections = FALSE, qr = TRUE,\n    contrasts = NULL, ...)\n\nformula是一个用公式指定的模型，如”y ~ x”，“y ~ a + b + c”，“y ~ a + b + a:b”等。其中常用的符号及其意义如 表 4.3 所示。data是包含所有变量的数据框。projections为逻辑值，指定是否返回投影。qr为逻辑值，指定是否返回QR分解结果。contrasts是用于公式中某些因素的对比列表。…是传递给线性模型lm()函数的其他参数。\n\n\n\n\n表 4.3: R中formula中常用的符号及其意义\n\n\n\n\n\n\n\n\n\n\n符号\n含义\n\n\n\n\n~\n左边为因变量，右边为自变量。例如通过a、b和c预测y，代码为y ~ a + b + c\n\n\n+\n分隔自变量\n\n\n:\n表示自变量的交互项。例如通过a、b及其交互项预测y，代码为y ~ a + b + a:b\n\n\n*\n表示所有可能交互项的简洁方式。例如y ~ a * b * c等价于y ~ a + b + c + a:b + b:c + c:a\n\n\n^\n表示交互项达到某个次数。例如y ~ (a + b + c)^2等价于y ~ (a + b + c) * (a + b + c)\n\n\n.\n点号，表示包含除因变量外的所有变量。例如数据包含变量y、a、b和c，代码y ~ . 等价于y ~ a + b + c\n\n\n-\n表示从等式中移除某个变量。例如y ~ (a + b + c)^2 – a:b等价于y ~ a + b + c + b:c + c:a\n\n\n%in%\n表示其左侧项嵌套如右侧项中。如y ~ a + b %in% a等价于y ~ a + a:b\n\n\n-1或+0\n删除截距项。例如y ~ a - 1或y ~ a + 0，强制直线方程通过原点\n\n\n函数I()\n避免公式中算术运算符和符号运算符产生混淆。例如y ~ a + (b + c)^2等价于y ~ a + b + c + b:c；相反, y ~ a + I((b + c)^2)等价于y ~ a + h，h是b与c之和的平方\n\n\n函数引用\n可以在表达式中用的数学函数。例如log(y) ~ a + b + c表示通过a、b和c来预测log(y)\n\n\n\n\n\n\n\n\n(1)单因素方差分析(one-way ANOVA)\n单因素方差分析就是研究一个因素的多个水平对观测变量是否产生显著性影响。\n\n例 4.38 某湖不同季节湖水中氯化物含量(mg/L)监测结果如下所示。试问不同季节湖水的氯化物含量的差异有无显著意义？\n春：22.6, 22.8, 21.0, 16.9, 24.0, 21.9, 21.5, 21.2\n夏：19.1, 22.8, 24.5, 18.0, 15.2, 18.4, 20.1, 21.5\n秋：18.9, 13.6, 17.2, 15.1, 16.6, 14.2, 16.7, 19.6\n冬：19.0, 16.9, 17.6, 14.8, 13.1, 16.9, 16.2, 14.8\n\n\n解：全部32个观测值之间的总变异包括了季节的影响以及随机误差；4个季节均值之间的差异是组间变异，反映了季节对湖水氯化物含量的可能影响，但也有随机误差的贡献；每个季节内8个观测值之间的差异是组内变异，与季节无关，反映的是随机误差的影响。\n\nlibrary(tidyverse)\n# 宽表\ndf = data.frame(\n  春 = c(22.6, 22.8, 21.0, 16.9, 24.0, 21.9, 21.5, 21.2),\n  夏 = c(19.1, 22.8, 24.5, 18.0, 15.2, 18.4, 20.1, 21.5),\n  秋 = c(18.9, 13.6, 17.2, 15.1, 16.6, 14.2, 16.7, 19.6),\n  冬 = c(19.0, 16.9, 17.6, 14.8, 13.1, 16.9, 16.2, 14.8)\n)\n# 转换为长表\ndf = df %&gt;% \n  pivot_longer(cols = everything(),\n               names_to = \"season\",\n               values_to = \"conc\")\n\nans = aov(conc ~ season, data = df)\nsummary(ans)\n\n            Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nseason       3  164.3   54.77    10.4 9.06e-05 ***\nResiduals   28  147.4    5.26                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n由结果可知，季节因素对氯化物浓度具有显著影响。\n当不满足方差齐性条件时(可用bartlett.test()检验)，使用oneway.test()函数执行方差分析：\n\noneway.test(conc ~ season, data = df)\n\n\n    One-way analysis of means (not assuming equal variances)\n\ndata:  conc and season\nF = 11.423, num df = 3.00, denom df = 15.41, p-value = 0.0003383\n\n\n结果仍然表明季节对氯化物浓度具有显著影响。\n当不满足正态性(可用shapiro.test()检验)和方差齐性(可用car::leveneTest()检验)时，使用kruskal.test()函数：\n\nkruskal.test(conc ~ season, data = df)\n\n\n    Kruskal-Wallis rank sum test\n\ndata:  conc by season\nKruskal-Wallis chi-squared = 16.399, df = 3, p-value = 0.0009392\n\n\n结果也表明季节对氯化物浓度具有显著影响。\n(2)双因素方差分析(two-way ANOVA)\n双因素方差分析用于研究两个因素对观测变量有无显著性影响，包括两种类型：不考虑两个因素间的交互作用和考虑两个因素间的交互作用。交互作用是指两个因素存在联合作用，表现为相互促进或相互制约。如果两个因素对观测变量的影响是独立的，不需要考虑交互作用；如果两个因素对观测变量的影响不是独立的，则需要考虑交互作用。开展考虑因素交互作用的方差分析，要求两个因素的各个水平组合下有重复实验观测值。\n\n例 4.39 某市2016-2017年不同地点不同季节的大气中飘尘浓度(mg/m3)日均值如下表所示：\n\n\n\n\n地点\n春\n夏\n秋\n冬\n\n\n\n\nA\n0.614\n0.475\n0.667\n1.150\n\n\nB\n0.620\n0.420\n0.880\n1.200\n\n\nC\n0.379\n0.200\n0.540\n0.940\n\n\n\n试推断地点和季节两个因素对飘尘浓度有无显著影响。\n解：\n\nlibrary(tidyverse)\ndf = data.frame(\n  地点 = LETTERS[1:3],\n  春   = c(0.614, 0.620, 0.379),\n  夏   = c(0.475, 0.420, 0.200),\n  秋   = c(0.667, 0.880, 0.540),\n  冬   = c(1.150, 1.200, 0.940)\n)\ndf = df %&gt;% \n  pivot_longer(cols = 2:5, names_to = \"季节\", values_to = \"飘尘浓度\")\n  \nans = aov(飘尘浓度 ~ 季节 + 地点, data = df)\nsummary(ans)\n\n            Df Sum Sq Mean Sq F value   Pr(&gt;F)    \n季节         3 0.8796 0.29318   88.76 2.32e-05 ***\n地点         2 0.1574 0.07871   23.83   0.0014 ** \nResiduals    6 0.0198 0.00330                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n结果表明季节和地点两个因素都对飘尘浓度具有极显著影响，其中季节的影响大于地点的影响。\n\n例 4.40 2015-2016年不同城市不同季节PM2.5浓度(μg/m^3)月均值如下表所示。试推断城市和季节两个因素及二者交互项对PM2.5浓度有无显著影响(假定数据符合正态性、独立性和方差齐性要求)。\n\n\n\n\n\n\n\n\n\n\n解：将数据整理成csv格式数据文件(本例中文件名为PM2.5-for-annova.csv，编码为UTF-8)，列名分别为pm25、city、month和season。\n\nlibrary(tidyverse)\nfilename = \"./data/PM2.5-for-anova.csv\"\ndf = read_csv(filename, col_names = TRUE, col_types = \"nfff\", \n              locale = locale(encoding = \"UTF-8\"))\nans = aov(pm25 ~ city * season, data = df)\nsummary(ans)\n\n            Df Sum Sq Mean Sq F value   Pr(&gt;F)    \ncity         5  22497    4499  13.436 3.34e-08 ***\nseason       3  10894    3631  10.845 1.48e-05 ***\ncity:season 15   3293     220   0.656    0.813    \nResiduals   48  16073     335                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n检验结果表明城市和季节两个因素对PM2.5浓度具有显著影响，但二者没有交互作用。\n当数据不满足正态性和方差齐性条件时，可利用R包rcompanion提供的scheirerRayHare()函数执行Scheirer-Ray-Hare非参数检验：\n\nlibrary(rcompanion)\nscheirerRayHare(pm25 ~ city * season, data = df)\n\n\nDV:  pm25 \nObservations:  72 \nD:  0.9992765 \nMS total:  438 \n\n\n            Df  Sum Sq      H p.value\ncity         5 17035.6 38.922 0.00000\nseason       3  5232.4 11.955 0.00754\ncity:season 15  2859.1  6.532 0.96935\nResiduals   48  5948.3               \n\n\n以上数据集中季节变量中的3个月份可视为三次重复。如果是非重复区组设计实验得到的数据，可用friedman.test()进行方差分析。\n\n\n# A tibble: 24 × 3\n# Groups:   season [4]\n   season city   pm25\n   &lt;fct&gt;  &lt;fct&gt; &lt;dbl&gt;\n 1 春     北京   70.7\n 2 春     重庆   45.7\n 3 春     济南   77.3\n 4 春     合肥   61  \n 5 春     兰州   47  \n 6 春     广州   32.7\n 7 夏     北京   55.3\n 8 夏     重庆   41.7\n 9 夏     济南   68.3\n10 夏     合肥   41.3\n# ℹ 14 more rows\n\n\n\n    Friedman rank sum test\n\ndata:  df1$pm25, df1$season and df1$city\nFriedman chi-squared = 15, df = 3, p-value = 0.001817\n\n\n(3)多重比较\n当方差分析结果显示差异有显著性，并不代表所有的因素水平间都有显著性差异，此时需要通过多重比较(两两比较)才能明确具体水平间的差异显著性。多重比较的方法有很多，其中R预置了Tukey方法(TukeyHSD()函数)。R包agricolae提供了更多多重比较的方法，除了Tukey方法HSD.test()外，还有LSD方法LSD.test()(最小差数显著法)、Bonferroni方法LSD.test()(设置参数p.adj = “bonferroni”)、SNK方法SNK.test()(也称q检验)、Duncan方法duncan.test()(新复极差法)、Scheffé方法scheffe.test()、Dunnet方法multcomp::glht()等。Tukey方法一般用于较多组(≥6)且容量相同样本的多重比较，较LSD方法保守，不易发现差异显著性。Bonferroni方法般用于5组以下容量相同样本的多重比较。Dunnet方法用于多个实验组与一个对照组的比较。Scheffé方法适用于各种情况下的多重比较，特别是样本容量不同的情况。\n\n例 4.41 对 例 4.38 中方差分析的结果做多重比较。\n\n\nlibrary(agricolae)\nlibrary(tidyverse)\n# 宽表\ndf = data.frame(\n  春 = c(22.6, 22.8, 21.0, 16.9, 24.0, 21.9, 21.5, 21.2),\n  夏 = c(19.1, 22.8, 24.5, 18.0, 15.2, 18.4, 20.1, 21.5),\n  秋 = c(18.9, 13.6, 17.2, 15.1, 16.6, 14.2, 16.7, 19.6),\n  冬 = c(19.0, 16.9, 17.6, 14.8, 13.1, 16.9, 16.2, 14.8)\n)\n# 转换为长表\ndf = df %&gt;% \n  pivot_longer(cols = everything(),\n               names_to = \"season\",\n               values_to = \"conc\")\n# 方差分析\nmodel = aov(conc ~ season, data = df)\n# R内置的Tukey方法多重比较\nout1 = TukeyHSD(model); \nout1\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = conc ~ season, data = df)\n\n$season\n         diff        lwr      upr     p adj\n冬-春 -5.3250 -8.4573904 -2.19261 0.0004083\n秋-春 -5.0000 -8.1323904 -1.86761 0.0008718\n夏-春 -1.5375 -4.6698904  1.59489 0.5460365\n秋-冬  0.3250 -2.8073904  3.45739 0.9918917\n夏-冬  3.7875  0.6551096  6.91989 0.0132270\n夏-秋  3.4625  0.3301096  6.59489 0.0260275\n\n# agricolae包中Tukey方法多重比较\nout2 = HSD.test(model, \"season\")\nout2$groups\n\n      conc groups\n春 21.4875      a\n夏 19.9500      a\n秋 16.4875      b\n冬 16.1625      b\n\n# agricolae包中LSD方法多重比较\nout3 = LSD.test(model, \"season\", p.adj = \"bonferroni\")\nout3$groups\n\n      conc groups\n春 21.4875      a\n夏 19.9500      a\n秋 16.4875      b\n冬 16.1625      b\n\n\nTukeyHSD.test()函数的检验结果与agricolae中的HSD.test()函数的结果相同，但结果展示形式不同，前者给出各次比较的均值差的区间估计和p值，后者以字母形式标注组间差异显著性，字母相同表示无显著性差异，字母不同表示有显著性差异。本例中，Bonferroni方法与Tukey方法检验结果一致。\n此外，R内置的pairwise.t.test()函数基于t检验方法执行多重比较，并给出p值。\n\nattach(df)\npairwise.t.test(conc, season, p.adjust.method = \"bonferroni\")\n\n\n    Pairwise comparisons using t tests with pooled SD \n\ndata:  conc and season \n\n   春      冬      秋     \n冬 0.00044 -       -      \n秋 0.00096 1.00000 -      \n夏 1.00000 0.01579 0.03223\n\nP value adjustment method: bonferroni \n\ndetach(df)\n\n如果需要对因素间交互作用进行可视化，可使用R中interaction.plot()函数，具体使用方法参考该函数的帮助信息。\n\n练习 4.3 \n四个实验室测定浓度为0.100mg/L的铬标准溶液的结果分别为：A实验室0.099、0.098、0.101、0.099、0.100，B实验室0.099、0.097、0.101、0.098、0.099，C实验室为0.097、0.094、0.098、0.098、0.099，D实验室0.101、0.104、0.101、0.101、0.102。试分析在检验水准为0.01时，四个实验室测定结果是否存在显著性差异？如有显著性差异，进一步开展多重比较。",
    "crumbs": [
      "进阶知识",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>环境数据统计分析</span>"
    ]
  },
  {
    "objectID": "CH4.html#相关分析与回归分析",
    "href": "CH4.html#相关分析与回归分析",
    "title": "4  环境数据统计分析",
    "section": "4.3 相关分析与回归分析",
    "text": "4.3 相关分析与回归分析\n现实世界中，各种因素(变量)交织在一起，相互依赖和制约，数学研究其中的确定性关系——函数关系，而统计学则研究其中的不确定性关系——相关关系。在相关关系分析的基础上，可进一步开展回归分析，建立因变量与其相关的自变量之间的回归模型。\n\n4.3.1 相关分析\n\n4.3.1.1 双变量简单相关系数的计算与检验\n相关系数定量地表征两个变量间的线性相关程度及相关方向，其值范围在-1至+1之间，正数为正相关，负数为负相关；绝对值越接近于1表示相关性越高，反之则相关性越低，接近于0为零相关或无相关。\n研究两个变量之间的相关关系，可以应用Pearson积矩相关系数(r)、Spearman秩相关系数(\\rho)以及Kendall秩相关系数(\\tau)。Pearson积矩相关系数基于矩计算，要求两个变量是服从正态分布的连续型变量。Kendall秩相关系数利用原始数据的秩计算，适用于两个定序变量以及不服从正态分布的等间隔数据，是一种非参数统计分析方法。Spearman秩相关系数也利用原始数据的秩计算，适用于连续型和离散型变量，对变量分布类型不作要求，也是一种非参数统计分析方法，可用于使用Pearson积矩相关系数和Kendall秩相关系数的场合，但统计功效低于Pearson积矩相关系数。\nR中cor()函数能够计算上述三种相关系数，其中参数method用于设置计算具体相关系数的类型；对于存在缺失值的样本，需要对use参数进行相关设置。对于数据框形式的多变量数据集，cor()函数会给出相关系数矩阵，相关系数矩阵的可视化参见 小节 3.4.6.2 相关内容。R中cor.test()函数用于对相关系数进行检验(H0：相关系数为0)。\n\n在某交通点连续三天监测汽车流量和大气NO2浓度，每天采样三次，汽车流量(辆/h)监测数据为1500, 960, 1200, 1784, 1176, 1476, 1820, 1060, 1436；NO2浓度(mg/m^3)监测数据为0.1200, 0.0390, 0.1000, 0.2220, 0.1290, 0.1450, 0.1350, 0.0785, 0.0985。试求二者相关系数并进行检验。\n\n\nx = c(1500, 960, 1200, 1784, 1176, 1476, 1820, 1060, 1436)\ny = c(0.1200, 0.0390, 0.1000, 0.2220, 0.1290, 0.1450, 0.1350, 0.0785, 0.0985)\n# 计算pearseon相关系数\ncor(x, y, method = \"pearson\")\n\n[1] 0.7988311\n\n# 计算spearman相关系数\ncor(x, y, method = \"spearman\")\n\n[1] 0.7833333\n\n# 计算kendall相关系数\ncor(x, y, method = \"kendall\")\n\n[1] 0.6111111\n\n# 计算pearseon相关系数并进行检验\ncor.test(x, y, method = \"pearson\")\n\n\n    Pearson's product-moment correlation\n\ndata:  x and y\nt = 3.5134, df = 7, p-value = 0.009814\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.2869339 0.9558528\nsample estimates:\n      cor \n0.7988311 \n\n\n检验结果表明pearson相关系数不等于0。\n\n\n4.3.1.2 多变量偏相关系数的计算与检验\n偏相关分析是在三个及以上多变量的相关分析中，在消除其他变量影响的前提下考察两个变量之间的相关关系，因此也称为净相关分析。在三个变量的偏相关分析中，任意两个变量的偏相关系数是消除余下一个变量的影响得到的，称为一阶偏相关系数；在四个变量的偏相关分析中，任意两个变量的偏相关系数称为二阶偏相关系数。以此类推，还有更高阶的偏相关系数，但超过4个变量的情况下，较少采用偏相关分析。\nR包ppcor提供了偏相关系数的计算和检验函数：pcor()和pcor.test()。下面对R内置数据集airquality的前四个变量(Ozone、Solar.R、Wind、Temp)进行相关分析和偏相关分析。\n\n例 4.42 对airquality数据集进行相关和偏相关分析。\n\n\nlibrary(ppcor)\nlibrary(dplyr)\ndf = airquality %&gt;% \n  dplyr::select(Ozone:Temp) %&gt;% \n  na.omit()\n# 计算简单相关系数\ncor(df, method = \"spearman\")\n\n             Ozone     Solar.R        Wind       Temp\nOzone    1.0000000  0.34818647 -0.60513642  0.7729319\nSolar.R  0.3481865  1.00000000 -0.06169636  0.2095369\nWind    -0.6051364 -0.06169636  1.00000000 -0.4993228\nTemp     0.7729319  0.20953692 -0.49932278  1.0000000\n\n# 计算偏相关系数\npcor(df, method = \"spearman\")\n\n$estimate\n             Ozone     Solar.R        Wind        Temp\nOzone    1.0000000  0.34980011 -0.43451018  0.66817934\nSolar.R  0.3498001  1.00000000  0.19474883 -0.08967312\nWind    -0.4345102  0.19474883  1.00000000 -0.04363683\nTemp     0.6681793 -0.08967312 -0.04363683  1.00000000\n\n$p.value\n               Ozone      Solar.R         Wind         Temp\nOzone   0.000000e+00 0.0001926827 2.349034e-06 2.064670e-15\nSolar.R 1.926827e-04 0.0000000000 4.243081e-02 3.537753e-01\nWind    2.349034e-06 0.0424308090 0.000000e+00 6.523181e-01\nTemp    2.064670e-15 0.3537753412 6.523181e-01 0.000000e+00\n\n$statistic\n            Ozone    Solar.R       Wind       Temp\nOzone    0.000000  3.8623681 -4.9903112  9.2899174\nSolar.R  3.862368  0.0000000  2.0538218 -0.9313381\nWind    -4.990311  2.0538218  0.0000000 -0.4518132\nTemp     9.289917 -0.9313381 -0.4518132  0.0000000\n\n$n\n[1] 111\n\n$gp\n[1] 2\n\n$method\n[1] \"spearman\"\n\n\n从计算结果可以发现，Ozone与Solar.R、Wind和Temp三个变量的偏相关系数都具有显著性，其中与Solar.R的偏相关系数比简单相关系数略有增大，而与Wind和Temp两个变量的偏相关强度都有所减弱。因此，多变量的两两变量相关分析应采用偏相关分析，以消除其他变量的影响，从而真实呈现两两变量之间的相关性。\n\n\n4.3.1.3 多变量复相关系数的计算与检验\n在三个及以上多变量的相关分析中，复相关分析是要考察其中一个变量y(因变量)受其他一组变量x_i(自变量)的综合影响。采用复相关系数(亦称多重相关系数)来量化表征一个变量与一组变量之间的相关程度，其取值范围为[0, 1]。复相关分析常用于多元线性回归分析。\n计算复相关系数需要先建立y与x_i的多元线性回归方程，再基于多元线性回归方程代入x_i的观测值，计算出y的预测值\\hat{y}，然后计算y与\\hat{y}的简单相关系数即为y与x_i的复相关系数。例 4.43 演示了R中三种计算复相关系数的方法。\n\n例 4.43 计算airquality数据集中Ozone变量与Solar.R、Wind和Temp三个变量的复相关系数。\n\n\nlibrary(dplyr)\ndf = airquality %&gt;% \n  dplyr::select(1:4) %&gt;% \n  na.omit()\nfm = lm(Ozone ~ ., data = df)\ny = df$Ozone\nyp = fm$fitted.values\nR1 = cor(y, yp)\nR1\n\n[1] 0.7783923\n\nR2 = cov(y, yp)/sqrt(var(y) * var(yp))\nR2\n\n[1] 0.7783923\n\nR3 = sqrt(summary(fm)$r.sq)\nR3\n\n[1] 0.7783923\n\n\n以上三种方法的计算结果完全一致，其中第二种方法借助协方差计算函数cov()，第三种方法是取回归模型的决定系数(R^2)的平方根。\nrstatix和easystats是与tidyverse设计理念一致的tidy风格的统计分析工具包，整合了R及相关工具包的统计分析与推断的功能，具体功能和使用可阅读相关帮助信息。\n\n练习 4.4 \n基于R内置数据集mtcars，选择mpg、disp、hp、drat、wt和qsec等6个变量，\n(1)计算两两相关系数矩阵并绘制相关系数矩阵图；\n(2)计算mpg、wt和qsec之间偏相关系数；\n(3)计算mpg与其他5个变量的复相关系数。\n\n\n\n\n4.3.2 回归分析\n回归分析是一种定量描述变量间相互依赖关系的统计分析方法，常用于截面数据。一个自变量(也称预测变量)与一个因变量(也称响应变量)之间的回归分析称为一元回归分析(也称简单回归分析)，多个自变量与一个因变量之间的回归分析称为多元回归分析(也称多重回归分析)。根据自变量与因变量之间的关系类型，可分为线性回归分析和非线性回归分析。\n相关分析只是定量描述了变量间相关关系的性质和程度，回归关系则是定量描述变量间相关依赖的函数形式。相关分析是回归分析的基础和前提，回归分析是相关分析的深入和继续，能够量化解释因变量对自变量的依赖性质和程度，并可在一定范围内进行预测。\n\n4.3.2.1 线性回归\n线性回归是在因变量和自变量之间建立如下的线性回归模型：\n\n  Y = X \\cdot \\beta + \\epsilon\n\\tag{4.29}\n其中Y为因变量，X为自变量(包含一个或多个自变量)，\\beta为回归系数(也称权重)\\epsilon为随机误差，且假定\\epsilon \\sim N(0,\\sigma)。每个自变量对应回归系数的符号(正或负)表明该自变量和因变量之间相关关系的方向，每个自变量的回归系数的绝对值表示在模型中其他自变量保持不变的情况下，该自变量变化一个单位，因变量的平均值就会发生多大的变化。\nR中lm()函数用于求解回归系数，其算法是基于普通最小二乘法(OLS)。该函数和aov()函数一样采用formula参数来指定因变量和自变量。\n\n例 4.44 以airquality数据集中Ozone为因变量、Solar.R为自变量建立一元线性回归模型。\n\n\nfm = lm(Ozone ~ Solar.R, data = airquality)\nsummary(fm)\n\n\nCall:\nlm(formula = Ozone ~ Solar.R, data = airquality)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-48.292 -21.361  -8.864  16.373 119.136 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 18.59873    6.74790   2.756 0.006856 ** \nSolar.R      0.12717    0.03278   3.880 0.000179 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 31.33 on 109 degrees of freedom\n  (42 observations deleted due to missingness)\nMultiple R-squared:  0.1213,    Adjusted R-squared:  0.1133 \nF-statistic: 15.05 on 1 and 109 DF,  p-value: 0.0001793\n\n\nsummary()给出的摘要信息中，Call部分显示了函数调用形式，Residuals部分给出了模型残差最小和最大值以及四分位数，Coefficients部分给出了回归系数的估计值、标准差、t检验统计量和显著性检验的p值，其中Intercept为截距(亦称回归常数，即\\beta_0)，接下来是各个自变量的回归系数。最后一段信息给出了残差标准差和自由度、模型决定系数(\\text{R}^2)和校正决定系数(\\text{R}_a^2)以及模型显著性检验(F检验)的结果；如有缺失值，还会给出因数据缺失而删除的观测数。\n每个自变量回归系数的p值大于检验水准时，即没有显著性意义，表明样本不足以拒绝在总体上该自变量与因变量之间相关性为零的原假设；当p值小于检验水准时，即有显著性意义，则表明样本足以拒绝在总体上该自变量与因变量之间相关性为零的原假设，该自变量可能值得添加到回归模型中。决定系数\\text{R}^2是自变量能够解释因变量变异的部分与因变量总变异的比例，即\n\n  \\text{R}^2 = \\frac {\\sum{(\\hat {y}_i - \\bar{y})^2}} {\\sum{(y_i - \\bar{y})^2}}\n  = 1 - \\frac {\\sum{(\\hat {y}_i - y_i)^2}} {\\sum{(y_i - \\bar{y})^2}}\n\\tag{4.30}\n\\text{R}^2常用于回归方程拟合优度的评价，但其只是相关性而非准确性的度量指标。随着回归方程中自变量个数的增加，\\text{R}^2会出现过高的估计，为此提出考虑自变量个数的\\text{R}_a^2，以避免对回归方程拟合优度的乐观估计(\\text{R}_a^2总是小于\\text{R}^2)。总是假定样本内观测数为n，每个观测的变量数为k，则\n\n  \\text{R}_a^2 = 1 - \\frac {\\sum{(\\hat {y}_i - y_i)^2}/(n-k-1)} {\\sum{(y_i - \\bar{y})^2}/(n-1)}\n  = 1 - (1 - \\text{R}^2) \\frac {n-1} {n-k-1}\n\\tag{4.31}\n另一个\\text{R}_a^2的计算公式是将上式中(n-p-1)改为(n-p)，这在一个自变量的情况下，\\text{R}_a^2 = \\text{R}^2。\n从 例 4.44 中summary()函数的摘要结果来看，回归模型和两个回归系数都具有显著意义，但从\\text{R}^2来看，模型的拟合优度很低。下面，将Temp和Wind变量引入回归模型，观察回归系数和决定系数的变化。\n\n例 4.45 以airquality数据中的Ozone为因变量，以 Solar.R、Temp及Wind为自变量，建立多元回归模型。\n\n\nfm1 = lm(Ozone ~ Solar.R + Temp + Wind, data = airquality)\nsummary(fm1)\n\n\nCall:\nlm(formula = Ozone ~ Solar.R + Temp + Wind, data = airquality)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-40.485 -14.219  -3.551  10.097  95.619 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -64.34208   23.05472  -2.791  0.00623 ** \nSolar.R       0.05982    0.02319   2.580  0.01124 *  \nTemp          1.65209    0.25353   6.516 2.42e-09 ***\nWind         -3.33359    0.65441  -5.094 1.52e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 21.18 on 107 degrees of freedom\n  (42 observations deleted due to missingness)\nMultiple R-squared:  0.6059,    Adjusted R-squared:  0.5948 \nF-statistic: 54.83 on 3 and 107 DF,  p-value: &lt; 2.2e-16\n\n\n显然，引入两个新的自变量后，新模型的回归系数和决定系数都发生了变化，其中截距变为负值且绝对值增大，而Solar.R的回归系数明显减小。此外，\\text{R}^2明显增大，残差减小，说明回归模型的拟合优度得到了改善。从 例 4.45 的结果可以发现，新引入的自变量Temp和Wind与因变量Ozone的相关性更高，对其影响大于Solar.R。\n回归模型建立后，除了利用summary()函数提取模型摘要信息外，还可以通过以下函数提取其他信息以进一步分析:\n\ncoefficients(): 提取回归模型的回归系数\nresiduals(): 提取回归模型的残差\nconfint(): 计算回归系数的置信区间(默认 95%)\nfitted(): 计算回归模型对建模数据集的拟合值\nanova(): 计算回归模型的方差分析表\nAIC(): 计算赤池信息统计量\nBIC(): 计算贝叶斯信息统计量\nplot(): 生成回归模型的诊断图形\npredict(): 利用回归模型对新数据集进行预测\n\n例如，利用confint()获取 例 4.45 中多元回归模型的回归系数的置信区间：\n\nconfint(fm1)\n\n                    2.5 %      97.5 %\n(Intercept) -110.04538108 -18.6387768\nSolar.R        0.01385613   0.1057851\nTemp           1.14949967   2.1546862\nWind          -4.63087706  -2.0363055\n\n\n回归系数体现了自变量变化对因变量的影响方向和程度，是回归模型解释功能的重要部分。\npredict()基于已经建立的回归模型，用新的自变量数据预测对应的因变量的值。对于线性回归模型，该函数提供点预测和区间预测两种功能，执行区间预测只需设置参数interval和level值即可。interval设置为”confidence”时，计算置信区间(窄区间)，设置为”prediciton”时，计算公差区间(宽区间)。level默认值是0.95，即计算置信概率为95%的区间。\n\nset.seed(2023)\nnew_data = data.frame(\n  Solar.R = sample(na.omit(airquality$Solar.R), 5),\n  Temp = sample(na.omit(airquality$Temp), 5),\n  Wind = sample(na.omit(airquality$Wind), 5)\n)\nnew_data\n\n  Solar.R Temp Wind\n1     167   81 10.3\n2     190   65 13.8\n3     223   92 11.5\n4     284   73  9.7\n5     273   82  7.4\n\n# 点预测\npredict(fm1, newdata = new_data)\n\n        1         2         3         4         5 \n45.131495  8.406312 62.654160 40.913915 62.791985 \n\n# 窄区间预测\npredict(fm1, newdata = new_data, interval = \"confidence\")\n\n        fit      lwr      upr\n1 45.131495 40.56799 49.69500\n2  8.406312  1.20245 15.61018\n3 62.654160 53.64414 71.66418\n4 40.913915 33.89565 47.93218\n5 62.791985 56.71742 68.86655\n\n# 宽区间预测\npredict(fm1, newdata = new_data, interval = \"prediction\")\n\n        fit        lwr       upr\n1 45.131495   2.895862  87.36713\n2  8.406312 -34.195550  51.00818\n3 62.654160  19.709970 105.59835\n4 40.913915  -1.656956  83.48479\n5 62.791985  20.366481 105.21749\n\n\n需要注意的是，新数据集中变量名必须与建立回归模型的变量名一致。另外，预测使用的新数据，严格意义上，各自变量的值应在建立回归模型使用的自变量值域内，否则预测的准确性将难以保证。\n除了对因变量进行预测，线性回归模型还可以反过来对自变量进行控制，即在确保因变量在某个概率下的置信区间时，如何控制自变量的取值范围，亦即反预测。一元线性回归模型的控制相对简单，但多元线性回归的控制就很复杂，有兴趣的读者可研读相关文献资料。\n在多元线性回归中，经常需要考虑自变量之间的交互作用，这可以利用符号”:“或”*“来实现，例如考虑 例 4.45 中回归模型中Wind和Temp的交互作用对Ozone的影响：\n\nfm2 = lm(Ozone ~ Solar.R + Temp * Wind, data = airquality)\nsummary(fm2)\n\n\nCall:\nlm(formula = Ozone ~ Solar.R + Temp * Wind, data = airquality)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-38.888 -11.938  -3.084   8.753  94.235 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -245.08368   46.84632  -5.232 8.53e-07 ***\nSolar.R        0.06599    0.02152   3.067 0.002745 ** \nTemp           3.91373    0.57217   6.840 5.26e-10 ***\nWind          14.38471    4.13249   3.481 0.000727 ***\nTemp:Wind     -0.22795    0.05259  -4.334 3.34e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 19.61 on 106 degrees of freedom\n  (42 observations deleted due to missingness)\nMultiple R-squared:  0.6652,    Adjusted R-squared:  0.6526 \nF-statistic: 52.66 on 4 and 106 DF,  p-value: &lt; 2.2e-16\n\n\n结果表明Temp和Wind交互项的回归系数具有显著性，这表明因变量与其中一个自变量的关系依赖于另外一个自变量的水平，例如Ozone与Temp的关系依Wind不同而不同。同时，模型残差进一步减小，\\text{R}^2也增大了一些。\n\n\n4.3.2.2 非线性回归\n非线性关系在现实世界中极为普遍，其中一些非线性关系可以通过数学变换(如对数变换)转变为线性关系，称之为本质线性关系；而无法通过数学变换将其转化为线性关系的，则称之为本质非线性关系。对于前一种情况，对变量进行数学变换，然后对变换后的新变量建立线性回归模型。对于后者，则需要采用非线性回归算法进行建模。\n尽管本质线性关系的非线性问题可以使用线性回归模型，但往往会降低模型的可解释性。例如一级动力学方程C_t=C_t e^{-kt} 、米氏方程v_0=v_{max} C/(k_m+C)等，按其方程固有形式估计出的参数都有清晰的解释，如一级动力学模型中k是反应速率，米氏方程中的v_{max}是指最大反应速率。\n统计分析中，可以利用多项式回归、样条回归、广义加法模型(GAM)等方法来拟合变量间的非线性关系，这里仅介绍基于lm()函数的多项式回归。\n\n例 4.46 基于一个三次多项式方程构造一个包含50个观测的数据集，然后分别建立直线回归、2阶多项式回归和3阶多项式回归模型，并对拟合结果进行可视化。\n\n\nlibrary(tidyverse)\n# 构造数据集\nset.seed(2022)\nx = seq(0.5, 20, by = 0.5)\ndf = data.frame(\n  x = x,\n  y = 20 + 0.5 * (x - 0.5)^3 + rnorm(40, mean = 30, sd = 100)\n)\n# 绘制散点图\np = ggplot(df,aes(x = x, y = y)) +\n  geom_point(size = 1.5, alpha = 0.5) + theme_bw()\n# 建立直线回归方程并可视化拟合结果\nfm1 = lm(y ~ x, data = df)\n# 绘制fm1的拟合曲线\np = p + geom_line(aes(x, fitted(fm1)), size = 0.6, color = \"blue\")\n# 建立2阶多项式回归方程并可视化拟合结果\nfm2 = lm(y ~ poly(x, 2), data = df)\n# 绘制fm2的拟合曲线\np = p + geom_line(aes(x, fitted(fm2)), size = 0.6, color = \"green\")\n# 建立3阶多项式回归方程并可视化拟合结果\nfm3 = lm(y ~ poly(x, 3), data = df)\n# 绘制fm3的拟合曲线\np = p + geom_line(aes(x, fitted(fm3)), size = 0.6, color = \"red\")\np    \n\n\n\n\n\n\n\n\n下面利用anova()函数对三个模型进行显著性检验：\n\nanova(fm1, fm2, fm3)\n\nAnalysis of Variance Table\n\nModel 1: y ~ x\nModel 2: y ~ poly(x, 2)\nModel 3: y ~ poly(x, 3)\n  Res.Df     RSS Df Sum of Sq      F    Pr(&gt;F)    \n1     38 7512978                                  \n2     37  520393  1   6992585 900.96 &lt; 2.2e-16 ***\n3     36  279405  1    240988  31.05 2.593e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n结果表明直线回归模型fm1没有显著性，而2阶和3阶多项式回归模型fm2和fm3都具有显著性。\n很多领域针对特定问题已经建立了相应的数学模型，如一级动力学方程、米氏方程等，这类问题需要估计模型参数，此时，可以用R中基于非线性最小二乘法的函数nls()来解决。该函数通过迭代来求解参数，一般需要根据经验预先设定参数初始值：\n\nmodel = nls(y ~ a * exp(b * x), data = df, start = list(a = ..., b = ...), ...) \n\n参数初始值对迭代计算具有极大影响，其设定需要经验，主观性太强。为此，R提供了部分常用方程的自启动模型(Self-Satrt models)，避免人为设置初始参数。输入?selfStart，即可打开关于自启动模型的帮助页面，查询相关自启动模型参数设置的函数。下面是其中的部分自启动模型函数：\n- SSfol(Dose, input, lKe, lKa, lCl)：一级动力学房室模型\n- SSbiexp(input, A1, lrc1, A2, lrc2)：双指数模型\n- SSgompertz(x, Asym, b2, b3)：Gompertz增长曲线模型\n- SSlogis(input, Asym, xmid, scal)：逻辑斯蒂曲线模型\n- SSmicmen(input, Vm, K)：米氏方程模型\n例如，对米氏方程采用自启动模型：fit = nls(y ~ SSmicmen(x, vm, k), data = df)。\n\n例 4.47 利用nls函数求解一级动力学方程的参数。\n\n\n# 构造数据集\nset.seed(2022)\nx = seq(0, 20, length.out = 40)\ndf = data.frame(\n  x = x,\n  y = 5.5 * exp(-0.52 * x) + rnorm(40, 0.08, 0.2))\n# 用nls求解参数\nfm = nls(y ~ a * exp(-b * x), data = df,\n         start = c(a = 1, b = 0.1)) \nsummary(fm)\n\n\nFormula: y ~ a * exp(-b * x)\n\nParameters:\n  Estimate Std. Error t value Pr(&gt;|t|)    \na  5.54533    0.15719   35.28   &lt;2e-16 ***\nb  0.53944    0.02468   21.86   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1921 on 38 degrees of freedom\n\nNumber of iterations to convergence: 7 \nAchieved convergence tolerance: 1.491e-06\n\n# 绘制拟合图形\nggplot(df) +\n  geom_point(aes(x,y),size = 1, alpha = 0.5) +\n  geom_line(aes(x, fitted(fm)),color = \"red\") +\n  theme_bw()\n\n\n\n\n\n\n\n\n从摘要信息中可见，参数a和b的求解结果非常接近于前面构造数据所使用的参数值；从拟合图形可见拟合程度较好。\n\n\n4.3.2.3 线性回归模型诊断与改善方法\n基于OLS的线性回归模型有以下六个假设：\n(1)线性：因变量和自变量之间满足线性关系；\n(2)可加性：不同自变量对因变量的影响可直接相加；\n(3)独立性：自变量之间互不相关；反之，自变量之间存在多重共线性；\n(4)随机性：残差序列完全随机，特别是时间序列数据；反之，残差存在自相关；\n(5)正态性：残差满足正态分布；\n(6)方差齐性：残差的方差为常数，也称同方差性。\n线性回归模型诊断包括模型残差分析(检验随机性、正态性和方差齐性)、自变量多重共线性分析以及其他影响因素的分析。\nplot()可以绘制线性回归模型(lm()返回的对象)的4个诊断图形，对残差的随机性(反映自变量与因变量有无线性关系)、正态性、方差齐性以及离群点(模型拟合误差很大的数据点)、强影响点(对模型参数估计影响过大的数据点)和高杠杆值点(自变量空间中的离群点，因变量不参与计算)，但无法检验独立性和多重共线性。\ncar包提供了更丰富的诊断函数，其中qqPlot()绘制分位数比较图检验正态性，crPlot()绘制残差成分与残差图执行残差分析，durbinWatsonTest()检验残差的随机性，ncvTest()检验残差的方差齐性，spreadLevelPlot()绘制拟合值与残差图来判断残差的方差齐性，outlierTest()检验离群点，influencePlot()绘制残差与帽子值(hat统计量)的气泡图来显示离群点、强影响点和高杠杆值点，vif()计算方差膨胀因子来检验多重共线性。\ngvlma包中的gvlma()函数可以对线性模型的假设进行全局验证，同时检验偏度、峰度和异方差性。该函数返回的结果中，Global Stat项的决策结果直接给出该线性模型是否通过了综合检验。\n\n例 4.48 利用gvlma函数对 例 4.46 中fm2和fm3模型进行综合检验。\n\n\nlibrary(gvlma)\ngvlma(fm2)\n\n\nCall:\nlm(formula = y ~ poly(x, 2), data = df)\n\nCoefficients:\n(Intercept)  poly(x, 2)1  poly(x, 2)2  \n      993.5       6445.6       2644.3  \n\n\nASSESSMENT OF THE LINEAR MODEL ASSUMPTIONS\nUSING THE GLOBAL TEST ON 4 DEGREES-OF-FREEDOM:\nLevel of Significance =  0.05 \n\nCall:\n gvlma(x = fm2) \n\n                      Value   p-value                   Decision\nGlobal Stat        21.41383 2.621e-04 Assumptions NOT satisfied!\nSkewness            0.28416 5.940e-01    Assumptions acceptable.\nKurtosis            1.38066 2.400e-01    Assumptions acceptable.\nLink Function      19.72332 8.950e-06 Assumptions NOT satisfied!\nHeteroscedasticity  0.02569 8.727e-01    Assumptions acceptable.\n\ngvlma(fm3)\n\n\nCall:\nlm(formula = y ~ poly(x, 3), data = df)\n\nCoefficients:\n(Intercept)  poly(x, 3)1  poly(x, 3)2  poly(x, 3)3  \n      993.5       6445.6       2644.3        490.9  \n\n\nASSESSMENT OF THE LINEAR MODEL ASSUMPTIONS\nUSING THE GLOBAL TEST ON 4 DEGREES-OF-FREEDOM:\nLevel of Significance =  0.05 \n\nCall:\n gvlma(x = fm3) \n\n                    Value p-value                Decision\nGlobal Stat        5.3380  0.2543 Assumptions acceptable.\nSkewness           1.3014  0.2540 Assumptions acceptable.\nKurtosis           0.2838  0.5942 Assumptions acceptable.\nLink Function      1.6340  0.2012 Assumptions acceptable.\nHeteroscedasticity 2.1188  0.1455 Assumptions acceptable.\n\n\n检验结果表明，二阶多项式模型fm2不满足假设，而三阶多项式模型fm3符合假设。\n对于有很多自变量的多元线性回归模型，需要检验多重共线性。此时可以先借助car包中vif()计算出模型中每个自变量的VIF值，然后对其取平方根(sqrt()函数)，结果大于2的变量被认为存在多重共线性。\n改善线性回归模型的方法包括：\n(1)删除离群点和强影响点，删除后需要重新建立回归模型；或采用稳健回归(MASS包中的rlm()函数)；\n(2)存在多重共线性的，可以采取逐步回归法(step()函数或MASS包中的stepAIC()函数)剔除自变量；或采用岭回归方法(ridge regression)或拉索回归(LASSO regression)，这两种回归分析需要借助glmnet包。另一个值得考虑的方法是利用主成分分析提取主成分，然后以主成分作为自变量建立回归模型，或直接采用pls包中的pcr()函数，该包中的plsr()函数执行偏最小二乘回归建模，特别适用于观测数较少而变量数过多的数据；\n(3)如果违背正态性假设，可采用非参数回归模型；或采用Box-Cox变量变换方法(car包中powerTransform()函数)；\n(4)如果存在异方差性，可利用car包中的spreadLevelPlot()函数来改善；\n(4)如果违背线性假设，可采用car包中的boxTidwell()函数来改善；如果存在显著的非线性关系，应采用非线性回归方法；\n(5)如果残差存在自相关性，应考虑采用时间序列分析方法或多层次回归方法。\n线性混合模型(nlme包)和广义线性模型(glmnet包)可以用于很多不符合普通线性回归假设的情况，可以进一步学习并掌握。另外，quantreg包提供的分位数回归(rq()函数)是一种非参数回归方法，可以估算因变量的条件概率分布。\n\n练习 4.5 基于R内置数据集mtcars，选择mpg、disp、hp、drat、wt和qsec等6个变量，\n(1)以mpg为因变量，其他5个变量为自变量，建立多元线性回归模型；\n(2)利用gvlma包中的gvlma()函数对模型进行综合检验；\n(3)提出改善模型的方法。",
    "crumbs": [
      "进阶知识",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>环境数据统计分析</span>"
    ]
  },
  {
    "objectID": "CH4.html#主成分分析与因子分析",
    "href": "CH4.html#主成分分析与因子分析",
    "title": "4  环境数据统计分析",
    "section": "4.4 主成分分析与因子分析",
    "text": "4.4 主成分分析与因子分析\n主成分分析(PCA)和因子分析(FA)很相似，都是一种多变量降维分析方法。主成分分析属于数学分析方法，而因子分析属于统计分析方法。主成分分析方法主要用来解决多变量多重共线性问题，计算得到的主成分之间互相独立，计算结果唯一，常用于多元回归分析、机器学习建模等，并衍生出主成分回归(PCR)、偏最小二乘回归(PLSR)等回归分析模型。因子分析方法用来探索或验证公共因子的结构模型，重点在于找到并解释现有变量背后的少数控制因素(即公共因子)，而且这些控制因素具有实际意义但往往又难以观测，其计算结果不唯一。\n\n4.4.1 主成分分析\n对于高维数据，每一个样本单位(一个观测)的特征(变量)数非常多，甚至多于样本含量(表现为数据表中列数远多于行数)，例如一个样品的红外光谱数据包含上千个波长的吸光度，基因数据的变量也是成千上万。这些变量间存在错综复杂的相关性，很难深入理解，也不能建立稳健的多元回归模型。主成分分析是一种应用广泛的数据降维算法，基于正交变换将n个原始变量转换为n个正交的新变量，即主成分(PCs)。本质上，每个主成分是n个原始变量的线性组合，且各主成分之间互不相关。主成分对数据总方差的解释比例依次从大到小，一般选择累积方差可解释比例(累积贡献率)≥85%的前k(通常k≤3)个主成分数据代表原始数据用于后续的分析与建模，以免丢弃更多原始数据的信息。降维后的k个主成分只丢弃了原始数据的极少部分信息，但带来的好处是消除了数据中冗余的信息，并显著降低后续建模的计算量和模型的复杂度。\n主成分分析不要求数据来自正态总体，但要求变量间存在相关性。主成分分析的步骤：\n(1)确认原始变量间存在相关性(可使用Bartlett球形检验或Kaiser-Meyer-Olkin检验，对应于R包psych中的cortest.bartlett()函数和KMO()函数)；\n(2)对原始数据进行标准化处理以无量纲化；\n(3)计算协方差或相关系数矩阵；\n(4)基于协方差或相关系数矩阵求解特征值和特征向量；\n(5)将特征值从大到小排列，根据累积贡献率确定前k个特征值；\n(6)以特征值对应的特征向量为权重构建主成分(以特征向量为权重将原始变量线性组合为新变量)；\n(7)计算每个主成分对应的贡献率；\n(8)根据主成分的特征值和特征向量进行解释。\n实际上，基于原始数据计算得到的相关矩阵，与原始数据标准化处理后计算得到的协方差矩阵相同。\nR中内置的princomp()函数和prcomp()函数都可用于主成分分析，前者基于协方差(参数cor = FALSE，输入数据需要标准化处理)或相关系数矩阵(参数cor = TRUE，输入数据不需要标准化处理)进行特征分解得到特征值与特征向量，而后者提供了标准化处理参数，利用奇异值分解(SVD)从原始数据矩阵直接求解特征值与特征向量，计算速度快，在数据规模很大时具有优势，一般作为首选的方法。此外，很多R包如FactoMineR、psych、MASS等都提供了主成分分析功能的函数。factoextra是一个用于对包括主成分分析在内的多元统计分析的结果进行解释和可视化的包。\n\n例 4.49 利用prcomp()对iris数据集进行主成分分析。\n\n\ndf = iris[ , 1:4]\n# 相关性检验\nlibrary(psych)\ncortest.bartlett(cor(df), n = nrow(df))\n\n$chisq\n[1] 706.9592\n\n$p.value\n[1] 1.92268e-149\n\n$df\n[1] 6\n\n\n根据p值拒绝变量间相关系数为0的假设，说明可以对数据进行主成分分析。\n\nKMO(cor(df))\n\nKaiser-Meyer-Olkin factor adequacy\nCall: KMO(r = cor(df))\nOverall MSA =  0.54\nMSA for each item = \nSepal.Length  Sepal.Width Petal.Length  Petal.Width \n        0.58         0.27         0.53         0.63 \n\n\nKMO检验比Bartlett球形检验严格，相关性越高，总MSA(Measure of Sampling Adequacy，抽样充足性测度)值越接近于1，反之则越接近于0，大于0.5即表明存在相关性。\n\npca_iris = (prcomp(df,scale. = TRUE, center = TRUE))    # 标准化处理\nsummary(pca_iris)    # 对结果进行摘要\n\nImportance of components:\n                          PC1    PC2     PC3     PC4\nStandard deviation     1.7084 0.9560 0.38309 0.14393\nProportion of Variance 0.7296 0.2285 0.03669 0.00518\nCumulative Proportion  0.7296 0.9581 0.99482 1.00000\n\n\n以上结果表明第一个主成分对总方差的解释比例约73%，第二个主成分约23%，二者累计解释比例接近96%，后两个主成分的解释比例很低。因此可选择前2个主成分进行后续的分析与建模。\n可用$sdev单独返回标准差(本质上就是对应特征值的平方根)。方差本质上就是对应的特征值，方差比例就是通过特征值计算得到的。返回载荷矩阵通过$rotation：\n\npca_iris$rotation\n\n                    PC1         PC2        PC3        PC4\nSepal.Length  0.5210659 -0.37741762  0.7195664  0.2612863\nSepal.Width  -0.2693474 -0.92329566 -0.2443818 -0.1235096\nPetal.Length  0.5804131 -0.02449161 -0.1421264 -0.8014492\nPetal.Width   0.5648565 -0.06694199 -0.6342727  0.5235971\n\n\n载荷矩阵非常重要，因为通过载荷矩阵，可以将原始变量线性组合成各个主成分，从而得到主成分作为新变量的得分(新观测值)。下面来看主成分作为新变量的前6个观测：\n\nhead(pca_iris$x)\n\n           PC1        PC2         PC3          PC4\n[1,] -2.257141 -0.4784238  0.12727962  0.024087508\n[2,] -2.074013  0.6718827  0.23382552  0.102662845\n[3,] -2.356335  0.3407664 -0.04405390  0.028282305\n[4,] -2.291707  0.5953999 -0.09098530 -0.065735340\n[5,] -2.381863 -0.6446757 -0.01568565 -0.035802870\n[6,] -2.068701 -1.4842053 -0.02687825  0.006586116\n\n\n主成分分析结果的提取和可视化可借助factoextra包中的相关函数来实现，其中get_eig()提取主成分对应的特征值和总方差解释比例，get_pca()提取观测和变量的各种信息(坐标coord、相关系数cor、余弦平方cos2和贡献率contrib)；fviz_screeplot()用于绘制碎石图，可视化每个主成分对总方差的解释百分比，可用于确定最佳主成分数(根据折线图的拐点来判断)；fviz_pca_ind()基于前两个主成分来可视化样本中观测之间的距离；fviz_pca_var()基于矢量来可视化样本中变量的相关性；而fviz_pca_biplot()则是前两个函数的综合，同时可视化观测与变量。\n绘制pca_iris的碎石图：\n\nlibrary(factoextra)\nfviz_screeplot(pca_iris, xlab = \"主成分\", ylab = \"方差解释百分比(%)\", main = \"\")\n\n\n\n\n\n\n\n图 4.9: 主成分分析的碎石图\n\n\n\n\n\n基于主成分可以绘制观测点图：\n\nfviz_pca_ind(pca_iris,\n             mean.point = F,  # 删除中心点\n             label = \"none\",  # 删除标签\n             habillage = iris$Species,  # 分组\n             palette = c(\"orange\",\"blue\",\"red\"),  # 分组颜色标度\n             addEllipses = T,  # 添加椭圆\n             ellipse.type = \"convex\",  # 指定椭圆类型\n             repel = T)  # 避免重叠\n\n\n\n\n\n\n\n图 4.10: 主成分分析的样本散点图\n\n\n\n\n\n图 4.10 本质上是iris数据集中150个观测在两个主成分空间中的投影点。\n基于主成分还可以绘制变量图：\n\nfviz_pca_var(pca_iris, \n             col.var=\"contrib\",  # 根据贡献度着色\n             gradient.cols = c(\"green\", \"red\"),\n             repel = TRUE)\n\n\n\n\n\n\n\n图 4.11: 主成分分析的变量相关圆图\n\n\n\n\n\n图 4.11 中四个矢量代表4个原始变量，横轴(Dim1)代表第一个主成分PC1，纵轴(Dim2)代表第二主成分PC2。矢量与横轴和纵轴的夹角越小，意味着与对应主成分的相关性越强(同时也呈现了相关方向)；矢量在横轴和纵轴上的投影长度分别代表该变量对PC1和PC2的贡献程度。对于PC1，应横着看，其中Petal.Length贡献最大(红色)，其次是Petal.Width和Sepal.Length，这三个变量都是正向影响，而Sepal.Width的微弱负向影响可以忽略。对于PC2，四个变量都是负向影响，而Sepal.Width影响最大，其余三个的影响可以忽略。\n可以用get_pca_var()提取变量的坐标(主成分空间)、相关系数、平方余弦和贡献度。\n\nvar_res = get_pca_var(pca_iris) \n# 变量坐标\nvar_res$coord\n\n                  Dim.1       Dim.2       Dim.3       Dim.4\nSepal.Length  0.8901688 -0.36082989  0.27565767  0.03760602\nSepal.Width  -0.4601427 -0.88271627 -0.09361987 -0.01777631\nPetal.Length  0.9915552 -0.02341519 -0.05444699 -0.11534978\nPetal.Width   0.9649790 -0.06399985 -0.24298265  0.07535950\n\n# 相关性系数，亦即变量坐标\nvar_res$cor\n\n                  Dim.1       Dim.2       Dim.3       Dim.4\nSepal.Length  0.8901688 -0.36082989  0.27565767  0.03760602\nSepal.Width  -0.4601427 -0.88271627 -0.09361987 -0.01777631\nPetal.Length  0.9915552 -0.02341519 -0.05444699 -0.11534978\nPetal.Width   0.9649790 -0.06399985 -0.24298265  0.07535950\n\n# 平方余弦\nvar_res$cos2\n\n                 Dim.1       Dim.2       Dim.3        Dim.4\nSepal.Length 0.7924004 0.130198208 0.075987149 0.0014142127\nSepal.Width  0.2117313 0.779188012 0.008764681 0.0003159971\nPetal.Length 0.9831817 0.000548271 0.002964475 0.0133055723\nPetal.Width  0.9311844 0.004095980 0.059040571 0.0056790544\n\n# 贡献度\nvar_res$contrib\n\n                 Dim.1       Dim.2     Dim.3     Dim.4\nSepal.Length 27.150969 14.24440565 51.777574  6.827052\nSepal.Width   7.254804 85.24748749  5.972245  1.525463\nPetal.Length 33.687936  0.05998389  2.019990 64.232089\nPetal.Width  31.906291  0.44812296 40.230191 27.415396\n\n\n可以利用fviz_contrib()函数可视化变量对指定主成分的贡献：\n\n# 可视化变量对PC1的贡献\nfviz_contrib(pca_iris, choice = \"var\", axes = 1)\n# 可视化变量对PC2的贡献\nfviz_contrib(pca_iris, choice = \"var\", axes = 2)\n\n\n\n\n\n\n\n\n\n\n(a) PC1\n\n\n\n\n\n\n\n\n\n\n\n(b) PC2\n\n\n\n\n\n\n图 4.12: 变量对主成分的贡献率条形图\n\n\n\n\n\n\n4.4.2 因子分析\n因子分析是主成分分析方法的推广，其目的是从大量相关的原始变量中提取少数更具解释性的潜在变量，即公共因子。因子分析的结果是将全部原始变量的信息由少数公共因子的线性组合与一个特殊因子(表征公共因子不能解释的剩余信息)的和来表征。与主成分相比，公共因子具有更好的可解释性和实际意义。因子分析与主成分分析的算法相似，但主成分是原始变量的线性组合，解是唯一的，而公共因子通过线性组合与特殊因子构成原始变量，解不是唯一的。\n因子分析分为R和Q两种类型，前者是对变量进行因子分析，后者是对观测(样品)进行因子分析。因子分析还分为探索性因子分析(EFA)和验证性因子分析(CFA)，前者用于从原始变量中发掘公共因子并建立因子模型解释原始变量，应用广泛；而后者是检验已有因子模型拟合原始变量的能力，判断是否与预先假设一致，研究数据之间因果联系的结构方程模型(Sructural Equation Model，SEM)可以看成是CFA和路径分析的结合。本书仅介绍EFA方法，该方法要求变量之间存在相关关系，通常要求样本含量(观测数)不能太小，一般最少不低于50，且必须大于变量数，最好是变量数的5倍以上。\n探索性因子分析的步骤：\n(1)确认原始变量间存在较强相关关系(Bartlett球形检验或Kaiser-Meyer-Olkin检验)；\n(2)根据原始变量计算相关系数矩阵；\n(3)确定要提取的公因子数；\n(4)计算公因子及其载荷矩阵(常用方法有极大似然法、主轴因子法、最小二乘法、alpha因子分解法等)；\n(5)利用旋转方法(包括正交旋转和斜交旋转，前者要求数据服从多元正态分布)使因子具有更好解释性；\n(6)计算因子得分，用于后续分析、建模和解释。\nR中factanal()函数执行基于极大似然法的因子分析。psych包中的fa()函数可执行基于极大似然法、主轴因子法、最小二乘法、alpha因子分解法等多种方法的因子分析。\n\n例 4.50 以合肥市2021年冬季(2020年12月到2021年2月)6种空气污染物的日均浓度数据为例，演示psych包中fa函数在因子分析中的应用。\n\n\nlibrary(psych)\nlibrary(tidyverse)\ndf = read_csv(\"./data/hf-aqi-2021-winter.csv\")\ndf = df[ , 4:9]\n# 相关性检验\ncortest.bartlett(cor(df), n = nrow(df))\n\n$chisq\n[1] 157.8759\n\n$p.value\n[1] 6.526881e-26\n\n$df\n[1] 15\n\n\n根据p值，拒绝变量相关系数为0的假设，可以对数据进行因子分析。\n\nKMO(cor(df))\n\nKaiser-Meyer-Olkin factor adequacy\nCall: KMO(r = cor(df))\nOverall MSA =  0.66\nMSA for each item = \nPM2.5  PM10    CO   NO2   SO2 O3_8h \n 0.41  0.69  0.68  0.64  0.76  0.37 \n\n\n对于因子分析，KMO检验的总MSA值一般要求大于0.6，低于0.5则不建议进行因子分析。\n确定提取多少个公因子对于因子分析非常关键，可用psych包中的fa.parallel()函数确定最佳的公因子数：\n\nfa.parallel(cor(df), fm = \"pa\", fa = \"fa\")\n\nParallel analysis suggests that the number of factors =  3  and the number of components =  NA \n\n\n\n\n\n\n\n\n图 4.13: 平行分析碎石图\n\n\n\n\n\n该函数返回最佳公共因子数为3，同时返回一个平行分析的碎石图(图 4.13)。该函数中因子分解方法参数fm可选”minres”(最小残差法)、“ml”(极大似然估计法)、“uls”(与”minres”类似)、“wls”(加权最小二乘法)、“gls”(广义加权最小二乘法)以及 “pa”(主因子法)等，参数fa可选择”pc”、“fa”和”both”，以指定返回的结果和碎石图中显示主成分分析、因子分析或两者的结果。\n接着利用fa()函数计算公共因子并返回载荷矩阵等信息。该函数除了要指定公共因子数参数nfactors外，还要指定公共因子解法参数fm以及载荷旋转方法参数rotate。fm参数设置同fa.parallel()函数。rotate参数包括正交旋转法(常用的有”varimax”、“quartimax”等)和斜交旋转法(常用的有”promax”、“oblimin”等)，前者使公共因子之间保持不相关，后者则让公共因子变得相关，常用于改善公共因子的可解释性。当rotate参数设置为”none”则不执行旋转，设置的详细信息参考函数帮助信息。下面采用主因子法(fm = “pa”)和最大方差法(rotate = “varimax”)求解：\n\nfa_aqi = fa(cor(df), nfactors = 3, fm = \"pa\", rotate = \"varimax\")\nfa_aqi\n\nFactor Analysis using method =  pa\nCall: fa(r = cor(df), nfactors = 3, rotate = \"varimax\", fm = \"pa\")\nStandardized loadings (pattern matrix) based upon correlation matrix\n        PA1  PA2   PA3   h2   u2 com\nPM2.5 -0.06 0.63  0.03 0.40 0.60 1.0\nPM10   0.61 0.47 -0.02 0.60 0.40 1.9\nCO    -0.77 0.29  0.07 0.67 0.33 1.3\nNO2    0.93 0.07  0.04 0.88 0.12 1.0\nSO2   -0.61 0.01 -0.47 0.59 0.41 1.9\nO3_8h -0.01 0.02  0.47 0.22 0.78 1.0\n\n                       PA1  PA2  PA3\nSS loadings           2.21 0.71 0.45\nProportion Var        0.37 0.12 0.07\nCumulative Var        0.37 0.49 0.56\nProportion Explained  0.66 0.21 0.13\nCumulative Proportion 0.66 0.87 1.00\n\nMean item complexity =  1.3\nTest of the hypothesis that 3 factors are sufficient.\n\ndf null model =  15  with the objective function =  1.83\ndf of  the model are 0  and the objective function was  0 \n\nThe root mean square of the residuals (RMSR) is  0 \nThe df corrected root mean square of the residuals is  NA \n\nFit based upon off diagonal values = 1\nMeasures of factor score adequacy             \n                                                   PA1  PA2   PA3\nCorrelation of (regression) scores with factors   0.96 0.77  0.67\nMultiple R square of scores with factors          0.91 0.60  0.44\nMinimum correlation of possible factor scores     0.82 0.20 -0.11\n\n\n返回结果首先给出的是标准化载荷矩阵，显示了三个因子对每个变量的标准化载荷。载荷大小表明因子对该变量的解释程度，其绝对值越接近1，解释程度越高，越接近0，解释程度越低，一般以0.5为临界值，PA1对PM10、CO、NO2和SO2的载荷绝对值都大于0.6，PA2仅对PM2.5的载荷绝对值大于0.6，PA3对SO2和O3_8h的载荷最大，但绝对值低于0.5。h2表示提取的因子对每个变量的解释程度，本例中三个因子对NO2的解释程度最高，而u2=1-h2，意义与h2相反。com是霍夫曼复杂度指数，反映原始变量受几个因子支配，本例中，对于PM10和SO2变量，都受两个因子的支配。\n接下来的矩阵给出了三个因子的载荷平方和(SS Loadings)、所解释的方差(Proportion Var)、累积所解释的方差(Cumulative Var)、方差解释比例(Proportion Explained)和累积方差解释比例(Cumulative Proportion)。结果表明，三个因子所解释的累积方差只有0.56，其中PA1占比最大，为66%。\n假设检验结果表明三个因子足以解释这些变量。\n返回结果还包括因子得分充分性的测度，如相关性、R2、最小相关系数等。\n利用fa.diagram()函数可以绘制出原始变量与公共因子之间的支配关系：\n\nfa.diagram(fa_aqi)\n\n\n\n\n\n\n\n图 4.14: 原始变量与公共因子之间的支配关系\n\n\n\n\n\n由 图 4.14 可见，前四个变量受第一个公共因子支配(红色表示载荷为负数)，PM2.5受第二个公共因子支配，O3_8h受第三个公共因子支配。在此基础上，可以尝试探究这三个公共因子的实际意义，PA1支配的4种污染物主要来自燃料燃烧、工业生产、机动车尾气排放等，可理解为初次污染物影响因子。PM2.5是PM10的组成部分，既有一次来源又有二次来源，还包括外源输入，因此PA2可能是综合的细颗粒物影响因子(PM10在PA2上的载荷为0.47)。O3是光化学反应产生的二次污染物，因此PA3可理解为二次污染物影响因子。\n因子分子主要目的在于探究支配或影响原始变量的公共因子结构，而PCA方法则非常注重主成分的得分，以克服多变量多重共线性，用于建立定量模型。如果需要计算公共因子的得分，可以利用psych包中的facotr.scores()函数来计算，但计算结果因不同估计方法而不具有唯一性，而PCA中主成分的得分是唯一的。\n\nfs = factor.scores(df, fa_aqi, method = \"Thurstone\")\n\n输入fs$scores即可浏览原始数据集中每个观测的公共因子变量的得分。\n结构方程模型是一种建立、估计和检验研究系统中多变量间因果关系的模型方法，利用图形化模型来展示研究系统中变量间的因果网络关系，可以替代多元回归、因子分析、协方差分析等方法，已经成为多元数据分析的重要工具，并在地学、生态学、环境科学与工程、医学、社会学、经济学等领域中应用十分广泛。如果需要执行SEM分析，可安装lavaan包，并在https://lavaan.ugent.be/网站学习该包的使用。\n\n练习 4.6 (1)安装pls工具包，导入该包中内置的gasoline数据集，该数据集包括60个汽油样品的近红外光谱数据(NIR矩阵)。NIR矩阵的401列对应401个波长(从900nm到1700nm间隔2nm一个波长)的漫反射系数，octane为对应的辛烷值(octane)。对NIR矩阵进行主成分分析，并对前2个主成分进行可视化；然后以octane为因变量，以前2个主成分为自变量建立线性回归模型，并进行综合检验。\n(2)基于内置数据集mtcars，执行因子分析；绘制前2个因子的载荷散点图，并标注对应的变量。",
    "crumbs": [
      "进阶知识",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>环境数据统计分析</span>"
    ]
  },
  {
    "objectID": "CH4.html#聚类分析和判别分析",
    "href": "CH4.html#聚类分析和判别分析",
    "title": "4  环境数据统计分析",
    "section": "4.5 聚类分析和判别分析",
    "text": "4.5 聚类分析和判别分析\n聚类分析和判别分析都属于分类研究方法。前者是在类型未知情况下，利用样品(观测)或变量间的相似性，对样品或变量进行分类(一般称之为簇)；后者是在类型已知情况下，利用样品的特征(变量)值按选定的判定规则进行归类。按照机器学习方法的分类方法，聚类分析属于无监督学习，而判别分析属于有监督学习，前者主要用于特征降维，后者主要用于分类任务。\n\n4.5.1 聚类分析\n聚类分析也分为R型和Q型，前者对变量进行聚类，后者对样品进行聚类。通常，将聚集在一起的观测集合称之为一个簇(cluster)。聚类分析方法有很多种，包括基于划分的聚类(如k-means法等)、基于层次的聚类(如AGNES法等)、基于密度的聚类(如DBSACN法等)、基于网格或图的聚类(如STING法等)和基于模型的聚类(如EM法、SOM神经网络等)。聚类分析的目标就是使簇内相似度最高，簇间相似度最低。\n聚类分析中相似度的测度有很多，例如欧氏距离、明氏距离(欧式距离的推广)、曼哈顿距离、马氏距离、切比雪夫距离、海明距离、余弦相似度、杰卡德相似系数(也称杰卡德指数)、杰卡德距离(杰卡德相似系数的补集，即与杰卡德相似系数之和为1)等。根据这些相似度的计算方法，可以编写相应的R语言聚类分析函数。\nR语言已经预置了kmeas()和hclust()两个函数，分别用于执行k-means聚类和层次聚类(也称系统聚类)，此外，R包cluster、mclust、flexclust、fpc等还提供了更多类型的聚类分析方法。下面以R内置的iris数据集为例，利用cluster和factoextra包来演示Q型聚类分析。\n\n例 4.51 对iris数据集进行聚类分析。\n\n\nlibrary(tidyverse)\nlibrary(factoextra)\nlibrary(cluster)\ndf = iris %&gt;% dplyr::select(1:4)\n\n先对各变量的观测值进行数值标准化：\n\ndfs = scale(df)\n\n接着用get_clust_tendency()函数对聚类趋势进行评估：\n\nct = get_clust_tendency(dfs, 50)\n\n返回的结果为Hopkins统计量，该统计量的值越接近1，表明数据集的可聚类性越高，一般要求大于0.5。\n\nct$hopkins_stat\n\n[1] 0.7997314\n\n\n下面用eclust()函数进行聚类分析。该函数先估计最佳聚类簇数，然后根据最佳聚类簇数和指定的聚类算法进行聚类。如果指定了参数k的值，则无需估计最佳聚类簇数，直接根据k的值进行聚类分析。实际上，eclust()估计最佳聚类簇数是调用cluster包中的clusGap()函数，该函数通过自举抽样方法计算从1到指定最大簇数(参数k.max，默认值为10)的Gap统计量来确定最佳聚类簇数。\n\n# 在1~6之间确定最佳簇数，并调用kmeans()函数执行聚类分析\nkmc = eclust(df, FUNcluster = \"kmeans\", k.max = 6, nboot = 200, \n             seed = 2022, graph = FALSE)\nkmc$gap_stat\n\nClustering Gap statistic [\"clusGap\"] from call:\ncluster::clusGap(x = x, FUNcluster = fun_clust, K.max = k.max, B = nboot, verbose = verbose)\nB=200 simulated reference sets, k = 1..6; spaceH0=\"scaledPCA\"\n --&gt; Number of clusters (method 'firstSEmax', SE.factor=1): 3\n         logW   E.logW        gap     SE.sim\n[1,] 4.551642 4.639505 0.08786357 0.02798343\n[2,] 3.808397 4.190886 0.38248862 0.02328599\n[3,] 3.519407 4.011100 0.49169324 0.02410995\n[4,] 3.446685 3.917451 0.47076570 0.02519009\n[5,] 3.301023 3.836240 0.53521663 0.02487646\n[6,] 3.248167 3.760672 0.51250510 0.02451569\n\n\n计算结果建议最佳簇数为3，这与iris数据集中的实际分类一致。 下面用factoextra包中的fviz_gap_stat()可视化最佳聚类簇数估计的结果，如 图 4.15 所示：\n\nfviz_gap_stat(kmc$gap_stat)\n\n\n\n\n\n\n\n图 4.15: 通过Gap统计量估计最佳聚类簇数\n\n\n\n\n\n接着使用fviz_cluster()对样品聚类结果进行可视化，结果如 图 4.16 所示：\n\nfviz_cluster(kmc, df)\n\n\n\n\n\n\n\n图 4.16: k-means聚类结果的样品簇分布\n\n\n\n\n\n已知iris中1-50、51-100以及101-150号样品分别归属为setosa、versicolor和virginica，查看哪些样品的聚类结果出现了偏差可以输入view(as_tibble(kmc$cluster))。\n聚类结果有效性的评价方法分为外部评价法和内部评价法。外部评价法用于类别已知的数据集，包括纯度(Purity)、标准化互信息(Normalized Mutual Information)、兰德指数(Rand Index)、F值(F-score)等。内部评价法用于类别未知的数据集，同时考虑簇内和簇间相似度的评价指标有轮廓系数(Silhouette Coefficient)、CH指数(Calinski-Harabaz Index)、DB指数(Davies-Bouldin Index)、DV指数(Dunn Validity Index)等。这里只介绍轮廓系数的计算及其在R中的实现。对于某个样品，令a是其与同簇中其他样品的平均距离，b是与其距离最近的不同簇中样品的平均距离，则该样品的轮廓系数为\n\n  s=\\frac{b-a}{\\text{max}⁡(a,b)}\n\\tag{4.32}\n对于样品集合，其轮廓系数是所有样品轮廓系数的平均值。轮廓系数取值范围为[-1, 1]，取值越接近1表明聚类效果越好，越接近-1表明聚类效果越差。\ncluster包中silhouette()函数用于计算轮廓系数：\n\nsil = silhouette(kmc$cluster, dist(dfs))\n\n该函数返回以个n×3的矩阵(n为集合中样品总数)，给出每个样品所在的簇(列名为cluster)、最近的族(列名为neighbor)和轮廓系数(列名为sil_width)。\n下面用fviz_silhouette()对轮廓系数进行可视化：\n\nfviz_silhouette(sil)\n\n  cluster size ave.sil.width\n1       1   38          0.36\n2       2   62          0.34\n3       3   50          0.64\n\n\n\n\n\n\n\n\n图 4.17: k-means聚类的轮廓系数\n\n\n\n\n\n图 4.17 显示簇3轮廓系数最大，簇1和簇2都较低，而且出现了负值。下面来查看负值对应的样品号：\n\nsil %&gt;% as_tibble() %&gt;% \n  mutate(row_ID = rownames(.)) %&gt;% \n  filter(sil_width &lt; 0)\n\n# A tibble: 6 × 4\n  cluster neighbor sil_width row_ID\n    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt; \n1       2        1  -0.226   51    \n2       2        1  -0.0669  57    \n3       2        1  -0.00928 66    \n4       2        1  -0.124   87    \n5       1        2  -0.0123  112   \n6       1        2  -0.342   135   \n\n\n接下来采用层次聚类法(调用hclust()函数)进行聚类。为了与上面的k-means聚类结果进行公平的比较，最佳聚类簇数保持一致，即k = 3。但要注意，如果基于hclust()函数进行自举抽样估计，推荐聚类簇数为k = 5，按此簇数进行聚类，结果可以发现，簇数的增加主要是在原簇1和簇2下进行了细分。\n\nhc = eclust(df, FUNcluster = \"hclust\", k = 3, graph = F)\n\n对样品聚类结果进行可视化，结果如 图 4.18 所示：\n\nfviz_cluster(hc, df, labelsize = 7, pointsize = 1, ggtheme = theme_bw())\n\n\n\n\n\n\n\n图 4.18: 层次聚类结果的样品簇分布\n\n\n\n\n\n计算轮廓系数并可视化，结果如 图 4.19 所示：\n\nsil_hc = silhouette(hc$cluster, dist(dfs))\n#rownames(sil_hc) = iris$Species\nfviz_silhouette(sil_hc)\n\n  cluster size ave.sil.width\n1       1   50          0.64\n2       2   64          0.34\n3       3   36          0.39\n\n\n\n\n\n\n\n\n图 4.19: 层次聚类的轮廓系数\n\n\n\n\n\n比较 图 4.16 与 图 4.18 ，可见k-means聚类和层次聚类的结果在簇1和簇2(层次聚类结果对应簇3和簇2)有差异，层次聚类结果略有改善。图 4.19 显示层次聚类方法的平均轮廓系数为0.45，略高于k-means聚类方法的0.44，且仍有个别样品的轮廓系数为负值，下面来查看这些样品：\n\nsil_hc %&gt;% as_tibble() %&gt;% \n  mutate(row_ID = rownames(.)) %&gt;% \n  filter(sil_width &lt; 0)\n\n# A tibble: 5 × 4\n  cluster neighbor sil_width row_ID\n    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt; \n1       2        3   -0.205  51    \n2       2        3   -0.222  53    \n3       2        3   -0.0557 57    \n4       2        3   -0.0955 87    \n5       3        2   -0.0326 112   \n\n\n结果显示只有5个样品的轮廓系数为负值，而k-means聚类结果则有6个样品为负值。\n传统的k-means聚类算法不能用于非凸数据集，而且对噪声和离群点非常敏感，大数据集的聚类结果容易陷入局部最优。kernlab包提供的基于核函数的k均值算法(kkmeans()函数)能够克服经典k均值算法的缺点。此外，fpc包提供了很多灵活的聚类算法，其中就包括基于密度的聚类算法(dbscan()函数)，同时也提供了聚类簇数估计、聚类结果有效性评估以及可视化的功能。mclust包提供了基于模型的聚类分析方法(还具有分类和密度估计功能)，通过期望最大化(EM)算法拟合高斯有限混合模型。pvclust包提供了基于多尺度自举抽样方法来评估层次聚类算法的不确定性。\n\n\n4.5.2 判别分析\n判别分析是在分类确定的条件下，根据样品的各种特征值(亦称判别变量)判定其类别归属问题的一种多元统计分析方法。判别分析需要先通过所谓的“训练样本”建立判别规则，然后利用判别规则对新的样品进行分类。 判别分析主要分为距离判别分析、Fisher判别分析和Bayes判别分析三大类。距离判别分析根据欧氏距离、马氏距离等测度建立判别规则。Fisher判别分析则是一种基于投影的方法，将高维数据投影到一维空间，并使得投影后的类间方差最大、类内方差最小，从而实现分类，包括线性判别分析(LDA)、二次判别分析(QDA)等。其中LDA假设每个类别的数据服从多元正态分布，且各类别的协方差矩阵相同，判别函数是线性的；而QDA则允许各类别的协方差矩阵不同，判别函数是非线性二次函数。Bayes判别分析(BDA)基于贝叶斯定理，利用先验概率和似然函数来计算后验概率，然后将样品划分到后验概率最大的类别中。\nR中MASS包中的lda()和qda()函数分别用于执行LDA和QDA。mda包中的mda()函数用于混合判别分析(MDA)，该方法仍然假设各类别的协方差矩阵相同，但每个类别被假设为子类的高斯混合)；该包中的fda()函数则用于弹性判别分析(FDA)，这是LDA的一种扩展，用于处理非线性关系或多变量非正态分布。klaR包中的rda()函数用于正则化判别分析(RDA)，对于存在多重共线性的数据集更稳健。klaR包中的NaiveBayes()函数和e1071包中的naiveBayes()函数都可用于执行BDA。距离判别分析可用kknn包中的kknn()函数，该函数执行k最近邻算法(k-Nearest Neighbor)，这也是一种原理简单但高效的机器学习算法。\n下面演示LDA和BDA在内置数据集iris上的应用。\n\n例 4.52 对iris数据集应用LDA和BDA。\n\n先加载相关的R包：\n\nlibrary(MASS)\nlibrary(klaR)\nlibrary(dplyr)\n\n载入iris数据集：\n\ndf = iris \n\n将数据集分割为训练集和测试集：\n\nset.seed(2022)\ndftr = df %&gt;% group_by(Species) %&gt;% sample_n(40) \ndfte = setdiff(df,dftr)\n\n基于训练集建立LDA判别模型：\n\nm_lda = lda(Species ~ ., data = dftr)\n\n对预测集进行预测：\n\npre_lda = predict(m_lda, dfte)\n\n比较实际分类和LDA模型分类结果：\n\n(tbl_tr_lda = table(dftr$Species, predict(m_lda, dftr)$class))\n\n            \n             setosa versicolor virginica\n  setosa         40          0         0\n  versicolor      0         39         1\n  virginica       0          0        40\n\n\n训练集上，LDA方法将1个veriscolor类的样品错分为virginica类。\n\n(tbl_te_lda = table(dfte$Species, pre_lda$class))\n\n            \n             setosa versicolor virginica\n  setosa         10          0         0\n  versicolor      0          9         1\n  virginica       0          1         9\n\n\n预测集上，LDA方法分别将versicolor和virginica类中的1个样品错分。\n下面来计算LDA方法的判对率：\n\ncat(\"训练判对率：\", sum(diag(prop.table(tbl_tr_lda))))\n\n训练判对率： 0.9916667\n\ncat(\"\\n\")    # 换行\ncat(\"预测判对率：\", sum(diag(prop.table(tbl_te_lda))))\n\n预测判对率： 0.9333333\n\n\n接下来采用klaR包中NaiveBayes()函数进行BDA。 基于训练集建立BDA模型：\n\nm_bda = NaiveBayes(Species ~ ., data = dftr)\n\n对预测集进行预测：\n\npre_bda = predict(m_bda, dfte)\n\n比较实际分类和BDA模型分类结果：\n\n(tbl_tr_bda = table(dftr$Species, predict(m_bda, dftr)$class))\n\n            \n             setosa versicolor virginica\n  setosa         40          0         0\n  versicolor      0         38         2\n  virginica       0          3        37\n\n\n在训练集上，与LDA相比，BDA错分了5个样品。\n\n(tbl_te_bda = table(dfte$Species, pre_bda$class))\n\n            \n             setosa versicolor virginica\n  setosa         10          0         0\n  versicolor      0         10         0\n  virginica       0          1         9\n\n\n在预测集上，与LDA相比，Bayes判别分析只错分了virginica类别中的1个样品。 下面来计算Bayes方法的判对率：\n\ncat(\"训练判对率：\", sum(diag(prop.table(tbl_tr_bda))))\n\n训练判对率： 0.9583333\n\ncat(\"\\n\")\ncat(\"预测判对率：\", sum(diag(prop.table(tbl_te_bda))))\n\n预测判对率： 0.9666667\n\n\nLDA模型可以通过plot()函数进行可视化，具体参考其帮助信息。klaR包中的partimat()函数可以基于指定的判别方法对数据集中所有判别变量的两两组合进行可视化，其中参数method可指定为”lda”、“qda”、“rpart”(决策树方法)、“naiveBayes”、“rda”、“sknn”(简单k最近邻方法)和”svmlight”(支持向量机方法，需要安装SVMlight软件)。\n\nlibrary(klaR)\npartimat(Species ~ ., data = iris, method = \"lda\", main = \"LDA\")\n\n\n\n\n\n\n\n图 4.20: 线性判别分析可视化\n\n\n\n\n\n\npartimat(Species ~ ., data = iris, method = \"qda\", main = \"QDA\")\n\n\n\n\n\n\n\n图 4.21: 二次判别分析可视化\n\n\n\n\n\n\npartimat(Species ~ ., data = iris, method = \"rda\", main = \"RDA\")\n\n\n\n\n\n\n\n图 4.22: 稳健判别分析可视化\n\n\n\n\n\n\nlibrary(klaR)\npartimat(Species ~ ., data = iris, method = \"naiveBayes\", main = \"BDA\")\n\n\n\n\n\n\n\n图 4.23: 贝叶斯判别分析可视化\n\n\n\n\n\n\nlibrary(klaR)\npartimat(Species ~ ., data = iris, method = \"rpart\", main = \"RPART\")\n\n\n\n\n\n\n\n图 4.24: 决策树判别分析可视化\n\n\n\n\n\n\nlibrary(klaR)\npartimat(Species ~ ., data = iris, method = \"sknn\", main = \"KNN\")\n\n\n\n\n\n\n\n图 4.25: k最近邻判别分析可视化\n\n\n\n\n\n\n练习 4.7 (1)从https://archive.ics.uci.edu/dataset/109/wine下载数据，基于葡萄酒化学分析变量进行聚类分析，然后根据class变量评价聚类分析的结果。\n(2)继续基于上述wine数据集，执行LDA和BDA。",
    "crumbs": [
      "进阶知识",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>环境数据统计分析</span>"
    ]
  },
  {
    "objectID": "CH5.html",
    "href": "CH5.html",
    "title": "5  R语言机器学习建模",
    "section": "",
    "text": "5.1 机器学习概述\n百度百科对人工智能的定义是：“人工智能是研究、开发用于模拟、延伸和扩展人的智能的理论、方法、技术及应用系统的一门新的技术科学”。机器学习是实现人工智能的一个重要途径，而深度学习则是机器学习的子领域。深度学习能够利用更强大的算力构建更复杂的深层神经网络模型，实现了特征工程的自动化，能够更好地从文本、语音、图像、视频等非结构性数据中学习内在规律和表示层次，代表了目前机器学习的最高水平。目前，除了我国学者周志华教授的团队于2020年提出的基于决策树集成技术的深度森林模型(gcForest)外，深度学习主要基于人工神经网络。\n卡内基梅隆大学汤姆·米歇尔教授对机器学习的定义：对于某类任务T和性能度量P，若一个计算机程序在T上用P衡量的性能随着经验E而自我完善，则称该计算机程序在从经验E学习。大多数场景下，机器学习就是让计算机通过算法对大量历史数据进行学习，生成经验模型，然后利用经验模型进行预测。机器学习更加注重模型的性能，如预测的准确性，而统计分析与建模则注重可解释性，但其理论仍然是机器学习的重要基础，并在机器学习模型可解释性的研究中发挥着重要作用。更重要的是，机器学习更擅长于解决现实世界中绝大部分的本质非线性关系的复杂问题。",
    "crumbs": [
      "进阶知识",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>R语言机器学习建模</span>"
    ]
  },
  {
    "objectID": "CH5.html#机器学习概述",
    "href": "CH5.html#机器学习概述",
    "title": "5  R语言机器学习建模",
    "section": "",
    "text": "5.1.1 机器学习的方式\n机器学习根据学习方式可分为有监督学习、无监督学习、半监督学习和强化学习。\n有监督学习使用一组由输入特征(Feature)和对应的输出标签(Label)构成的学习数据来建立模型，然后利用模型预测新的输入特征的输出标签(图 5.1 左图)。例如，根据已有的水质指标(输入特征)及对应富营养化程度(输出标签)的样本建立机器学习模型，然后用来预测新的输入特征对应的富营养程度。\n无监督学习使用只有输入特征而没有输出标签的学习数据，让机器学习算法挖掘学习数据中的内在规律和逻辑，形成合理的知识表示(图 5.1 右图)。例如，根据大量来自不同污染源的特征，让无监督学习算法自动分析其中哪些污染源是相似的，或者哪些是异常的。\n\n\n\n\n\n\n\n\n图 5.1: 有监督学习和无监督学习\n\n\n\n\n\n半监督学习是前两种学习方式的混合，数据中大多没有输出标签，少数有输出标签。半监督学习利用少量有标签数据点建立预测模型，为大量无标签数据点预测对应标签，然后利用所有数据建立机器学习模型。由于现实中存在大量无标签的数据，人工标签数据工作量极大甚至不可能，利用半监督学习算法进行自动化标签，可以让大量无标签数据得到充分利用。\n强化学习是在给定的环境中，通过环境的反馈机制和奖惩机制，让智能体以试错的方式去学习，改进行动方案以获胜或提升价值，从而适应环境。\n在机器学习和深度学习中，统计分析与建模中的自变量或预测变量一般称为特征，而因变量或响应变量、目标变量称为标签。\n由于机器学习涉及的知识范围极为广泛，本书只介绍基于R包mlr3的传统机器学习和基于R包torch的深度学习，并聚焦于有监督学习。\n\n\n5.1.2 机器学习的任务类型\n常见的机器学习任务类型包括分类、回归、聚类和降维，如 图 5.2 所示。\n\n\n\n\n\n\n\n\n图 5.2: 机器学习任务类型\n\n\n\n\n\n分类和回归任务属于有监督学习。尽管分类模型输出标签是类别，回归模型输出标签是数值，但本质相同，因为类别可以用离散变量甚至连续变量(如概率值)来表示。\n聚类是一种典型的无监督学习。作为一种重要的归纳方法，聚类通常用于数据预处理和探索性分析，从大量数据聚焦到少数类别上，以获得更有价值的数据洞察。\n降维是将高维数据的维度降低，是在机器学习建模前对数据进行预处理的一种重要方法。大数据不仅观测多，更重要的是变量维度(特征)太多，如光谱分析数据、基因测序数据的变量动辄成千上万，存在大量冗余和噪声。降维算法能够显著减少冗余特征、弱化噪声干扰、降低存储空间占用、加快学习建模进程，并有利于实现可视化等。降维的途径主要有特征选择和特征提取。特征选择是从现有特征空间中取一个低维度的有代表性的子集，“取其精华、去其糟粕”，没有改变特征的物理意义。而特征提取则是将不同特征加以融合，形成新的特征空间，改变了原始特征的物理意义，主成分分析就是一种典型的特征提取算法。\n\n\n5.1.3 机器学习的基本术语\n\n5.1.3.1 算法与模型、参数与超参数\n机器学习算法是在数据上运行并建立机器学习模型的过程，称为算法从数据“学习(learning)”或“拟合(fitting)”的过程，也称为训练(training)模型的过程。机器学习模型是机器学习算法在数据上运行的输出结果。机器学习算法有很多种，如神经网络、决策树、支持向量机等。\n模型拥有确定的参数(parameters)，是算法参数空间中的一个具体示例。模型参数可以视为一种知识的归纳或表示，是算法从数据中学习或拟合得到的具体成果。算法在数据上运行之前需要根据学习任务的具体情况而设定的参数称为超参数(hyperparameters)。与模型参数不同，超参数不是算法从数据中学习得到的，而需要在学习前预先设定。超参数影响算法的学习过程，对模型性能有重要影响。例如神经网络的每个神经元层的激活函数类型、决策树的最大建树深度、支持向量机的惩罚系数等。算法的超参数可以通过调节或优化而让学习或建模过程更有效和更高效，从而提升最终模型的泛化性能(Generalization Performance)。\n\n\n5.1.3.2 训练集、测试集和验证集\n机器学习建模前，通常需要将学习数据分割成训练集(train set)和测试集(test set)，分割比例一般根据学习数据量的大小来确定，对于中小规模的数据，训练集比例一般为2/3~4/5，而大规模数据，可以适当减少训练集的比例。算法在训练集上学习以建立模型，模型在测试集上预测，通过计算预测值和真实值的偏差来评估模型的预测性能(估计模型泛化性能)。此外，往往还需要在训练集中分割出验证集(validation set)，通过交叉验证或早期停止等方法，优化算法超参数或控制算法过拟合。训练集和测试集的分割，要特别注意样本对总体的代表性问题。\n\n\n5.1.3.3 欠拟合和过拟合\n欠拟合(under-fitting)表现为模型训练误差(模型对训练集的拟合误差)和预测误差(模型对于测试集的预测误差)都很大，即所谓的高偏差、高方差( 图 5.3 左图)。这表明模型没有很好地学习到训练集中的知识。通常，提高模型复杂度可以有效改善欠拟合问题。\n过拟合(over-fitting)则表现为模型训练误差很小(甚至为0)而预测误差很大，即所谓的低偏差、高方差( 图 5.3 右图)。这表明模型在训练集上拟合过度，对数据噪声也进行了拟合，严重干扰了模型对有效知识的学习。通常，降低模型复杂度可以有效改善过拟合问题。\n机器学习的目的是获得恰当拟合的模型( 图 5.3 中图)，即实现偏差和方差的权衡，获得更好的预测性能。\n\n\n\n\n\n\n\n\n图 5.3: 欠拟合与过拟合\n\n\n\n\n\n\n\n5.1.3.4 目标函数、代价函数和损失函数\n在有监督学习中，学习数据的每一行都由一组输入特征组成，每一组特征对应一个输出标签，这样的一组特征和对应的标签即为学习样本的一个样例或观测(即统计学概念中的个体)。有监督学习的目的是获得一个预测性能优良的模型，机器学习模型本质上是机器学习算法对学习样本进行拟合而得到的函数。在拟合学习样本建立模型的过程中，需要计算模型对样例的预测标签和实际标签之间的偏差，然后通过指定的优化算法进行迭代学习，以将偏差降低到允许的或可接受的水平，从而获得可以部署应用的具有良好预测性能的模型。\n损失函数(loss function)用于计算单个样例的实际标签与模型预测标签之间的偏差，亦即损失。代价函数(cost function)用于计算模型对全体样例的损失，通常是所有样例损失函数计算结果的平均值。目标函数(objective function)通常是代价函数加上正则化项(也称为惩罚项，一般是模型结构复杂度的函数)，其中代价函数代表经验风险，用于控制训练误差，而正则化项代表结构风险，用于控制模型复杂度。机器学习建模的过程就是最小化目标函数的过程，如果只是最小化经验风险，结构足够复杂的模型就可以让经验风险为0，但模型会过拟合。因此，目标函数引入正则化项，以控制模型结构复杂度，在训练过程中同时最小化经验风险和结构风险，即实现训练误差最小化和模型结构最简化的权衡，以获得恰当拟合的模型，实现更好的泛化性能。\n对于回归任务，常用的损失函数是均方误差(MSE)和平均绝对误差(MAE)，前者让训练过程收敛更快，但后者对异常值更稳健。Huber损失函数则是 MSE 和 MAE的结合，在误差接近 0 时使用 MSE，在误差较大时使用 MAE。 对于分类任务，常用的损失函数是交叉熵(Cross Entropy，常用于二分类)，对于多分类任务，通常使用交叉熵的拓展形式即Softmax损失函数。 此外，机器学习中还有很多形式的损失函数，一些机器学习算法可能采用特殊的损失函数，例如支持向量机(SVM)模型采用的Hinge损失函数。\n\n\n5.1.3.5 泛化性能和交叉验证误差\n泛能性能是指模型推广到未见数据上的预测性能。显然，模型在测试集上的误差(测试误差)可以作为泛化性能的一个简单估计，但更优良的估计是模型的交叉验证误差(CVE)。交叉验证是指将训练集随机分割成n个互不重叠的子集(n折)，然后执行n个重复的迭代：取出1个子集作为验证集，余下n-1个子集合并作为训练集来训练中间模型，训练好的中间模型对验证集进行预测，然后计算本次迭代中的中间模型的性能测度。n个迭代结束后，计算n个性能测度的均值，即得到n折CVE。特别的，当n等于训练集的样例数量时，称为留一法(LOO)交叉验证，理论上，LOO法对应的CVE是模型泛化性能的最佳无偏估计。\n但模型训练的时间成本与n成正比。当训练集很大时，采用LOO或较大的n折交叉验证会显著提高时间成本和能耗，此时应考虑采用3、5(如 图 5.4 所示)或10-折交叉验证。建议在n折交叉验证基础上进行多次的重复，即重复n折交叉验证(Repeated n-fold CV)，以减小每一折内数据分布的差异。对于不平衡分类数据，需要采用分层分割的交叉验证方法，以提高每一折内各类样例的分布对总体具有更高的代表性。\n\n\n\n\n\n\n\n\n图 5.4: 5折交叉验证\n\n\n\n\n\n时间序列数据的顺序具有重要意义，不能随意打乱，不可采用随机分割的方法，其交叉验证一般按时间顺序划分训练集和验证集，如 图 5.5 所示。\n\n\n\n\n\n\n\n\n图 5.5: 时间序列数据的交叉验证",
    "crumbs": [
      "进阶知识",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>R语言机器学习建模</span>"
    ]
  },
  {
    "objectID": "CH5.html#机器学习建模的步骤",
    "href": "CH5.html#机器学习建模的步骤",
    "title": "5  R语言机器学习建模",
    "section": "5.2 机器学习建模的步骤",
    "text": "5.2 机器学习建模的步骤\n机器学习建模的步骤一般包括明确任务、数据收集、数据整理、特征工程、模型筛选、模型优化、模型评估和模型部署八个环节。\n(1)明确任务：明确任务的内容和目标，判定任务的类型，为选择模型确定范围。\n(2)数据收集：围绕任务收集相关数据，数据质量直接关系到机器学习建模质量。\n(3)数据整理：识别和处理错误值、重复值、缺失值、异常值等，开展数据变换(如归一化、标准化、对数化等)，进一步提升数据质量，并确保数据结构符合机器学习建模的要求。该环节也称为数据清洗。\n(4)特征工程：结合领域经验，尝试多种手段，选择或提取重要特征，形成高质量的建模数据。数据和特征决定了机器学习的上限，而机器学习算法只是逼近这个上限。因此，特征工程的好坏直接影响模型的性能。手动特征工程只适合中小规模数据，大数据建议采用自动化特征工程，但选择和提取特征的算法仍需要根据任务类型、数据分布、领域经验等确定。\n(5)模型筛选：选择若干合适的机器学习算法在训练集上预建模，采用统一的性能测度(亦即性能评价指标)，筛选出性能最优的算法。对特定的任务而言，没有最好的算法，只有最合适的算法。\n(6)模型优化：在训练集上用选定的算法进行学习，采用超参数优化算法并结合交叉验证等重抽样策略优化模型。模型优化往往会耗费较多的时间和算力成本。\n(7)模型评估：在测试集上估计模型的泛化性能，通常需要与基线模型(baseline model)和目前最先进模型(SOTA model)进行比较。如果不能满足任务要求，需要重新开展特征工程、模型选择及优化，甚至重新收集数据。\n(8)模型部署：将满足要求的最终模型部署到生产环境中应用，并定期监测和更新。\n特征工程和模型优化是耗费时间最多的两个环节，模型筛选和模型评估两个环节需要有多次的重复试验，以在不同的训练集、验证集和测试集上建模和评估，确保模型性能测度具有统计置信度，而不能只是一次试验的结果。此外，在模型筛选环节中，必然包含模型评估的步骤。",
    "crumbs": [
      "进阶知识",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>R语言机器学习建模</span>"
    ]
  },
  {
    "objectID": "CH5.html#mlr3机器学习建模基础",
    "href": "CH5.html#mlr3机器学习建模基础",
    "title": "5  R语言机器学习建模",
    "section": "5.3 mlr3机器学习建模基础",
    "text": "5.3 mlr3机器学习建模基础\nmlr3是目前R语言中功能全面、接口统一、面向对象且具有高扩展性的通用机器学习框架，支持data.table数据类型和R6范式的面向对象编程(OOP)，能够针对分类、回归、聚类、生存分析以及密度估计等多种类型的任务开展机器学习。此外，从caret发展而来的tidymodels也是一个正在不断完善的具有tidy规范的R语言通用机器学习框架。h2o是一个基于Java语言开发的分布式、可扩展开源机器学习框架，支持全自动机器学习(AutoML)。\n考虑到mlr3框架更加完善，且支持更广泛的机器学习算法，因此本书选择基于mlr3框架来介绍R语言有监督的机器学习建模。\n\n5.3.1 mlr3verse的安装\nmlr3只包含了机器学习的基本功能，为了使用更丰富的功能，建议安装mlr3verse套件，因为加载该套件可一次性导入mlr3、mlr3learners、mlr3tuning、mlr3pipelines、mlr3tuningspaces、mlr3data、mlr3filters、mlr3fselect、mlr3cluster、mlr3viz、bbotk、paradox等工具包，实现更高级的机器学习功能。\n从CRAN网站安装mlr3verse套件：\n\ninstall.packages(\"mlr3verse\")\n\n如果需要安装最新的开发版，可以通过remotes或devtools包的install_github()函数来安装：\n\nremotes::install_github(\"mlr-org/mlr3verse\")\n# devtools::install_github(\"mlr-org/mlr3verse\")\n\n在mlr-org的github仓库中还有很多扩展包，如mlr3extralearners(提供更多额外的学习器)、mlr3proba(概率有监督学习)、mlr3temporal(时间序列预测)等，亦可利用上述方法安装。\n需要注意的是，mlr3只是一个具有统一接口的通用机器学习框架，其中各个算法的实现仍然依赖其他R包，只有安装了相应的R包，才能在mlr3中正常调用这些机器学习算法进行建模。\nmlr3的生态系统如 图 5.6 所示。\n\n\n\n\n\n\n\n\n图 5.6: mlr3的生态系统(源自https://mlr-org.com/ecosystem.html)\n\n\n\n\n\nmlr3建模的基本步骤：\n(1)创建任务；\n(2)分割数据；\n(3)创建学习器；\n(4)训练模型；\n(5)模型预测；\n(6)模型性能评价。\n但实际建模步骤是相对复杂的，因为针对特定任务通常需要比较多个学习器，在比较的过程中需要采用交叉验证等重抽样(也称重采样)策略并开展性能评价。此外，对模型超参数进行调节或优化是一个非常重要的环节，其中涉及优化算法、重抽样策略等重要知识与经验。\n\n\n5.3.2 创建任务\n开展机器学习建模，首先要根据整理好的数据来创建任务，明确任务类型以及输入特征与输出标签。\n\n5.3.2.1 创建分类任务\nmlr3中分类任务的类名为TaskClassif，创建方法是$new()：\n\nlibrary(mlr3verse)\ntask = TaskClassif$new(id = \"iris\", \n                       backend = iris, \n                       target = \"Species\")\n\n参数id是必需的，参数backend指定数据来源，参数target指定目标变量，即标签。上述代码通过$new()方法创建了TaskClassif类的一个实例，即task对象。\nas_task_classif()函数可将符合要求的数据对象(如数据框等)转换为分类任务对象：\n\ntask1 = as_task_classif(iris, target = \"Species\")\n\nas_task_classif()函数需要先指定数据来源，参数id自动由数据来源的名称生成，不再要求显式指定。\n在控制台输入task和task1，就会显示任务的信息。\n\ntask\n\n&lt;TaskClassif:iris&gt; (150 x 5)\n* Target: Species\n* Properties: multiclass\n* Features (4):\n  - dbl (4): Petal.Length, Petal.Width, Sepal.Length, Sepal.Width\n\ntask1\n\n&lt;TaskClassif:iris&gt; (150 x 5)\n* Target: Species\n* Properties: multiclass\n* Features (4):\n  - dbl (4): Petal.Length, Petal.Width, Sepal.Length, Sepal.Width\n\n\n两个任务的信息完全一致，第一行给出任务对象的类、id和数据维度，接下来三行分别给出该任务对象的目标变量名称、任务性质和输入特征，输入特征根据变量类型分类排列。\n\n\n5.3.2.2 创建回归任务\nmlr3中回归任务的类名为TaskRegr，创建方法也是$new()：\n\ntask = TaskRegr$new(id = \"aq\",\n                    backend = airquality,\n                    target = \"Ozone\")\n\n同样，as_task_regr()函数能够将符合要求的数据对象转换为回归任务对象：\n\ntask1 = as_task_regr(airquality, target = \"Ozone\")\n\n在控制台输入task和task1，同样会显示任务的信息。\n\ntask\n\n&lt;TaskRegr:aq&gt; (153 x 6)\n* Target: Ozone\n* Properties: -\n* Features (5):\n  - int (4): Day, Month, Solar.R, Temp\n  - dbl (1): Wind\n\ntask1\n\n&lt;TaskRegr:airquality&gt; (153 x 6)\n* Target: Ozone\n* Properties: -\n* Features (5):\n  - int (4): Day, Month, Solar.R, Temp\n  - dbl (1): Wind\n\n\n除了任务的id不同以外，两个任务的其他信息保持一致。\n此外，mlr3verse还支持聚类分析(TaskClust)、密度估计(TaskDens)和生存分析(TaskSurv)，前者需要安装mlr3cluster包，后二者需要安装mlr3proba包。\n\n\n5.3.2.3 任务对象的属性和方法\n创建任务对象后，可以采用形如task$xxx(task是任务对象的名称，而$xxx为指定的属性名称)的方式查询任务对象的各种属性信息。例如查询任务数据的行数和列数：$ncol、$nrow；查询输入特征类型和名称：$feature_types、$feature_names；查询输出标签的名称：$target_names；查询任务数据中列变量的角色：$col_roles；查询任务数据的行标识：$row_ids，注意行标识与行序号的区别。\n\ntask = TaskRegr$new(id = \"aq\",\n                    backend = airquality,\n                    target = \"Ozone\")\ntask$ncol\n\n[1] 6\n\ntask$nrow\n\n[1] 153\n\ntask$feature_names\n\n[1] \"Day\"     \"Month\"   \"Solar.R\" \"Temp\"    \"Wind\"   \n\n\n同样可以采用形如task$xxx()的方式调用任务对象的各种方法，此处的$xxx()为指定的方法名称，本质上是执行特定操作的函数。例如获取创建任务的数据：$data()；获取任务数据中的实际输出标签：$truth()；复制任务对象：$clone()；获取任务数据的子集：$select()就地改变任务的输入特征，$filter()就地改变任务数据的样例(观测)；新增任务数据：$rbind()就地增加新的样例(行)，$cbind()就地增加新的输入特征。\n\ntask \n\n&lt;TaskRegr:aq&gt; (153 x 6)\n* Target: Ozone\n* Properties: -\n* Features (5):\n  - int (4): Day, Month, Solar.R, Temp\n  - dbl (1): Wind\n\ntask_copy = task$clone() \ntask_copy\n\n&lt;TaskRegr:aq&gt; (153 x 6)\n* Target: Ozone\n* Properties: -\n* Features (5):\n  - int (4): Day, Month, Solar.R, Temp\n  - dbl (1): Wind\n\n\n注意，mlr3中对象的方法会就地(in-place)改变对象，如果不采用$clone()复制任务对象，而是直接赋值，则task会随task_copy的改变而改变。\n\ntask_copy$filter(c(1, 10, 30, 60, 100))  # 这里是行标识符号，而不是行序号\ntask_copy\n\n&lt;TaskRegr:aq&gt; (5 x 6)\n* Target: Ozone\n* Properties: -\n* Features (5):\n  - int (4): Day, Month, Solar.R, Temp\n  - dbl (1): Wind\n\n# 就地改变了对象的数据\ntask_copy$data()\n\n   Ozone   Day Month Solar.R  Temp  Wind\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;   &lt;int&gt; &lt;int&gt; &lt;num&gt;\n1:    41     1     5     190    67   7.4\n2:    NA    10     5     194    69   8.6\n3:   115    30     5     223    79   5.7\n4:    NA    29     6      31    77  14.9\n5:    89     8     8     229    90  10.3\n\n# 任务数据row_id仍然保留\ntask_copy$row_ids  \n\n[1]   1  10  30  60 100\n\n\nmlr3中已经内置了一些封装好的任务，在控制台执行mlr_tasks即可查询内置任务的关键字(或名称字典)。传入关键字，tsk()和tsks()函数就可以直接加载内置任务对象，tsk()用于加载单个任务对象，例如task = tsk(\"iris\")，而tsks()则用于同时加载多个任务对象。内置任务对象的关键字可用mlr_tasks查询，了解详细信息可用as.data.table(mlr_tasks)查询。\n\n\n5.3.2.4 任务对象的可视化\nmlr3viz包提供了autoplot()泛型函数(实际调用ggplot2包中的autoplot()函数)，可根据对象类型自动调用对应可视化函数进行可视化，可用于任务对象Task、学习器对象Learner、预测对象Prediction、特征过滤器对象Filter、基准测试结果对象BenchmarkResult、重抽样结果对象ResampleResult、单一准则调优实例对象TuningInstanceSingleCrit以及单一准则优化实例对象OptimInstanceSingleCrit等。\n对于不同类型的Task对象，如TaskClassif、TaskRegr、TaskClust等，泛型函数autoplot()可以自动判断任务类型来绘制图形，并可通过设置type参数的取值来指定可视化类型。对于分类任务对象TaskClassif，参数type可选”target”(默认值)、“duo”和”pairs”。选择”target”，则绘制目标变量的条形图；选择”duo”，则将任务数据传给GGally::ggduo()绘制预测变量与目标变量的箱线图；选择”pairs”，则将任务数据传给GGally::ggpairs()绘制复杂的变量矩阵图，并将颜色标度映射到目标变量。\n\ntask_c = as_task_classif(iris, target = \"Species\")\n\n绘制目标变量条形图，结果如 图 5.7 所示：\n\nautoplot(task_c)\n\n\n\n\n\n\n\n图 5.7: 分类任务可视化-目标变量条形图\n\n\n\n\n\n绘制预测变量与目标变量箱线图，结果如 图 5.8 所示：\n\nautoplot(task_c, type = \"duo\")\n\n\n\n\n\n\n\n图 5.8: 分类任务可视化-特征变量箱线图\n\n\n\n\n\n绘制变量矩阵图，结果如 图 5.8 所示：\n\nautoplot(task_c, type = \"pairs\")\n\n\n\n\n\n\n\n图 5.9: 分类任务可视化-矩阵图\n\n\n\n\n\n对于回归任务对象TaskRegr，参数type可选”target”(默认值)和”pairs”。选择”target”，则绘制目标变量的箱线图；选择”pairs”，则将任务数据传给GGally::ggpairs()绘制变量矩阵图。\n\ntask_r = as_task_regr(airquality, target = \"Ozone\")\n\n绘制目标变量箱线图，结果如 图 5.10 所示：\n\nautoplot(task_r)\n\n\n\n\n\n\n\n图 5.10: 回归任务可视化-箱线图\n\n\n\n\n\n绘制变量矩阵图，结果如图5-11所示：\n\nautoplot(task_r, type = \"pairs\")\n\n\n\n\n\n\n\n图 5.11: 回归任务可视化-矩阵图\n\n\n\n\n\n\n\n\n5.3.3 划分数据\n创建任务后，需要将任务数据分割为训练集和测试集。简单的随机分割可采用partition()函数，该函数形式为：\n\npartition(task, ratio = 0.67, stratify = TRUE, bins = 3L)\n\n参数task是任务对象；ratio是分配给训练集的样例比例(默认是0.67，余下样例为测试集)，采用随机不回置抽样方法；stratify = TRUE表示按目标变量分层抽样，对于回归任务，则是先将目标变量分箱(binning，即分组，默认分成3组：bins = 3L)，然后按目标变量分组抽样。这里的划分是对行标识符号进行分配，而不是数据样例的分配，这可以减少对内存资源的占用。\n\nlibrary(mlr3verse)\ntask = tsk(\"iris\")\nset.seed(2023)\nsplit = partition(task)\nsplit$train\n\n  [1]   1   2   3   4   5   6   7   8   9  12  14  15  16  17  18  20  21  23\n [19]  24  25  26  29  30  32  34  38  40  41  44  45  47  48  49  50  53  54\n [37]  56  57  59  60  61  62  63  65  66  67  68  69  71  73  74  75  76  78\n [55]  80  82  83  84  85  87  90  91  92  93  95  97  98 100 102 103 105 106\n [73] 108 109 110 112 116 117 118 119 120 121 123 124 127 128 129 130 131 132\n [91] 133 135 136 138 139 140 143 144 146 147 148 149\n\nsplit$test\n\n [1]  10  11  13  19  22  27  28  31  33  35  36  37  39  42  43  46  51  52  55\n[20]  58  64  70  72  77  79  81  86  88  89  94  96  99 101 104 107 111 113 114\n[39] 115 122 125 126 134 137 141 142 145 150\n\n\n也可以手动或使用其他方法来划分训练集和测试集：\n\ntrain = c(1:34, 51:84, 101:134)\ntest = setdiff(1:task$nrow, train)\n\n划分数据必须满足训练集和测试集对数据总体具有良好的代表性，特别是一些不平衡的数据。\n在模型筛选和优化环节，为了获得对模型泛化性能的良好估计，通常需要采用合理的重抽样策略进行足够次数的数据划分，以让选定算法在不同分割的数据上训练和预测，以对模型性能进行无偏估计。\n\n\n5.3.4 创建学习器\n在mlr3中，Learner是学习器的基类，是机器学习算法及其超参数的封装。机器学习算法的实现与封装由相关的工具包完成，因此，应用算法必须安装相关工具包。在控制台输入mlr_learners可以查询mlr3支持的学习器关键字，输入as.data.table(mlr_learners)可以查询更详细的学习器信息，包括算法名称、算法能够处理的特征数据类型、算法依赖工具包、算法能够处理的特殊属性(如缺失值、变量重要性等)、算法支持的预测输出类型等。\n创建学习器对象，最简单的方式是将学习器关键字传入lrn()和lrns()，前者一次只能创建单个学习器，后者一次可创建多个学习器：\n\nlr_rpart = lrn(\"classif.rpart\")\nlr_rpart  # 查看学习器的元数据(metadata)\n\n&lt;LearnerClassifRpart:classif.rpart&gt;: Classification Tree\n* Model: -\n* Parameters: xval=0\n* Packages: mlr3, rpart\n* Predict Types:  [response], prob\n* Feature Types: logical, integer, numeric, factor, ordered\n* Properties: importance, missings, multiclass, selected_features,\n  twoclass, weights\n\n\n通过$feature_types可查询学习器能够处理的特征数据类型；$packages可查询学习器依赖的工具包；$properties可查看学习器能够处理的特殊属性，如“missings”意味着学习器能够处理缺失值，“importance”意味着学习器能够计算每个特征的相对重要性；$predict_types可查询学习器创建的模型能够给出的预测类型；$param_set可查看学习器的超参数集合。\n\nlr_rpart$param_set\n\n&lt;ParamSet&gt;\n                id    class lower upper nlevels\n            &lt;char&gt;   &lt;char&gt; &lt;num&gt; &lt;num&gt;   &lt;num&gt;\n 1:             cp ParamDbl     0     1     Inf\n 2:     keep_model ParamLgl    NA    NA       2\n 3:     maxcompete ParamInt     0   Inf     Inf\n 4:       maxdepth ParamInt     1    30      30\n 5:   maxsurrogate ParamInt     0   Inf     Inf\n 6:      minbucket ParamInt     1   Inf     Inf\n 7:       minsplit ParamInt     1   Inf     Inf\n 8: surrogatestyle ParamInt     0     1       2\n 9:   usesurrogate ParamInt     0     2       3\n10:           xval ParamInt     0   Inf     Inf\n                                                                                      default\n                                                                                       &lt;list&gt;\n 1:                                                                                      0.01\n 2:                                                                                     FALSE\n 3:                                                                                         4\n 4:                                                                                        30\n 5:                                                                                         5\n 6: &lt;NoDefault&gt;\\n  Public:\\n    clone: function (deep = FALSE) \\n    initialize: function () \n 7:                                                                                        20\n 8:                                                                                         0\n 9:                                                                                         2\n10:                                                                                        10\n     value\n    &lt;list&gt;\n 1:       \n 2:       \n 3:       \n 4:       \n 5:       \n 6:       \n 7:       \n 8:       \n 9:       \n10:      0\n\n\n该分类决策树的超参数有10个，id给出超参数的名称，class给出各个超参数的数据类型(包括浮点型ParamDbl、整数型ParamInt、逻辑型ParamLgl、因子型ParamFct以及无类型ParamUty)，lower和upper分别给出各超参数的下限值和上限值，default给出超参数的默认值，value给出各参数的设定值。\n学习器的超参数可以在学习器创建时来设置，也可在学习器创建后来设置。\n\nlr_rpart = lrn(\"classif.rpart\", \n               cp = 0.02, \n               maxdepth = 3)\nlr_rpart$param_set$values\n\n$xval\n[1] 0\n\n$cp\n[1] 0.02\n\n$maxdepth\n[1] 3\n\n\n\nlr_rpart = lrn(\"classif.rpart\")\nlr_rpart$param_set$values$cp = 0.03\nlr_rpart$param_set$values$maxdepth = 4\nlr_rpart$param_set$values\n\n$xval\n[1] 0\n\n$cp\n[1] 0.03\n\n$maxdepth\n[1] 4\n\n\n学习器的某些超参数的设置依赖于其他超参数，查看超参数的依赖项用$deps:\n\nlr_svm = lrn(\"classif.svm\")\nlr_svm$param_set$deps\n\n       id     on              cond\n   &lt;char&gt; &lt;char&gt;            &lt;list&gt;\n1:  coef0 kernel &lt;CondAnyOf:anyof&gt;\n2:   cost   type &lt;CondEqual:equal&gt;\n3: degree kernel &lt;CondEqual:equal&gt;\n4:  gamma kernel &lt;CondAnyOf:anyof&gt;\n5:     nu   type &lt;CondEqual:equal&gt;\n\n\n查询结果表明建立的支持向量机分类学习器有5个超参数存在依赖项，通过以下形式的代码可以继续查询超参数具体的依赖项：\n\nlr_svm$param_set$deps[[3, \"cond\"]]\n\nCondEqual: x = polynomial\n\nlr_svm$param_set$deps[[5, \"cond\"]]\n\nCondEqual: x = nu-classification\n\n\n以上查询结果表明第三个超参数degree只有在超参数kernel设置为polynomial时可用，第五超参数gamma在超参数kernel设置为polynomial、radial或sigmoid时均可用。\n\n\n5.3.5 训练模型\n完成任务和学习器的创建以及任务数据划分后，就可以将学习器置于训练集上进行训练，这是学习器在训练集上学习或拟合的过程，目的是提取训练集数据中的经验规则，得到机器学习模型。\nmlr3通过调用学习器对象的$train()方法来训练学习器已获得模型：\n\nlr_rpart$train(task, row_ids = split$train) \n\n通过$model可以查看结果模型：\n\nlr_rpart$model\n\nn= 102 \n\nnode), split, n, loss, yval, (yprob)\n      * denotes terminal node\n\n1) root 102 68 setosa (0.33333333 0.33333333 0.33333333)  \n  2) Petal.Length&lt; 2.7 34  0 setosa (1.00000000 0.00000000 0.00000000) *\n  3) Petal.Length&gt;=2.7 68 34 versicolor (0.00000000 0.50000000 0.50000000)  \n    6) Petal.Width&lt; 1.75 36  3 versicolor (0.00000000 0.91666667 0.08333333) *\n    7) Petal.Width&gt;=1.75 32  1 virginica (0.00000000 0.03125000 0.96875000) *\n\n\n上面的示例中，classif.raprt学习器从102个训练样例中学习，得到一个深度为2(超参数maxdepth的设置值是3)的决策树模型，第一层分裂建树利用的特征是Petal.Length，第二层分裂建树利用的特征是Petal.Width。训练集102个样例中，34个setosa样例全部正确分类，versicolor和virginica样例中均有少数被错误分类。\nrpart.plot包中的rpart.plot()函数可以对分类树模型进行可视化(如 图 5.12 所示)：\n\nrpart.plot::rpart.plot(lr_rpart$model)\n\n\n\n\n\n\n\n\n\n图 5.12: 分类决策树模型可视化\n\n\n\n\n\n基于决策树的模型，可以计算特征重要性，在模型训练完成后，可通过$importance()方法提取特征重要性计算结果：\n\nlr_rpart$importance()\n\n\n## Petal.Width Petal.Length Sepal.Length  Sepal.Width \n##    60.56250     54.75195     40.45117     27.64063\n\n计算结果表明，Petal.Width特征分值最高，其次是Petal.Length特征。由 图 5.12 可知，该模型第一层分裂建树特征采用的是Petal.Length，第二层分类建树特征采用的是Petal.Width。\n\n\n5.3.6 模型预测与预测性能\n模型建立后即可在测试集上进行预测并计算预测性能。调用$predict()方法进行预测：\n\npred = lr_rpart$predict(task, row_ids = split$test)\n\n$predict()方法返回的是Prediction对象，也拥有相应的方法和属性。通过$response和$truth可查询模型对测试集样例的预测标签及其真实标签：\n\npred$response\n\n\n##  [1] setosa     setosa     setosa     setosa     setosa     setosa     setosa     setosa     setosa    \n## [10] setosa     setosa     setosa     setosa     setosa     setosa     setosa     versicolor versicolor\n## [19] versicolor versicolor versicolor versicolor versicolor versicolor versicolor versicolor versicolor\n## [28] versicolor versicolor versicolor versicolor versicolor virginica  virginica  versicolor virginica \n## [37] virginica  virginica  virginica  virginica  virginica  virginica  versicolor virginica  virginica \n## [46] virginica  virginica  virginica \n## Levels: setosa versicolor virginica\n\n\npred$truth\n\n\n##  [1] setosa     setosa     setosa     setosa     setosa     setosa     setosa     setosa     setosa    \n## [10] setosa     setosa     setosa     setosa     setosa     setosa     setosa     versicolor versicolor\n## [19] versicolor versicolor versicolor versicolor versicolor versicolor versicolor versicolor versicolor\n## [28] versicolor versicolor versicolor versicolor versicolor virginica  virginica  virginica  virginica \n## [37] virginica  virginica  virginica  virginica  virginica  virginica  virginica  virginica  virginica \n## [46] virginica  virginica  virginica \n## Levels: setosa versicolor virginica\n\n利用$confusion可查看混淆矩阵：\n\npred$confusion\n\n\n##             truth\n## response     setosa versicolor virginica\n##   setosa         16          0         0\n##   versicolor      0         16         2\n##   virginica       0          0        14\n\n每列之和为测试集中每个类别的实际样例数，每行之和为模型预测的每个类别中的样例数。显然，setosa和versicolor两个类别中的预测样例都全部正确分类，但virginica中有2个样例错分到versicolor。\n混淆矩阵用于总结分类模型的预测结果，给出简明的频数分析表，二分类的混淆矩阵形式如 表 5.1 所示，多分类的混淆矩阵可据此扩展。\n\n\n\n\n\n表 5.1: 二分类混淆矩阵\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n显然，TP和TN越大、FP和FN越小，分类结果越好。基于混淆矩阵可以计算以下分类模型常用的性能评价指标：\n\n准确率：表示全部样例中被模型准确预测为正例和负例的比例。\n\n\\text{Accuracy(准确率)} = \\frac{TP+TN}{TP+TN+FP+FN}\n\\tag{5.1}\n\n精确率：表示被模型预测为正例的样例中实际为正例的比例，也称为查准率。\n\n\\text{Precision(精确率)} = \\frac{TP}{TP+FP}\n\\tag{5.2}\n\n灵敏度：表示样本全部正例中被模型准确预测为正例的比例，也称为真正率(TPR)、召回率(Recall)。\n\n\\text{Sensitivity(灵敏度)}=\\text{TPR}=\\text{Recall(召回率)}=\\frac{TP}{TP+FN}\n\\tag{5.3}\n\n特异度：表示样本全部负例中被模型准确预测为负例的比例，也称为真负率(TNR)。\n\n\\text{Specificity(特异度)} = \\text{TNR}=\\frac{TP}{TP+FN}\n\\tag{5.4}\n\nF1-Score：是精确率和召回率(灵敏度、真正率)的调和平均，取值范围为0~1，越接近1表示模型预测性能越好。\n\n\\text{F1-Score} =\\frac{2}{1/\\text{Precision}+1/\\text{Recall}}=\\frac{2TP}{2TP+FN+FP}\n\\tag{5.5}\n\n准确率是评价分类模型性能的常用指标，但在不平衡数据的分类任务上并非最佳指标。例如，在100条污染物排放数据中污染超标样例只有5个，分类模型对全部未超标样例和3个超标样例做出了准确预测，那么该分类模型的整体预测准确率高达98%，可是更受关注的5个污染超标样例只有60%的预测准确率。而精确率和灵敏度(召回率)都是专门评价正例分类准确率的指标，特异度则是用于评价分类模型对负例分类准确率的指标。F1-Score既可用于平衡数据集，亦可用于不平衡数据集。\nmlr3将评价模型性能的各种指标封装为相应的Measures对象，即测度对象，在控制台输入mlr_measures可查看所有测度的关键字，输入as.data.table(mlr_measures)可查询所有测度的详细信息。不同类型的任务和不同类型的预测输出，需要采用不同的测度来计算，其中评价分类模型性能的测度如 表 5.2 所示，评价回归模型性能的测度如 表 5.3 所示。\n\n\n\n\n表 5.2: 分类任务测度\n\n\n\n\n\n\n\n\n\n\n\nkey\nlabel\npredict_type\n\n\n\n\nclassif.acc\nClassification Accuracy\nresponse\n\n\nclassif.auc\nArea Under the ROC Curve\nprob\n\n\nclassif.bacc\nBalanced Accuracy\nresponse\n\n\nclassif.bbrier\nBinary Brier Score\nprob\n\n\nclassif.ce\nClassification Error\nresponse\n\n\nclassif.costs\nCost-sensitive Classification\nresponse\n\n\nclassif.dor\nDiagnostic Odds Ratio\nresponse\n\n\nclassif.fbeta\nF-beta score\nresponse\n\n\nclassif.fdr\nFalse Discovery Rate\nresponse\n\n\nclassif.fn\nFalse Negatives\nresponse\n\n\nclassif.fnr\nFalse Negative Rate\nresponse\n\n\nclassif.fomr\nFalse Omission Rate\nresponse\n\n\nclassif.fp\nFalse Positives\nresponse\n\n\nclassif.fpr\nFalse Positive Rate\nresponse\n\n\nclassif.logloss\nLog Loss\nprob\n\n\nclassif.mauc_au1p\nWeighted average 1 vs. 1 multiclass AUC\nprob\n\n\nclassif.mauc_au1u\nAverage 1 vs. 1 multiclass AUC\nprob\n\n\nclassif.mauc_aunp\nWeighted average 1 vs. rest multiclass AUC\nprob\n\n\nclassif.mauc_aunu\nAverage 1 vs. rest multiclass AUC\nprob\n\n\nclassif.mbrier\nMulticlass Brier Score\nprob\n\n\nclassif.mcc\nMatthews Correlation Coefficient\nresponse\n\n\nclassif.npv\nNegative Predictive Value\nresponse\n\n\nclassif.ppv\nPositive Predictive Value\nresponse\n\n\nclassif.prauc\nPrecision-Recall Curve\nprob\n\n\nclassif.precision\nPrecision\nresponse\n\n\nclassif.recall\nRecall\nresponse\n\n\nclassif.sensitivity\nSensitivity\nresponse\n\n\nclassif.specificity\nSpecificity\nresponse\n\n\nclassif.tn\nTrue Negatives\nresponse\n\n\nclassif.tnr\nTrue Negative Rate\nresponse\n\n\nclassif.tp\nTrue Positives\nresponse\n\n\nclassif.tpr\nTrue Positive Rate\nresponse\n\n\n\n\n\n\n\n\n\n\n\n\n表 5.3: 回归任务测度\n\n\n\n\n\n\nkey\nlabel\npredict_type\n\n\n\n\nregr.bias\nBias\nresponse\n\n\nregr.ktau\nKendall’s tau\nresponse\n\n\nregr.mae\nMean Absolute Error\nresponse\n\n\nregr.mape\nMean Absolute Percent Error\nresponse\n\n\nregr.maxae\nMax Absolute Error\nresponse\n\n\nregr.medae\nMedian Absolute Error\nresponse\n\n\nregr.medse\nMedian Squared Error\nresponse\n\n\nregr.mse\nMean Squared Error\nresponse\n\n\nregr.msle\nMean Squared Log Error\nresponse\n\n\nregr.pbias\nPercent Bias\nresponse\n\n\nregr.rae\nRelative Absolute Error\nresponse\n\n\nregr.rmse\nRoot Mean Squared Error\nresponse\n\n\nregr.rmsle\nRoot Mean Squared Log Error\nresponse\n\n\nregr.rrse\nRoot Relative Squared Error\nresponse\n\n\nregr.rse\nRelative Squared Error\nresponse\n\n\nregr.rsq\nR Squared\nresponse\n\n\nregr.sae\nSum of Absolute Errors\nresponse\n\n\nregr.smape\nSymmetric Mean Absolute Percent Error\nresponse\n\n\nregr.srho\nSpearman’s rho\nresponse\n\n\nregr.sse\nSum of Squared Errors\nresponse\n\n\n\n\n\n\n\n\n调用Prediction对象的$score()方法即可根据默认的测度计算测度值：\n\npred$score()\n\n\n## classif.ce \n## 0.04166667 \n\n结果显示，默认测度是classif.ce(分类错误率)，其值为0.04166667，亦即约4.2%的样例被错误分类。显然，分类准确度(classif.acc)是100% - 分类错误率。\n可以通过msr()或msrs()函数传入一个或多个测度来计算指定的性能测度值：\n\npred$score(msr(\"classif.acc\"))\n\n\n## classif.acc \n##   0.9583333 \n\n\npred$score(msrs(c(\"classif.acc\",\"classif.ce\")))\n\n\n## classif.acc  classif.ce \n##  0.95833333  0.04166667 \n\n如果需要计算模型在训练集上的拟合误差，可调用$predict()方法对训练集进行预测，然后对返回的预测对象调用$score()方法计算性能测度值：\n\nfitted = lr_rpart$predict(task, row_ids = split$train)\nfitted$score()\n\n\n## classif.ce \n## 0.03921569 \n\n\nms = msrs(c(\"classif.acc\",\"classif.ce\"))\nfitted$score(ms)\n\n\n## classif.acc  classif.ce \n##  0.96078431  0.03921569 \n\n对于多分类任务，可以通过混淆矩阵信息来计算每个分类的真正率、真负率等指标。\n除此以外，还可以绘制ROC曲线和PR曲线对分类模型性能进行可视化比较。ROC曲线称为受试者操作特征曲线(Receiver Operating Characteristic Curve)，最早用于评价雷达信号的可靠性，后广泛用于医学和机器学习领域。该曲线的横坐标为1 - 特异度(FPR, 假正率)，纵坐标为灵敏度(TPR, 真正率)。显然，好的分类模型应该是TPR越大越好，同时FPR越小越好)。PR曲线(Precision-Recall Curve)的横坐标为灵敏度，纵坐标为精确度，这两个指标计算公式中的分子都为TP，但灵敏度公式中分母为TP＋FN，是样本中所有实际分类标签为正例的样例总数，而精确度公式中分母为TP＋FP，是样本中被模型预测为正例的样例总数。显然，PR曲线对不平衡数据集更敏感，而ROC曲线不敏感。简言之，就是PR曲线在数据集中正负两类样例相差悬殊时，能够比ROC曲线更好地甄别分类模型的优劣。\n\n例 5.1 ROC和PR曲线应用案例。\n\nmlr3内置的声呐分类任务(sonar)有60个输入特征(V1~V60)，输出标签为Class(二分类，R为岩石，M为金属矿石)，共208个样例。比较四个分类模型(超参数均为默认配置)的性能：无特征classif.featureless(基线模型，随机猜测分类)、支持向量机classif.svm、决策树classif.rpart和K最近邻classif.kknn。\n要保证比较的公平性，必须确保所有模型在相同的训练集上训练和在相同的测试集上预测，为此需要采用benchmark_grid()函数设计基准测试方案：封装任务、学习器和重抽样策略，并用benchmark()函数执行基准测试方案，返回的结果是BenchmarkResult对象。\n\nlibrary(\"mlr3verse\")\n\ntask = tsk(\"sonar\", positive = \"M\") \n\nset.seed(2023)\nsplit = partition(task)\n\nlrs = lrns(c(\"classif.featureless\",\"classif.svm\", \n             \"classif.rpart\", \"classif.kknn\"),\n           predict_type = \"prob\", predict_sets = c(\"train\",\"test\"))\nrss = rsmp(\"cv\", folds = 5)\n\ndesign = benchmark_grid(\n  task = task,\n  learners = lrs,\n  resamplings = rss\n) \n\n\nlgr::get_logger(\"mlr3\")$set_threshold(\"warn\")\nlgr::get_logger(\"bbotk\")$set_threshold(\"warn\")\n\nbmr = benchmark(design)\n\n四个模型的预测输出类型(predict_type)均设置为概率(prob)，并在训练集和测试集上都执行预测，重抽样策略为”cv”(5折交叉验证)，然后开始进行基准测试。调用BenchmarkResult对象的$score()方法可返回四个模型所有迭代的分类错误率，调用$aggregate()方法即返回四个模型的平均分类错误率：\n\nbmr$score()\n\n       nr task_id          learner_id resampling_id iteration classif.ce\n    &lt;int&gt;  &lt;char&gt;              &lt;char&gt;        &lt;char&gt;     &lt;int&gt;      &lt;num&gt;\n 1:     1   sonar classif.featureless            cv         1 0.40476190\n 2:     1   sonar classif.featureless            cv         2 0.52380952\n 3:     1   sonar classif.featureless            cv         3 0.42857143\n 4:     1   sonar classif.featureless            cv         4 0.58536585\n 5:     1   sonar classif.featureless            cv         5 0.39024390\n 6:     2   sonar         classif.svm            cv         1 0.26190476\n 7:     2   sonar         classif.svm            cv         2 0.14285714\n 8:     2   sonar         classif.svm            cv         3 0.11904762\n 9:     2   sonar         classif.svm            cv         4 0.14634146\n10:     2   sonar         classif.svm            cv         5 0.17073171\n11:     3   sonar       classif.rpart            cv         1 0.35714286\n12:     3   sonar       classif.rpart            cv         2 0.23809524\n13:     3   sonar       classif.rpart            cv         3 0.42857143\n14:     3   sonar       classif.rpart            cv         4 0.31707317\n15:     3   sonar       classif.rpart            cv         5 0.21951220\n16:     4   sonar        classif.kknn            cv         1 0.14285714\n17:     4   sonar        classif.kknn            cv         2 0.19047619\n18:     4   sonar        classif.kknn            cv         3 0.11904762\n19:     4   sonar        classif.kknn            cv         4 0.19512195\n20:     4   sonar        classif.kknn            cv         5 0.04878049\n       nr task_id          learner_id resampling_id iteration classif.ce\nHidden columns: uhash, task, learner, resampling, prediction\n\n\n显然，在不同分割的交叉验证集上，同一模型的分类错误率都存在差异。\n\nbmr$aggregate()\n\n      nr task_id          learner_id resampling_id iters classif.ce\n   &lt;int&gt;  &lt;char&gt;              &lt;char&gt;        &lt;char&gt; &lt;int&gt;      &lt;num&gt;\n1:     1   sonar classif.featureless            cv     5  0.4665505\n2:     2   sonar         classif.svm            cv     5  0.1681765\n3:     3   sonar       classif.rpart            cv     5  0.3120790\n4:     4   sonar        classif.kknn            cv     5  0.1392567\nHidden columns: resample_result\n\n\n从平均分类错误率来看，kknn模型最好，svm模型次之，rpart模型第三，featureless模型最差。\n再来比较四个模型对正类M和负类R的分类准确率以及综合分类准确率，分别将classif.tpr、classif.tnr和classif.fbeta测度传入$aggregate()：\n\nbmr$aggregate(msr(\"classif.tpr\"))\n\n      nr task_id          learner_id resampling_id iters classif.tpr\n   &lt;int&gt;  &lt;char&gt;              &lt;char&gt;        &lt;char&gt; &lt;int&gt;       &lt;num&gt;\n1:     1   sonar classif.featureless            cv     5   1.0000000\n2:     2   sonar         classif.svm            cv     5   0.8626078\n3:     3   sonar       classif.rpart            cv     5   0.7536078\n4:     4   sonar        classif.kknn            cv     5   0.9341373\nHidden columns: resample_result\n\nbmr$aggregate(msr(\"classif.tnr\"))\n\n      nr task_id          learner_id resampling_id iters classif.tnr\n   &lt;int&gt;  &lt;char&gt;              &lt;char&gt;        &lt;char&gt; &lt;int&gt;       &lt;num&gt;\n1:     1   sonar classif.featureless            cv     5   0.0000000\n2:     2   sonar         classif.svm            cv     5   0.7853461\n3:     3   sonar       classif.rpart            cv     5   0.6177956\n4:     4   sonar        classif.kknn            cv     5   0.7823604\nHidden columns: resample_result\n\nbmr$aggregate(msr(\"classif.fbeta\", beta = 1))  # beta默认值为1\n\n      nr task_id          learner_id resampling_id iters classif.fbeta\n   &lt;int&gt;  &lt;char&gt;              &lt;char&gt;        &lt;char&gt; &lt;int&gt;         &lt;num&gt;\n1:     1   sonar classif.featureless            cv     5     0.6924971\n2:     2   sonar         classif.svm            cv     5     0.8439107\n3:     3   sonar       classif.rpart            cv     5     0.7138022\n4:     4   sonar        classif.kknn            cv     5     0.8724274\nHidden columns: resample_result\n\n\n比较这三个测度的结果可以发现，featureless模型的TPR为1，排名第一，但TNR为0，排名第四；kknn模型的TPR排名第二，TNR排名第二；svm模型的TPR排名第三，TNR排名第一；rpart模型TPR排名第四，TNR排名第三；F1分值，kknn模型排名第一，svm模型排名第二，rpart模型排名第三，featureless模型排名第四。综合考虑，可以选择kknn模型和svm模型继续开展超参数优化后再比较泛化性能。\n下面再来计算四个模型在测试集和预测集上的ROC曲线下面积(classif.auc测度)：\n\nms = list(\n  msr(\"classif.auc\", predict_sets = \"train\", id = \"auc_train\"),\n  msr(\"classif.auc\", id = \"auc_test\")\n)\ntab = bmr$aggregate(ms)\nprint(tab[,c(1:4,7:8)])\n\n      nr task_id          learner_id auc_train  auc_test\n   &lt;int&gt;  &lt;char&gt;              &lt;char&gt;     &lt;num&gt;     &lt;num&gt;\n1:     1   sonar classif.featureless 0.5000000 0.5000000\n2:     2   sonar         classif.svm 0.9994480 0.9349302\n3:     3   sonar       classif.rpart 0.9110997 0.7284968\n4:     4   sonar        classif.kknn 0.9987746 0.9384684\nHidden columns: resample_result\n\n\n从计算结果来看，训练集上，svm模型AUC最大，其次是kknn模型，rpart模型排名第三；而在测试集上，kknn模型最大，svm模型其次，rpart模型仍然第三。下面用autoplot()函数来绘制ROC曲线，结果如 图 5.13 所示：\n\nbmr_plot = bmr$clone()\nautoplot(bmr_plot, type = \"roc\")\n\n\n\n\n\n\n\n图 5.13: 不同学习器的ROC曲线\n\n\n\n\n\nROC曲线越靠近左上角表示泛化性能越好。显然，featureless模型最差，svm模型和kknn模型很接近，都比较好，均显著优于决策树模型rpart。\n继续用autoplot()绘制PR曲线，结果如 图 5.14 所示：\n\nautoplot(bmr_plot, type = \"prc\")\n\n\n\n\n\n\n\n图 5.14: 不同学习器的PR曲线\n\n\n\n\n\nPR曲线越靠近右上角越好，四个模型的性能比较结果与ROC曲线一致。\nautoplot()还可以绘制基准测试结果性能测度的箱线图(type = “boxplot”，默认值)，结果如 图 5.14 所示：\n\nlibrary(ggplot2)\nautoplot(bmr_plot) + \n  theme(axis.text.x = element_text(angle = 0, hjust = 0.5))\n\n\n\n\n\n\n\n图 5.15: 不同学习器的性能测度箱线图\n\n\n\n\n\n从 图 5.15 可见，svm模型和kknn模型的分类错误率中位数很接近，都比较小，但前者相对更稳定，而后者在分类错误率降低方面有较大的潜力。\n\n练习 5.1 \n(1)安装mlr3verse套件。\n(2)基于mlr3内置任务iris，参照 例 5.1 的全部内容进行练习。",
    "crumbs": [
      "进阶知识",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>R语言机器学习建模</span>"
    ]
  },
  {
    "objectID": "CH5.html#超参数优化",
    "href": "CH5.html#超参数优化",
    "title": "5  R语言机器学习建模",
    "section": "5.4 超参数优化",
    "text": "5.4 超参数优化\n机器学习算法的超参数对算法的学习过程以及最终模型的泛化性能具有重要影响。机器学习建模过程中，超参数优化(HPO，亦称模型调优)是一个极其重要但开销(时间和算力成本)很大的步骤，其本质是黑盒优化问题，其根本目的是提高模型的泛化性能。mlr3提供了寻找最佳超参数配置的功能，这些功能依赖于bbotk、paradox、mlr3tuning、mlr3hyperban、mlr3mbo等工具包。mlr3tuningspace工具包为许多流行的机器学习算法提供了随时可用的搜索空间配置。\n对于给定的有监督学习任务，无论是回归还是分类，都需要基于具体且有限的学习数据，在选定的机器学习算法的超参数空间中找到使最终模型泛化性能最佳的参数配置。寻找最佳超参数配置需要调用寻优算法，在控制台输入mlr_tuners可查询mlr3tuning包内置的寻优算法(调优器)的关键字，输入as.data.table(mlr_tuners)可查看寻优算法更详细的信息，包括寻优算法支持寻优参数的类型、支持寻优规则的类型(单规则、多规则以及依赖规则)以及必需安装的工具包等。\nmlr3tuning目前支持”cmaes”(协方差矩阵自适应进化策略)、“gensa”(模拟退火算法)、“grid_search”(网格搜索法)、“hyperband”(超频带法)、“irace”(迭代竞赛法)、“mbo”(基于模型的贝叶斯优化法)、“nloptr”(非线性优化法)、“random_search”(随机搜索法)、“successive_halving”(连续减半法)等多种寻优算法。其中”grid_search”是一种遍历式的搜索方法，可以找到全局最佳超参数配置，但计算开销极大；“random_search”容易陷入局部最优；“hyperband”和”successive_halving”可以显著减少超参数优化的时间和算力成本；“mbo”是一种基于贝叶斯优化原理的全局黑箱优化算法，通常能够发现良好泛化性能的超参数配置。\nHPO的一般步骤：\n(1)创建学习任务、学习器、设置超参数搜索空间、创建重抽样策略(rsmp()或rsmps()函数)、指定性能测度和寻优终止条件(trm()或trms()函数)；\n(2)利用ti()函数封装以上对象形成调优实例；\n(3)利用tnr()或tnrs()函数创建调优器对象(Tuner)；\n(4)将调优实例传递给调优器对象的$tune()方法进行寻优；\n(5)将调优实例中保存的最佳超参数配置传给学习器，学习器根据最优超参数配置进行训练和预测。\n其中设置超参数搜索空间，可以在学习器创建时利用to_tune()函数进行设置，也可在学习器创建后利用paradox工具包的ps()函数创建更高级更复杂的超参数搜索空间，或者利用lts()、ltss()函数直接加载mlr3tuningspace包中预置的超参数配置搜索空间。\n\n5.4.1 创建调优实例\n\n例 5.2 基于 例 5.1 基准测试的结果，选择支持向量机算法对其正则化参数cost和核函数(非线性核函数)参数gamma进行调优。\n\n(1)加载相关工具包。\n\nlibrary(mlr3verse)\nlibrary(ggplot2)\n\n(2)创建任务。\n\ntask = tsk(\"sonar\", positive = \"M\")\n\n(3)创建学习器，并用to_tune()函数设置超参数配置搜索空间。\n\nlr = lrn(\"classif.svm\",\n         cost  = to_tune(1e-5, 1e5, logscale = TRUE),\n         gamma = to_tune(1e-5, 1e5, logscale = TRUE),\n         kernel = \"radial\",\n         type = \"C-classification\")\n\n(4)创建重抽样策略。\n\nrs = rsmp(\"cv\", folds = 5)\n\n重抽样策略对于估计模型的泛化性能或误差非常重要。mlr3提供的重抽样策略可以通过mlr_resamplings查询其关键字，在控制台执行以下代码as.data.table(mlr_resamplings)可查询抽抽样策略的详细信息：\n\nas.data.table(mlr_resamplings)\n\nKey: &lt;key&gt;\n           key                         label        params iters\n        &lt;char&gt;                        &lt;char&gt;        &lt;list&gt; &lt;int&gt;\n1:   bootstrap                     Bootstrap ratio,repeats    30\n2:      custom                 Custom Splits                  NA\n3:   custom_cv Custom Split Cross-Validation                  NA\n4:          cv              Cross-Validation         folds    10\n5:     holdout                       Holdout         ratio     1\n6:    insample           Insample Resampling                   1\n7:         loo                 Leave-One-Out                  NA\n8: repeated_cv     Repeated Cross-Validation folds,repeats   100\n9: subsampling                   Subsampling ratio,repeats    30\n\n\nkey字段给出重抽样策略的关键字，label字段给出关键字对应的重抽样策略，params字段给出该重抽样策略的参数，iters字段为默认迭代次数。最常用的重抽样策略有交叉验证”cv”、重复交叉验证”repeated_cv”、自举抽样”bootstrap”等，对于特殊的任务，可以选用”custom”来自定义重抽样策略。\n如果随机划分的训练集和测试集对整体数据集有很好的代表性，那么可以考虑采用”holdout”策略，因为该策略计算成本最低；“subsampling”策略是”holdout”策略的多次重复，能够减小泛化性能或误差的估计偏差。“cv”策略尤其是”loo”策略是被实践证明估计偏差最小的策略，针对不平衡数据集可以采用分层交叉验证策略，“repeated_cv”策略可以克服分割子集对整体数据集代表性差的问题。“bootstrap”策略利用回置抽样方法分割子集并重复多次，“custom_cv”策略是根据指定的因子变量分割交叉验证子集，“insample”策略是将整体数据集同时作为训练集和测试集。mlr3spatiotempcv扩展包为时空数据提供了特殊的交叉验证策略。\n在创建重抽样策略后，可在控制台查询该策略的信息：\n\nrs\n\n&lt;ResamplingCV&gt;: Cross-Validation\n* Iterations: 5\n* Instantiated: FALSE\n* Parameters: folds=5\n\n\n第一行说明重抽样策略类型，第二行给出该策略的迭代次数，迭代次数越多，时间和算力成本越高；第三行表明该策略尚未实例化(即该策略尚未应用于具体学习任务数据)；第四行给出该策略的参数设置。\n(5)指定性能测度。\n\nms = msr(\"classif.ce\")\n\n同样可以在控制台查看性能测度对象的信息：\n\nms\n\n&lt;MeasureClassifSimple:classif.ce&gt;: Classification Error\n* Packages: mlr3, mlr3measures\n* Range: [0, 1]\n* Minimize: TRUE\n* Average: macro\n* Parameters: list()\n* Properties: -\n* Predict type: response\n\n\n第一行说明该测度的类型；第二行给出该测度的依赖工具包；第三行是该测度的取值范围是[0, 1]；第四行表明该测度是越小越好；第五行表明该测度的平均方式，macro表示宏平均，亦即计算分组平均值的平均值，而micro表示微平均，亦即对所有分组中个体的标志值求总体平均值；第六行给出该测度的参数列表；第七行给出该测度的属性；第八行给出该测度的支持的预测输出类型。不同的测度对象给出的信息有所差异，如果是msrs()函数创建的包含多个测度的对象，则按测度关键字给出各个测度的信息。\n(6)指定寻优终止条件。\n\ntm = trm(\"none\")\n\nmlr3提供的终止条件(终止器对象)可以通过mlr_terminators查询关键字，在控制台执行以下代码来查询终止条件的具体信息：\n\nas.data.table(mlr_terminators)\n\nKey: &lt;key&gt;\n                key                label             properties        unit\n             &lt;char&gt;               &lt;char&gt;                 &lt;list&gt;      &lt;char&gt;\n1:       clock_time           Clock Time single-crit,multi-crit     seconds\n2:            combo          Combination single-crit,multi-crit     percent\n3:            evals Number of Evaluation single-crit,multi-crit evaluations\n4:             none                 None single-crit,multi-crit     percent\n5:     perf_reached    Performance Level            single-crit     percent\n6:         run_time             Run Time single-crit,multi-crit     seconds\n7:       stagnation           Stagnation            single-crit     percent\n8: stagnation_batch     Stagnation Batch            single-crit     percent\n\n\nkey字段给出终止条件的关键字，label字段给出对应的终止条件名称，properties字段给出对应的应用范围(单准则和多准则)，unit字段给出终止条件的单位/量纲。“evals”表示评估次数超过设定值时终止，“stagnation”和”stagnation_batch”表示性能在过去n次迭代(前者)或过去n批次中不再改善时终止，“perf_reached”表示性能测度达到目标值时终止，“clcok_time”表示在固定时间点终止，“run_time”表示在运行一段时间后终止，“none”表示没有终止条件(主要用于遍历式的寻优算法)。如果需要采用多个终止条件，则需要采用”combo”来建立组合终止条件。有些超参数寻优算法必须指定终止条件，但有些寻优算法如”gird_search”是在有限的超参数配置空间中进行遍历，无需设置终止条件(故对应终止条件采用”none”)。同样可以在控制台输入终止器对象的名称来查看其信息。\n(7)创建调优实例。\nti()函数用于封装任务、学习器、重抽样策略、测度、终止器以及超参数搜索空间，以创建调优实例对象：\n\nins = ti(task = task,\n         learner = lr,\n         resampling = rs,\n         measures = ms,\n         terminator = tm)\n\n在控制台查看调优实例对象的基本信息：\n\nins\n\n&lt;TuningInstanceSingleCrit&gt;\n* State:  Not optimized\n* Objective: &lt;ObjectiveTuning:classif.svm_on_sonar&gt;\n* Search Space:\n       id    class     lower    upper nlevels\n   &lt;char&gt;   &lt;char&gt;     &lt;num&gt;    &lt;num&gt;   &lt;num&gt;\n1:   cost ParamDbl -11.51293 11.51293     Inf\n2:  gamma ParamDbl -11.51293 11.51293     Inf\n* Terminator: &lt;TerminatorNone&gt;\n\n\n第一行描述表明该调优实例是一个单准则调优实例；第二行表明该实例处于未优化状态；第三条说明调优目标是在sonar任务上调优classif.svm模型；第四行到第七行给出超参数搜索空间；最后一行给出设定的寻优终止条件。\n\n\n5.4.2 创建调优器并运行调优\n(1)创建调优器。\n通过mlr3tuning包中的tnr()或tnrs()指定寻优算法并配置相应参数，以创建调优器对象。\n\ntu = tnr(\"grid_search\", resolution = 11, batch_size = 10)\n\n上述代码指定优化算法为网格搜索法，resolution参数指定超参数均匀分割点数，所有超参数的分割点组合成超参数搜索空间。本例中优化2个超参数，因此将对11^2=121个超参数配置进行性能评估。batch_size参数指定一次输入模型的样例数(默认值为1)，模型按批学习和计算损失，因此，该参数越大，运算量越少。\n在控制台查看调优器的信息：\n\ntu\n\n&lt;TunerGridSearch&gt;: Grid Search\n* Parameters: resolution=11, batch_size=10\n* Parameter classes: ParamLgl, ParamInt, ParamDbl, ParamFct\n* Properties: dependencies, single-crit, multi-crit\n* Packages: mlr3tuning\n\n\n第一行说明寻优算法类型；第二行给出该寻优算法的参数及其设置值；第三行为该寻优算法支持寻优的参数类型，第四行为该寻优算法的性质，即可优化的目标类型；第五行为该寻优算法所依赖的工具包。\n(2)运行调优器。\n接下来，将调优实例传递给调优器对象的$optimize()方法，这将在每个重抽样子集上评估所有超参数配置，如果重抽样子集过多、超参数配置过多，则寻优过程极为耗时并极大地占用计算机硬件资源，尤其是遍历式的寻优算法，如”grid_search”。\n调优结束后返回最优超参数配置：\n\n# 运行需要一定时间，并产生较多信息\ntu$optimize(ins)\n\n调优结果保存在调优实例中，在控制台再次查看调优实例的信息：\n\nins\n\n\n## &lt;TuningInstanceSingleCrit&gt;\n## * State:  Optimized\n## * Objective: &lt;ObjectiveTuning:classif.svm_on_sonar&gt;\n## * Search Space:\n##        id    class     lower    upper nlevels\n##    &lt;char&gt;   &lt;char&gt;     &lt;num&gt;    &lt;num&gt;   &lt;num&gt;\n## 1:   cost ParamDbl -11.51293 11.51293     Inf\n## 2:  gamma ParamDbl -11.51293 11.51293     Inf\n## * Terminator: &lt;TerminatorNone&gt;\n## * Result:\n##       cost    gamma classif.ce\n##      &lt;num&gt;    &lt;num&gt;      &lt;num&gt;\n## 1: 9.21034 -4.60517  0.1297329\n## * Archive:\n##            cost      gamma classif.ce\n##           &lt;num&gt;      &lt;num&gt;      &lt;num&gt;\n##   1: -11.512925  -6.907755  0.4667828\n##   2: -11.512925  11.512925  0.4667828\n##   3:  -6.907755   2.302585  0.4667828\n##   4:   0.000000  -6.907755  0.2643438\n##   5:   2.302585 -11.512925  0.4667828\n##  ---                                 \n## 117:  -4.605170  -9.210340  0.4667828\n## 118:   2.302585  -6.907755  0.2349593\n## 119:   9.210340 -11.512925  0.2686411\n## 120:   9.210340   6.907755  0.4667828\n## 121:  11.512925   6.907755  0.4667828\n\n与未优化前相比，调优实例ins的State属性已经改变为Optimized，同时增加了Result和Archive内容，其中Result为寻优结果，即最优超参数配置与相应的性能测度，Archive保存了所有超参数配置及其性能测度。\n通过以下代码可查看实际最优超参数配置：\n\nins$result$learner_param_vals\n\n\n## [[1]]\n## [[1]]$kernel\n## [1] \"radial\"\n## \n## [[1]]$type\n## [1] \"C-classification\"\n## \n## [[1]]$cost\n## [1] 10000\n## \n## [[1]]$gamma\n## [1] 0.01\n\n这里显示的超参数cost和gamma的值之所以不同于调优实例Result中保存的值，是因为后者保存的超参数值通过log()函数进行了变换，通过exp()函数即可将其还原。\n通过autoplot()对调优结果进行可视化，结果如图 图 5.16 所示：\n\nautoplot(ins, type = \"surface\")\n\n\n\n\n\n\n\n\n\n图 5.16: 超参数寻优结果热力图\n\n\n\n\n\n使用网格搜索法的技巧：先在粗粒度的超参数配置网格上搜索，然后在性能测度最优区域建立细粒度的超参数配置网格再次搜索，从而找到更接近全局最优的超参数配置。\n\n\n5.4.3 训练最优模型\n(1)创建学习器。\n\nsvm_opt = lrn(\"classif.svm\")\n\n(2)将最优超参数配置传给学习器。\n\nsvm_opt$param_set$values = ins$result_learner_param_vals\n\n(3)在训练集上训练模型。\n\nsvm_opt$train(task, row_ids = split$train)\n\n(4)计算模型的拟合误差和预测误差。\n\nsvm_opt$predict(task, row_ids = split$train)$score()\n\n\n## classif.ce \n##          0\n\n\nsvm_opt$predict(task, row_ids = split$test)$score()\n\n\n## classif.ce \n## 0.05797101\n\n从结果看，模型在训练集上的分类错误率为0，而在测试集上分类错误率约5.80%。\n\n\n5.4.4 简化超参数优化操作\nmlr3提供了两种无需预先创建调优实例的超参数优化途径。\n(1)使用tune()函数。\n将创建调优实例的组件传给tune()，可直进行超参数优化，并返回调优实例对象：\n\n# 运行需要一定时间，并产生较多信息\nins = tune(tuner = tu,\n           task = task,\n           learner = lr,\n           resampling = rs,\n           measures = ms,\n           terminator = tm)\n\n然后通过$result_learner_param_vals查询该调优实例对象保存的最优超参数配置。\n(2)使用auto_tuner()函数。\n将创建调优实例的组件传给auto_tuner()，该函数将创建一个AutoTuner对象。AutoTuner对象继承了类Learner的属性和方法，并封装了所有调优信息，这意味着可以像操作Learner对象一样操作该对象。AutoTuner对象在调用$train()方法时，先在底层对任务数据执行tune()函数，然后将学习器超参数设置为最优配置，再在训练集上训练模型。\n\nat = auto_tuner(\n  tuner = tu,\n  learner = lr,\n  resampling = rs,\n  measure = ms,\n  terminator = tm\n)\n\n查看该对象的元信息：\n\nat\n\n\n## &lt;AutoTuner:classif.svm.tuned&gt;\n## * Model: list\n## * Search Space:\n## &lt;ParamSet&gt;\n##        id    class     lower    upper nlevels\n##    &lt;char&gt;   &lt;char&gt;     &lt;num&gt;    &lt;num&gt;   &lt;num&gt;\n## 1:   cost ParamDbl -11.51293 11.51293     Inf\n## 2:  gamma ParamDbl -11.51293 11.51293     Inf\n##                                                                                      default\n##                                                                                       &lt;list&gt;\n## 1: &lt;NoDefault&gt;\\n  Public:\\n    clone: function (deep = FALSE) \\n    initialize: function () \n## 2: &lt;NoDefault&gt;\\n  Public:\\n    clone: function (deep = FALSE) \\n    initialize: function () \n##     value\n##    &lt;list&gt;\n## 1:       \n## 2:       \n## Trafo is set.\n## * Packages: mlr3, mlr3tuning, mlr3learners, e1071\n## * Predict Type: response\n## * Feature Types: logical, integer, numeric\n## * Properties: multiclass, twoclass\n\n像Learner对象一样调用$train()方法，HPO过程融入模型训练过程：\n\n# 运行需要一定时间，并产生较多信息\nat$train(task, row_ids = train)\n\n计算训练集拟合性能：\n\nat$predict(task, row_ids = train)$score()\n\n\n## classif.ce \n##          0 \n\n计算预测集预测性能：\n\nat$predict(task, row_ids = test)$score()\n\n\n## classif.ce \n##  0.3958333 \n\n\n\n5.4.5 设置超参数配置搜索空间的其他方法\n\n5.4.5.1 利用ps()函数\nparadox包中的ps()可以更灵活地设置机器学习算法的超参数配置搜索空间。对于 例 5.2 中的classif.svm算法的超参数配置搜索空间，通过ps()进行配置，代码如下：\n\nlibrary(mlr3verse)\ntask = tsk(\"sonar\", positive = \"M\")\nset.seed(2023)\nsplit = partition(task)\nlr = lrn(\"classif.svm\", kernel = \"radial\", type = \"C-classification\")\nss = ps(cost  = p_dbl(lower = log(1e-1), upper = log(1e5),\n                      trafo = function(x) exp(x)),\n        gamma = p_dbl(lower = log(1e-5), upper = log(1e5),\n                      trafo = function(x) exp(x)))\n\nparadox包中函数p_dbl()、p_int()``、p_fct()、p_lgl()和p_uty()分别创建对应于mlr3包中基础类ParamDbl、Param_Int、Param_Fct、Param_Lgl和Param_Uty的超参数对象。\n\n# 配置重抽样策略、性能测度、寻优终止条件、寻优算法\nrs = rsmp(\"cv\", folds = 5)\nms = msr(\"classif.ce\")\ntm = trm(\"none\")\ntu = tnr(\"grid_search\", resolution = 11, batch_size = 10)\n# 创建自动调优器\nat = auto_tuner(\n  tuner = tu,\n  learner = lr,\n  search_space = ss,\n  resampling = rs,\n  measure = ms,\n  terminator = tm)\n\n\n# 训练\n# 运行需要一定时间，并产生较多信息 \nat$train(task, row_ids = split$train)\n\n\n# 预测\nat$predict(task, row_ids = split$train)$score()\n\n\n## classif.ce \n##          0 \n\n\n# 计算预测集性能测度\nat$predict(task, row_ids = split$test)$score()\n\n\n## classif.ce \n## 0.07246377\n\nps()对超参数配置搜索空间的设置，比to_tune()相对复杂一些，但功能更强，特别是能够通过.extra_trafo参数对超参数配置搜索空间提供更灵活和更复杂的设置。例如，可以建立“伪超参数”与超参数之间的关系，将超参数寻优转换为对“伪超参数”的寻优。\n\nss1 = ps(\n  a = p_int(1,  10),\n  b = p_int(4,36),\n  .extra_trafo = function(x, param_set){\n    x$rhp = rep(x$b, x$a)\n    x$a = NULL\n    x$b = NULL\n    x\n  }\n)\n\n上面的代码在ps()中，利用p_int()建立了两个伪超参数a和b，通过.eatra_trafo建立了实际超参数rhp与a和b之间的关系，本例中rhp是由a和b构成的向量，a是向量的长度，b是向量的元素；然后删除a和b，返回实际超参数rhp。可以将a和b的值传给$trafo()方法来查询输出结果：\n\nss1$trafo(list(a = 5, b = 6))\n\n\n## $rhp\n## [1] 6 6 6 6 6\n\n这对向量型超参数的寻优极为有用，因为to_tune()只能对标量型超参数设置搜索空间。\n此外，用ps()设置超参数搜索空间，还可以解决超参数依赖问题。所谓超参数依赖，是指一个超参数在另一个超参数被设置为特定值时才允许被设置。例如，在支持向量机算法中，超参数degree只有在超参数kernel的值为polynomial时，才允许被设置。\n\nss2 = ps(\n  cost = p_dbl(1e-3, 1e3),\n  kernel = p_fct(c(\"radial\", \"polynomial\")),\n  degree = p_int(1, 3, depends = (kernel == \"polynomial\")),\n  gamma = p_dbl(1e-3, 1e3, depends = (kernel %in% c(\"radial\", \"polynomial\")))\n)\n\n以上代码解决了超参数degree和gamma对超参数kernel的依赖问题。可在控制台输入ss2来查看以上代码创建的ParamSet对象：\n\nss2\n\n&lt;ParamSet&gt;\n\n\nWarning: Unknown argument 'on' has been passed.\n\n\nKey: &lt;id&gt;\n       id    class lower upper nlevels\n   &lt;char&gt;   &lt;char&gt; &lt;num&gt; &lt;num&gt;   &lt;num&gt;\n1:   cost ParamDbl 0.001  1000     Inf\n2: degree ParamInt 1.000     3       3\n3:  gamma ParamDbl 0.001  1000     Inf\n4: kernel ParamFct    NA    NA       2\n                                                                                     default\n                                                                                      &lt;list&gt;\n1: &lt;NoDefault&gt;\\n  Public:\\n    clone: function (deep = FALSE) \\n    initialize: function () \n2: &lt;NoDefault&gt;\\n  Public:\\n    clone: function (deep = FALSE) \\n    initialize: function () \n3: &lt;NoDefault&gt;\\n  Public:\\n    clone: function (deep = FALSE) \\n    initialize: function () \n4: &lt;NoDefault&gt;\\n  Public:\\n    clone: function (deep = FALSE) \\n    initialize: function () \n   parents  value\n    &lt;list&gt; &lt;list&gt;\n1:               \n2:  kernel       \n3:  kernel       \n4:               \n\n\n第一行表明ss2是基于ParamSet类创建的对象，其余内容为超参数的信息表，其中parents列为各个超参数的依赖项。\n在创建Learner对象后，利用$param_set$deps查询该学习器存在依赖项的超参数：\n\nlr1 = lrn(\"regr.svm\")\nlr1$param_set$deps\n\n        id     on              cond\n    &lt;char&gt; &lt;char&gt;            &lt;list&gt;\n1:   coef0 kernel &lt;CondAnyOf:anyof&gt;\n2:    cost   type &lt;CondAnyOf:anyof&gt;\n3:  degree kernel &lt;CondEqual:equal&gt;\n4: epsilon   type &lt;CondEqual:equal&gt;\n5:   gamma kernel &lt;CondAnyOf:anyof&gt;\n6:      nu   type &lt;CondEqual:equal&gt;\n\n\n结果表明，regr.svm学习器存在6个有依赖项的超参数。再通过$param_set$deps$cond查看具体的依赖项：\n\nlr1$param_set$deps$cond\n\n[[1]]\nCondAnyOf: x ∈ {polynomial, sigmoid}\n\n[[2]]\nCondAnyOf: x ∈ {eps-regression, nu-regression}\n\n[[3]]\nCondEqual: x = polynomial\n\n[[4]]\nCondEqual: x = eps-regression\n\n[[5]]\nCondAnyOf: x ∈ {polynomial, radial, sigmoid}\n\n[[6]]\nCondEqual: x = nu-regression\n\n\n\n\n5.4.5.2 利用mlr3tuningspaces工具包\n设置超参数配置的搜索空间往往需要大量专业知识。mlr3扩展包mlr3tuningspaces为很多流行的机器学习算法提供了预先设定的搜索空间，这些超参数配置搜索空间的关键字可在加载mlr3tuningspaces包后，通过mlr_tuning_spaces查询，查询指定学习器超参数配置搜索空间的具体方法如下：\n\nas.data.table(mlr_tuning_spaces)[learner == \"classif.svm\", ]\n\nKey: &lt;key&gt;\n                   key                             label     learner n_values\n                &lt;char&gt;                            &lt;char&gt;      &lt;char&gt;    &lt;int&gt;\n1: classif.svm.default   Classification SVM with Default classif.svm        4\n2:    classif.svm.rbv1 Classification SVM with RandomBot classif.svm        4\n3:    classif.svm.rbv2 Classification SVM with RandomBot classif.svm        5\n\n\n以上代码查询mlr3tuningspaces包中对classif.svm学习器预先设定的超参数配置搜索空间。\n将key列中的值传递给lts()或ltss()得相应搜索空间的具体信息：\n\nlts(\"classif.svm.rbv2\")\n\n&lt;TuningSpace:classif.svm.rbv2&gt;: Classification SVM with RandomBot\n          id lower upper                   levels logscale\n      &lt;char&gt; &lt;num&gt; &lt;num&gt;                   &lt;list&gt;   &lt;lgcl&gt;\n1:    kernel    NA    NA linear,polynomial,radial    FALSE\n2:      cost 1e-04  1000                              TRUE\n3:     gamma 1e-04  1000                              TRUE\n4: tolerance 1e-04     2                              TRUE\n5:    degree 2e+00     5                             FALSE\n\n\nlts()函数获取的搜索空间可以直接传递给ti()函数中的search_space参数：\n\nins = ti(\n  task = tsk(\"sonar\"),\n  learner = lrn(\"classif.svm\"),\n  resampling = rsmp(\"cv\", folds = 3),\n  measures = msr(\"classif.ce\"),\n  terminator = trm(\"evals\", n_evals = 20),\n  search_space = lts(\"classif.svm.rbv2\")\n)\n\n也可以间接传给学习器对象：\n\nlr = lrn(\"classif.svm\")\nss = lts(\"classif.svm.rbv2\")\nlr$param_set$set_values(.values = ss$values)\n\n\n\n\n5.4.6 基于嵌套重抽样的泛化性能评估\n当用于超参数优化和优化后模型性能评估的数据相同时，模型性能的估计可能过于乐观。为了减少模型性能估计的偏差，有必要将超参数优化和模型性能评估的过程分离，亦即用于超参数优化的数据与模型性能评估的数据保持独立。嵌套抽样技术包括外部抽样和内部抽样，外部抽样的训练集用于超参数优化，测试集用于评估最佳超参数配置模型的性能， 图 5.17 演示了外部3折交叉验证与内部4折交叉验证的嵌套重抽样方案。\n\n\n\n\n\n\n\n\n图 5.17: 嵌套重抽样估计泛化性能\n\n\n\n\n\n嵌套重抽样并非选择最佳超参数配置的方法，而是一种减小模型泛化性能估计偏差的策略。外部交叉验证重抽样的每一折都会选出一个对应的最佳超参数配置的模型，因此该策略会得到多个不同超参数配置的最优模型的性能，而不是单个配置的最优模型的性能。\n\n例 5.3 应用嵌套重抽样策略估计支持向量机算法在构造在friedman1数据集上的泛化性能。\n\n利用tgen()函数生成基于Friedman1回归任务：\n\nlibrary(mlr3verse)\n\nset.seed(2023)\ntk = tgen(\"friedman1\")\ntk_train = tk$generate(1000)\ntk_test = tk$generate(100000)\n\n该任务有10个特征变量(均匀分布于区间[0, 1])，前5个为有用特征(x1~x5)，后5个为无用特征，标签变量y根据以下数学表达式由前五个特征变量生成：\n y=10 sin⁡(π x_1 x_2 )+20(x_3-0.5)^2+10x_4+5x_5+e 式中e \\sim N(0,\\sigma)。\n接着创建学习器、超参数配置搜索空间、性能测度、寻优算法、终止条件、内部重抽样策略和外部重抽样策略：\n\nlr = lrn(\"regr.svm\", type = \"nu-regression\")\nss = lts(\"regr.svm.rbv2\")\nms = msr(\"regr.mse\")\ntu = tnr(\"random_search\")\ntr = trm(\"evals\", n_evals = 100)\nrs_inner = rsmp(\"holdout\")\nrs_outer = rsmp(\"cv\", folds = 5)\n\n然后根据内部重抽样策略调优并返回泛化性能估计：\n\n# 运行需要一定时间，并产生较多的信息\nfuture::plan(\"multisession\")  # 利用future执行多线程并行运算\nins = tune(\n  tuner = tu,\n  task = tk_train,\n  learner = lr,\n  resampling = rs_inner,\n  measures = ms,\n  terminator = tr,\n  search_space = ss\n)\nfuture::plan(\"sequential\")    # 恢复单线程\n\n计算非嵌套重抽样的泛化性能测度估计值：\n\ninsample = ins$result_y\n\n配置基于嵌套重抽样策略的自动调优器对象：\n\nat = auto_tuner(\n  tuner = tu,\n  learner = lr,\n  measure = ms,\n  resampling = rs_inner,\n  terminator = tr,\n  search_space = ss\n)\n\n计算嵌套重抽样的泛化性能测度估计值：\n\nfuture::plan(\"multisession\")    # 执行多线程并行运算\n# 运行需要一定时间，并产生较多的信息\noutsample = resample(tk_train, at, rs_outer, store_models = F)$aggregate()\nfuture::plan(\"sequential\")    # 恢复单线程\n\n根据最优超参数配置训练模型并计算该模型在测试集tk_test上的实际泛化性能：\n\nlr_tuned = lrn(\"regr.svm\", type = \"nu-regression\")\nlr_tuned$param_set$set_values(.values = ins$result_learner_param_vals)\ngp = lr_tuned$train(tk_train)$predict(tk_test)$score()\n\n输出并比较非嵌套重抽样和嵌套重抽样估计的泛化性能与实际泛化性能的大小：\n\ncat(\" 实际泛化性能 =\", as.numeric(gp),\n    \"\\n\",\n    \"非嵌套重抽样的估计值 = \", as.numeric(insample),\n    \"\\n\",\n\"嵌套重抽样的估计值 =\", as.numeric(outsample))\n\n\n## 实际泛化性能 = 7.770473 \n## 非嵌套重抽样的估计值 =  5.442172 \n## 嵌套重抽样的估计值 = 8.188057\n\n显然，非嵌套重抽样的泛化性能估计过于乐观了，而嵌套重抽样的估计值更接近实际泛化性能，甚至更保守。\n\n练习 5.2 \n基于mlr3内置任务iris和 练习 5.1 的结果，参照 例 5.2 的全部内容进行练习。",
    "crumbs": [
      "进阶知识",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>R语言机器学习建模</span>"
    ]
  },
  {
    "objectID": "CH5.html#特征选择",
    "href": "CH5.html#特征选择",
    "title": "5  R语言机器学习建模",
    "section": "5.5 特征选择",
    "text": "5.5 特征选择\n高维大数据中往往包括太多冗余变量，一般通过特征工程来解决，以克服此类问题对机器学习建模的不利影响。特征提取和特征选择是最常用的特征工程方法，前者通过特定算法从原始特征提取一组精简的新特征来替代原始特征，如主成分分析方法；后者是利用特定算法从原始特征中选择一个精简的特征子集来替代原始特征。特征提取的缺陷在于改变了原始特征，降低了机器学习模型的可解释性。通过特征工程可以为机器学习建模带来诸多好处，例如：因减少模型对不相关特征的过拟合而提升预测性能，因不依赖于噪声特征而让模型更稳健，因特征数减少而使模型更简单和更易于解释，模型训练和预测时间成本更低，以及无需收集成本更高的潜在特征等。\n\n5.5.1 特征过滤器\nmlr3filters工具包提供了多种特征选择算法的封装对象，称为特征过滤器，即Filter对象。在控制台输入mlr_filters可查看特征过滤器的关键字：\n\nmlr_filters\n\n&lt;DictionaryFilter&gt; with 21 stored values\nKeys: anova, auc, carscore, carsurvscore, cmim, correlation, disr,\n  find_correlation, importance, information_gain, jmi, jmim,\n  kruskal_test, mim, mrmr, njmim, performance, permutation, relief,\n  selected_features, variance\n\n\n查询关键字为importance的特征过滤器的详细信息：\n\nas.data.table(mlr_filters)[key == \"importance\"]\n\nKey: &lt;key&gt;\n          key            label task_types task_properties params\n       &lt;char&gt;           &lt;char&gt;     &lt;list&gt;          &lt;list&gt; &lt;list&gt;\n1: importance Importance Score    classif                 method\n                                          feature_types packages\n                                                 &lt;list&gt;   &lt;list&gt;\n1: logical,integer,numeric,character,factor,ordered,...     mlr3\n\n\nflt()和flts()函数用于创建单个和多个Filter对象：\n\n(fs_cor = flt(\"correlation\", method = \"kendall\"))\n\n&lt;FilterCorrelation:correlation&gt;: Correlation\nTask Types: regr\nProperties: missings\nTask Properties: -\nPackages: stats\nFeature types: integer, numeric\n\n\n\n(fs_mj = flts(c(\"mrmr\", \"jmim\")))\n\n$mrmr\n&lt;FilterMRMR:mrmr&gt;: Minimum Redundancy Maximal Relevancy\nTask Types: classif, regr\nProperties: -\nTask Properties: -\nPackages: praznik\nFeature types: integer, numeric, factor, ordered\n\n$jmim\n&lt;FilterJMIM:jmim&gt;: Minimal Joint Mutual Information Maximization\nTask Types: classif, regr\nProperties: -\nTask Properties: -\nPackages: praznik\nFeature types: integer, numeric, factor, ordered\n\n\n将创建好的Task对象传入Filter对象的$calculate()方法，即可计算任务数据中各特征的得分，然后再根据得分来选择或丢弃特征。例如：\n\nfs_cor$calculate(tsk(\"mtcars\"))\n\n再来查看fs_cor的信息：\n\nfs_cor\n\n&lt;FilterCorrelation:correlation&gt;: Correlation\nTask Types: regr\nProperties: missings\nTask Properties: -\nPackages: stats\nFeature types: integer, numeric\n    feature     score\n 1:     cyl 0.7953134\n 2:    disp 0.7681311\n 3:      hp 0.7428125\n 4:      wt 0.7278321\n 5:      vs 0.5896790\n 6:    carb 0.5043945\n 7:      am 0.4690128\n 8:    drat 0.4645488\n 9:    gear 0.4331509\n10:    qsec 0.3153652\n\n\n还可以利用具有$importance()和$selected_features()方法的学习器来筛选特征：\n\ntask = tsk(\"iris\")\nlr = lrn(\"classif.rpart\")\nfs_imp = flt(\"importance\", lr)\n(fs_imp$calculate(task))\n\n&lt;FilterImportance:importance&gt;: Importance Score\nTask Types: classif\nProperties: missings\nTask Properties: -\nPackages: mlr3, rpart\nFeature types: logical, integer, numeric, factor, ordered\n        feature    score\n1:  Petal.Width 88.96940\n2: Petal.Length 81.34496\n3: Sepal.Length 54.09606\n4:  Sepal.Width 36.01309\n\n\n\nkeep = names(which(fs_imp$scores &gt;= 50))    # 根据得分选择特征\ntask$select(keep)\ntask$feature_names\n\n[1] \"Petal.Length\" \"Petal.Width\"  \"Sepal.Length\"\n\n\n\nfs_sf = flt(\"selected_features\", lr)\n(fs_sf$calculate(task))\n\n&lt;FilterSelectedFeatures:selected_features&gt;: Embedded Feature Selection\nTask Types: classif\nProperties: missings\nTask Properties: -\nPackages: mlr3, rpart\nFeature types: logical, integer, numeric, factor, ordered\n        feature score\n1:  Petal.Width     1\n2: Petal.Length     1\n3: Sepal.Length     0\n\n\n\nkeep = names(which(fs_sf$scores == 1))    # 根据得分选择特征\ntask$select(keep)\ntask$feature_names\n\n[1] \"Petal.Length\" \"Petal.Width\" \n\n\n\n\n5.5.2 特征选择封装方法\n特征选择是服务于模型泛化性能的提升，因此，应考虑将学习器与特征选择封装在一起来筛选确保学习器性能最优的特征子集。这种方法与HPO方法类似，即搜索让学习器性能最优的特征子集，这需要让学习器在不同的特征子集上迭代训练和预测，因此也需要配置重抽样策略、测度和终止条件，再将其与任务对象和学习器封装在一起，创建显式的特征选择实例(FSelectInstance对象)，然后传入指定的特征选择器(FSelector对象)的$optimize()方法，或者采用fselect()函数封装FSelector对象和特征选择实例的各个组件，创建自动运行的隐式特征选择实例。为了无偏估计泛化性能，可以将特征选择器与学习器、测度、重抽样策略及终止条件一起封装为AutoFSelector对象，然后传给benchmark()函数或resample()函数，从而通过嵌套重抽样策略来无偏估计不同特征子集上的泛化性能。\nmlr3fselct工具包提供了封装方法，通过fsi()函数创建特征选择实例，通过fs()函数指定特征选择算法，也称特征选择器。查询特征选择器的关键字可在控制输入mlr_fselectors，查询详细信息只需将mlr_fselectors传入as.data.table()函数。\n\nmlr3fselect::mlr_fselectors\n\n&lt;DictionaryFSelector&gt; with 8 stored values\nKeys: design_points, exhaustive_search, genetic_search, random_search,\n  rfe, rfecv, sequential, shadow_variable_search\n\n\n查询具体特征选择器的帮助信息，可在控制台输入?mlr_fselectors_key(key为特征选择器的关键字)，例如输入?mlr_fselectors_rfe即可打开关键字为rfe的特征选择器的帮助信息页面。\n下面演示用fsi()函数创建特征选择实例，并通过fs()函数创建的特征选择器来执行特征子集选择：\n\nlibrary(mlr3verse)\n\nlgr::get_logger(\"mlr3\")$set_threshold(\"warn\")\nlgr::get_logger(\"bbotk\")$set_threshold(\"warn\")\n\ntask = tsk(\"penguins\")\nins = fsi(\n  task = task,\n  learner = lrn(\"classif.rpart\"),\n  resampling = rsmp(\"cv\", folds = 5),\n  measures = msr(\"classif.acc\"),    # 可通过msrs传入多个测度\n  terminator = trm(\"evals\", n_evals = 50)\n)\nfsel = fs(\"random_search\", batch_size = 5)\nfsel$optimize(ins)\n\n   bill_depth bill_length body_mass flipper_length island    sex   year\n       &lt;lgcl&gt;      &lt;lgcl&gt;    &lt;lgcl&gt;         &lt;lgcl&gt; &lt;lgcl&gt; &lt;lgcl&gt; &lt;lgcl&gt;\n1:       TRUE        TRUE      TRUE          FALSE  FALSE   TRUE   TRUE\n                                    features n_features classif.acc\n                                      &lt;list&gt;      &lt;int&gt;       &lt;num&gt;\n1: bill_depth,bill_length,body_mass,sex,year          5    0.941688\n\n\n运行的所有结果都保存在$archive属性中，可将转换为data.table对象，然后过滤查询：\n\nas.data.table(ins$archive)[batch_nr==1, c(1:5,8)]\n\n   bill_depth bill_length body_mass flipper_length island classif.acc\n       &lt;lgcl&gt;      &lt;lgcl&gt;    &lt;lgcl&gt;         &lt;lgcl&gt; &lt;lgcl&gt;       &lt;num&gt;\n1:       TRUE       FALSE      TRUE           TRUE  FALSE   0.7817136\n2:       TRUE        TRUE      TRUE          FALSE  FALSE   0.9387894\n3:       TRUE        TRUE     FALSE           TRUE   TRUE   0.9389173\n4:       TRUE       FALSE      TRUE           TRUE   TRUE   0.8487212\n5:       TRUE        TRUE      TRUE          FALSE  FALSE   0.9416880\n\n\n利用autoplot()进行可视化：\n\nautoplot(ins, type = \"performance\")  \n\n\n\n\n\n\n\n图 5.18: 特征选择迭代中的模型性能\n\n\n\n\n\ntype参数的设置选项参考mlr3viz工具包中的autoplot.OptimInstanceSingleCrit。\n最优特征子集保存在$result_features_set属性中：\n\nins$result_feature_set\n\n[1] \"bill_depth\"  \"bill_length\" \"body_mass\"   \"sex\"         \"year\"       \n\n\n$result属性保存了最优特征选择子集和对应性能测度值：\n\nins$result\n\n   bill_depth bill_length body_mass flipper_length island    sex   year\n       &lt;lgcl&gt;      &lt;lgcl&gt;    &lt;lgcl&gt;         &lt;lgcl&gt; &lt;lgcl&gt; &lt;lgcl&gt; &lt;lgcl&gt;\n1:       TRUE        TRUE      TRUE          FALSE  FALSE   TRUE   TRUE\n                                    features n_features classif.acc\n                                      &lt;list&gt;      &lt;int&gt;       &lt;num&gt;\n1: bill_depth,bill_length,body_mass,sex,year          5    0.941688\n\n\n函数fselect()能将特征选择实例的组件和特征选择器封装在一起并自动运行，返回的结果为特征选择实例对象：\n\nlgr::get_logger(\"mlr3\")$set_threshold(\"warn\")\nlgr::get_logger(\"bbotk\")$set_threshold(\"warn\")\n\nins1 = fselect(\n  fselector = fs(\"random_search\", batch_size = 5),\n  task = task,\n  learner = lrn(\"classif.rpart\"),\n  resampling = rsmp(\"cv\", folds = 5),\n  measures = msr(\"classif.acc\"),\n  terminator = trm(\"evals\",n_evals = 50)\n)\n\n同样可以查看特征选择结果和进一步的信息：\n\nins1$result_feature_set\n\n[1] \"bill_depth\"     \"bill_length\"    \"flipper_length\" \"sex\"           \n[5] \"year\"          \n\nins1$result\n\n   bill_depth bill_length body_mass flipper_length island    sex   year\n       &lt;lgcl&gt;      &lt;lgcl&gt;    &lt;lgcl&gt;         &lt;lgcl&gt; &lt;lgcl&gt; &lt;lgcl&gt; &lt;lgcl&gt;\n1:       TRUE        TRUE     FALSE           TRUE  FALSE   TRUE   TRUE\n                                         features n_features classif.acc\n                                           &lt;list&gt;      &lt;int&gt;       &lt;num&gt;\n1: bill_depth,bill_length,flipper_length,sex,year          5   0.9474851\n\n\n为了无偏估计特征子集上学习器的泛化性能，需要创建AutoFSelector对象并传给benchmark()或resample()函数，利用嵌套重抽样策略来评估性能。\nauto_fselector()函数用来创建AutoFSelector对象：\n\nafs = auto_fselector(\n  fselector = fs(\"random_search\", batch_size = 5),\n  learner = lrn(\"classif.rpart\"),\n  resampling = rsmp(\"cv\", folds = 5),\n  measure = msr(\"classif.acc\"),\n  terminator = trm(\"evals\",n_evals = 50)\n)\n\n创建基准测试方案：\n\ngrid = benchmark_grid(tsk(\"sonar\"), \n                      list(afs, lrn(\"classif.rpart\")),\n                      rsmp(\"cv\", folds = 5))\n\n采用多线程并行方案运行基准测试：\n\nlgr::get_logger(\"mlr3\")$set_threshold(\"warn\")\nlgr::get_logger(\"bbotk\")$set_threshold(\"warn\")\n\nfuture::plan(\"multisession\", workers = 12)    # 采用多线程并行运算\nbmr = benchmark(grid)\nfuture::plan(\"sequential\")    # 恢复单线程\n\n比较两个学习器的平均预测准确率：\n\nbmr$aggregate(msr(\"classif.acc\"))\n\n\n##    nr task_id              learner_id resampling_id iters classif.acc\n## 1:  1   sonar classif.rpart.fselector            cv     5   0.7013937\n## 2:  2   sonar           classif.rpart            cv     5   0.6587689\n\n结果表明，开展特征选择提升了决策树模型的性能。 与HPO一样，也可以将AutoFSelector对象传入resample()函数，利用嵌套重抽样策略来无偏估计泛化性能。",
    "crumbs": [
      "进阶知识",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>R语言机器学习建模</span>"
    ]
  },
  {
    "objectID": "CH5.html#管道与图学习器",
    "href": "CH5.html#管道与图学习器",
    "title": "5  R语言机器学习建模",
    "section": "5.6 管道与图学习器",
    "text": "5.6 管道与图学习器\nmlr3通过模块化的统一接口(如Task、Learner、Measure等对象)为机器学习应用者提供了非常简单、且无需底层专家知识就能快速开展机器学习建模，即使复杂的基准测试和HPO，也只需要几行代码就能实现。工具包mlr3pipelines将mlr3的模块化功能扩展到数据预处理、集成学习(ensemble learning)以及更复杂模型的构建。mlr3pipelines使用从PipeOp类继承的模块对象构建Graph对象(一种有向图)来表示操作的数据流或建模流程。在模型训练过程中，图中的管道操作转换给定的任务，随后的管道操作接收转换后的任务作为输入。Graph对象能够转换为GraphLearner(图学习器)对象，该对象和Learner对象一样，可以进行训练、调优和预测。因此，图学习器可以将数据的预处理和学习器的调优、训练整合成在一起，形成一个能够可视化的完整流程。\n\n5.6.1 管道操作与图对象\n加载mlr3pipelines包后，在控制台输入mlr_pipeops可以查看各种管道操作(即基类PipeOp的子类)的关键字，查询管道操作的详细信息可输入as.data.table(mlr_pipeops)。\n\nmlr3pipelines::mlr_pipeops\n\n&lt;DictionaryPipeOp&gt; with 64 stored values\nKeys: boxcox, branch, chunk, classbalancing, classifavg, classweights,\n  colapply, collapsefactors, colroles, copy, datefeatures, encode,\n  encodeimpact, encodelmer, featureunion, filter, fixfactors, histbin,\n  ica, imputeconstant, imputehist, imputelearner, imputemean,\n  imputemedian, imputemode, imputeoor, imputesample, kernelpca,\n  learner, learner_cv, missind, modelmatrix, multiplicityexply,\n  multiplicityimply, mutate, nmf, nop, ovrsplit, ovrunite, pca, proxy,\n  quantilebin, randomprojection, randomresponse, regravg,\n  removeconstants, renamecolumns, replicate, scale, scalemaxabs,\n  scalerange, select, smote, spatialsign, subsample, targetinvert,\n  targetmutate, targettrafoscalerange, textvectorizer, threshold,\n  tunethreshold, unbranch, vtreat, yeojohnson\n\n\n这些管道操作大多数是数据预处理的操作，如缺失值处理、数据转换、特征选择与提取、特征编码等。查询具体的管道操作的帮助信息，使用?mlr_pipeops_key的格式(key为对应的关键字)，例如查询encode管道操作的帮助信息：\n\n?mlr_pipeops_encode\n\nPipeOp对象具有$train()和$predict()方法，也拥有相应的超参数，同样通过$param_set属性查询和设置超参数。将管道操作的关键字传给po()或pos()函数即可创建一个或多个管道操作：\n\nlibrary(mlr3verse)\n(po_pca = po(\"pca\"))    # 引用R中prcomp函数进行主成分分析\n\nPipeOp: &lt;pca&gt; (not trained)\nvalues: &lt;list()&gt;\nInput channels &lt;name [train type, predict type]&gt;:\n  input [Task,Task]\nOutput channels &lt;name [train type, predict type]&gt;:\n  output [Task,Task]\n\n\n\n(po_pca_nop = pos(c(\"pca\", original = \"nop\")))    # 主成分分析和无操作\n\n$pca\nPipeOp: &lt;pca&gt; (not trained)\nvalues: &lt;list()&gt;\nInput channels &lt;name [train type, predict type]&gt;:\n  input [Task,Task]\nOutput channels &lt;name [train type, predict type]&gt;:\n  output [Task,Task]\n\n$original\nPipeOp: &lt;original&gt; (not trained)\nvalues: &lt;list()&gt;\nInput channels &lt;name [train type, predict type]&gt;:\n  input [*,*]\nOutput channels &lt;name [train type, predict type]&gt;:\n  output [*,*]\n\n\npo()和pos()函数还能将Learner对象和Filter对象(特征过滤器)封装成管道操作对象：\n\npo_dt = po(\"learner\", lrn(\"classif.rpart\", cp = 0.2))\npo_imp = po(\"filter\", flt(\"importance\"))\n\n也可以利用as_pipeop()函数转换：\n\npo_svm = as_pipeop(lrn(\"regr.svm\", id = \"svm\", type = \"nu-regression\"))\npo_cor = as_pipeop(flt(\"correlation\", id = \"cor\"))\n\n下面演示管道操作的训练和预测：\n\ntask = tsk(\"sonar\")\nset.seed(2023)\nsplit = partition(task)\npo_sc = po(\"scale\")\n\n注意，传给管道操作训练的任务必须用list()函数转换为列表类型。\n\npo_out = po_sc$train(list(task$clone()$filter(split$train)))\npo_out\n\n$output\n&lt;TaskClassif:sonar&gt; (139 x 61): Sonar: Mines vs. Rocks\n* Target: Class\n* Properties: twoclass\n* Features (60):\n  - dbl (60): V1, V10, V11, V12, V13, V14, V15, V16, V17, V18, V19, V2,\n    V20, V21, V22, V23, V24, V25, V26, V27, V28, V29, V3, V30, V31,\n    V32, V33, V34, V35, V36, V37, V38, V39, V4, V40, V41, V42, V43,\n    V44, V45, V46, V47, V48, V49, V5, V50, V51, V52, V53, V54, V55,\n    V56, V57, V58, V59, V6, V60, V7, V8, V9\n\n\n管道操作对象训练后的状态保存在$state属性中，一些管道操作对象的预测需要利用这些状态参数：\n\npo_pred = po_sc$predict(list(task$clone()$filter(split$test)))\npo_pred$output\n\n&lt;TaskClassif:sonar&gt; (69 x 61): Sonar: Mines vs. Rocks\n* Target: Class\n* Properties: twoclass\n* Features (60):\n  - dbl (60): V1, V10, V11, V12, V13, V14, V15, V16, V17, V18, V19, V2,\n    V20, V21, V22, V23, V24, V25, V26, V27, V28, V29, V3, V30, V31,\n    V32, V33, V34, V35, V36, V37, V38, V39, V4, V40, V41, V42, V43,\n    V44, V45, V46, V47, V48, V49, V5, V50, V51, V52, V53, V54, V55,\n    V56, V57, V58, V59, V6, V60, V7, V8, V9\n\n\n调用训练和预测返回对象的$output$data()方法，即可查看训练和预测的结果。\n\npo_out$output$data()[1:5, 1:5] \n\n    Class          V1        V10        V11        V12\n   &lt;fctr&gt;       &lt;num&gt;      &lt;num&gt;      &lt;num&gt;      &lt;num&gt;\n1:      R -0.13257495  2.9584957  3.0091724  3.4536107\n2:      R -0.81415548 -0.5738126 -1.1215268 -0.3634980\n3:      R  1.97106865  1.7153811  1.3567412  1.1127319\n4:      R  0.09882585  1.0375792 -0.4358550 -1.3682382\n5:      R -0.29665915 -0.4140348 -0.9131735 -0.6180724\n\n\n\npo_pred$output$data()[1:5, 1:5]\n\n    Class          V1        V10        V11        V12\n   &lt;fctr&gt;       &lt;num&gt;      &lt;num&gt;      &lt;num&gt;      &lt;num&gt;\n1:      R -0.39342676  0.0330566 -0.5699587 -0.6723012\n2:      R  0.67101691  0.5783074  1.9370999  3.0709960\n3:      R -0.03160006  0.6979617  0.4748384  1.3371791\n4:      R  0.94869786  0.5539467  0.3339158  0.4604793\n5:      R -0.54488909 -1.2996196 -1.1821387 -1.0684731\n\n\n在数据预处理阶段，往往需要针对特定类型的特征或选定的特征子集进行特定的管道操作，这可以借助工具包mlr3pipelines提供的selector_*()系列函数(见 表 5.4 )来执行特征选择，然后将其传给po()函数中的affect_columns参数，或利用po()函数直接将其封装为PipeOp对象，作为后续管道操作对象的前置对象。\n\n\n\n\n\n表 5.4: selector系列函数及其功能\n\n\n\n\n\n\n\n\n\n\nselector函数\n功能\n\n\n\n\nselector_all()\n选择所有特征\n\n\nselector_none()\n不选择特征\n\n\nselector_type()\n根据特征类型选择特征\n\n\nselector_name()\n根据特征名称选择特征\n\n\nselector_grep()\n根据模式(如正则化表达式)匹配选择特征\n\n\nselector_invert()\n针对给定的特征选择集反向选择特征\n\n\nselector_intersect()\n取两个特征选择集的交集\n\n\nselector_union()\n取两个特征选择集的并集\n\n\nselector_setdiff()\n取两个特征选择集的差集\n\n\nselector_missing()\n选择具有缺失值的特征\n\n\nselector_cardinality_greater_than()\n选择基数大于阈值的分类特征\n\n\n\n\n\n\n\n\n以mlr3内置的penguins任务为例：\n\ntask = tsk(\"penguins\")\nkeep = which(complete.cases(task$data()))\ntask = task$filter(keep)\ntask$feature_types\n\nKey: &lt;id&gt;\n               id    type\n           &lt;char&gt;  &lt;char&gt;\n1:     bill_depth numeric\n2:    bill_length numeric\n3:      body_mass integer\n4: flipper_length integer\n5:         island  factor\n6:            sex  factor\n7:           year integer\n\n\n该任务有7个特征，先选择factor类型特征：\n\nsel_fct = selector_type(\"factor\")\n\n然后选择余下的特征：\n\nsel_rest = selector_invert(sel_fct)\n\n在po()函数中通过affect_columns参数指定特征：\n\npo_pca = po(\"pca\", affect_columns = sel_rest, rank. = 2)\npo_enc = po(\"encode\", affect_columns = sel_fct)\npo_lr = po(\"learner\", lrn(\"classif.rpart\"), id = \"rpart\")\n\nPipeOp、Graph对象以及可转换为PipeOp对象的其他对象可以通过管道操作符%&gt;&gt;%和%&gt;&gt;!%连接为一个Graph对象：\n\ngr1 =  po_pca %&gt;&gt;% po_enc %&gt;&gt;% po_lr \ngr2 =  po_pca %&gt;&gt;!% po_enc %&gt;&gt;!% po_lr \n\n%&gt;&gt;%操作符(等同于concat_graph(g1, g2, in_place = F))总是创建其连接对象的深层副本，此后不能通过引用对其连接对象进行修改。%&gt;&gt;!%操作符(等同于concat_graph(g1, g2, in_place = T))在其第一个连接对象为Graph对象时，不会对其执行clone()操作，并对其就地(in_place)修改。通常情况下都建议使用%&gt;&gt;%操作符，当图中管道操作过多时(长图对象)，使用%&gt;&gt;!%操作符将带来显著的性能优势，因为%&gt;&gt;%操作符会产生很多次的clone()调用，增加内存资源占用。\nGraph对象的$plot()方法可默认借助igraph包进行可视化，如果传入参数html = TRUE，则借助visNetwork包执行可视化。\n\ngr1$plot(horizontal = T)\n\n\n\n\n\n\n\n\n上面这种结构的图很容易让人误解，下面将特征选择和特征处理分开，借助gunion()函数和featureunion管道操作对象，建立结构更容易理解的Graph对象：\n\npofs_fct = po(\"select\", selector = sel_fct, id = \"feat_fct\")\npofs_num = po(\"select\", selector = sel_rest, id = \"feat_num\")\npath_enc = pofs_fct %&gt;&gt;% po(\"encode\")\npath_pca = pofs_num %&gt;&gt;% po(\"pca\", rank. = 2)\ngr = gunion(list(path_enc, path_pca)) %&gt;&gt;% \n  po(\"featureunion\", id = \"feat_union\") %&gt;&gt;% po_lr\ngr$plot(html = T)\n\n\n\n\n\nGraph对象与PipeOp对象一样，具有$train()和$predict()以及$state等方法和属性，传入任务对象即可进行训练和预测。默认情况下，Graph对象只保留最后一个管道操作的输出，如果保留其中所有管道操作的结果，需要设置Graph对象的属性$keep_results = T，然后就可在训练后通过$pipeops$xxx$.result$output$data()来查看id为xxx的管道操作的结果。\nmlr3pipelines包预置了7分常用的Graph集合对象，在控制台输入mlr_graphs可查询关键字，查询帮助信息可按以下形式输入代码：?mlr_graphs_key，key为mlr_graphs中储存的关键字。\n\n\n\n\n表 5.5: Graph集合对象、功能及其调用方法\n\n\n\n\n\n\n\n\n\n\n\nGraph集合对象\n功能\n调用方法\n\n\n\n\nmlr_graphs_branch\n创建多个分支路径的Graph\nppl(“branch”, …)或pipeline_branch(…)\n\n\nmlr_graphs_bagging\n创建执行bagging集成学习的Graph\nppl(“bagging”, …)或pipeline_bagging(…)\n\n\nmlr_graphs_greplicate\n创建由输入对象(Graph或PipeOp)的n个副本组成的新的Graph\nppl(“greplicate”, …)或pipeline_greplicate(…)\n\n\nmlr_graphs_ovr\n为分类任务创建执行“One vs Rest”分类的Graph\nppl(“ovr”, …)或pipeline_ovr(…)\n\n\nmlr_graphs_robustify\n为后续学习器创建一个稳定的数据预处理的Graph\nppl(“robustify”, …)或pipeline_robustify(…)\n\n\nmlr_graphs_stacking\n创建执行stacking集成学习的Graph\nppl(“stacking”, …)或pipeline_stacking(…)\n\n\nmlr_graphs_targettrafo\n创建在训练时变换目标变量并在预测时反转变换的Graph\nppl(“robustify”, …)或pipeline_robustify(…)\n\n\n\n\n\n\n\n\n将关键字或关键字列表传入ppl()或ppls()函数即可快速创建关键字对应的Graph，也可以利用pipeline_key()函数来创建相应的Graph：\n\nlibrary(mlr3pipelines)\ng1 = ppl(\"branch\", pos(c(\"pca\",\"encode\")))\ng1 = g1 %&gt;&gt;% po(\"learner\", lrn(\"classif.svm\"), id = \"svm\")\ng1$plot(horizontal = T)\n\n\n\n\n\n\n\n\n同样可以用pipeline_branch()函数来构建：\n\nlibrary(mlr3pipelines)\ng2 = pipeline_branch(list(po(\"pca\"), po(\"encode\")))\ng2 = g2 %&gt;&gt;% po(lrn(\"classif.svm\"), id = \"svm\")\n\n注意，此时不能使用g2 = pipeline_branch(pos(c(\"pca\",\"encode\")))。\nmlr_graphs_robustify图对象包含了常用的数据预处理方法(PipeOp对象)，包括恒定特征删除、特征类型转换、缺失值填补、特征编码等。\n\ng2 = ppl(\"robustify\")\ng2$plot()\n\n\n\n\n\n\n\n\n\n\n5.6.2 图学习器\nGraph对象可以非常简单地通过as_learner()函数转换为GraphLearner对象，也可将Graph对象作为参数传给GraphLearner对象的$new()方法来转换成图学习器，然后就可以如Learner对象一样来调优、训练和预测。\n\n例 5.4 基于mlr3内置任务penguins和classif.rpart学习器，比较增加robustify预处理流程对预测性能的影响。\n\n创建学习器、任务及分割数据集：\n\nlibrary(mlr3verse)\nlibrary(mlr3pipelines)\n\nlr = lrn(\"classif.rpart\", id = \"rpart\")  # 普通学习器模型\ngr = ppl(\"robustify\") %&gt;&gt;% lr  # 图学习器\ngr$keep_results = T\nglr = as_learner(gr)\n# glr = GraphLearner$new(gr)\n\ntask = tsk(\"penguins\")\n\nset.seed(2023)\nsplit = partition(task)\n\n训练图学习器和普通学习器模型：\n\nglr$train(task, row_ids = split$train)\nlr$train(task, row_ids = split$train)\n\n计算图学习器模型预测集性能：\n\nglr$predict(task, split$test)$score(msr(\"classif.acc\"))\n\nclassif.acc \n  0.9646018 \n\n\n计算普通学习器模型预测集性能：\n\nlr$predict(task, split$test)$score(msr(\"classif.acc\"))\n\nclassif.acc \n  0.9557522 \n\n\n图学习器的超参数优化和普通学习器对象一样，可以利用tune()生成调优实例对象，也可以利用auto_tuner()封装成AutoTuner对象。需要注意的是，图学习器的超参数不仅仅包括其中学习器对象的超参数，还包括封装在其内的管道操作对象的超参数。与学习器的超参数id相比，图学习器的超参数id增加了超参数所属对象的前缀，例如id为pca的PipeOp对象的超参数rank.，其在图学习器中的id为pca.rank.。图学习器中各对象的超参数调优范围可在创建各对象时设置(此时超参数id无需添加对象id作为前缀)，也可在封装为图学习器后设置(此时超参数id需要添加对象id作为前缀)，建议在创建各组件对象时设置超参数的调优范围。\n\n例 5.5 基于mlr3内置任务sonar，创建封装pca管道操作和classif.kknn学习器的图学习器，比较执行HPO对其泛化性能的影响。\n\n创建图学习器：\n\nlibrary(mlr3verse)\nlibrary(mlr3pipelines)\n\npo_pca = po(\"pca\", rank. = to_tune(2, 30))\nlr_kknn = lrn(\"classif.kknn\", k = to_tune(1, 35))\nglr = as_learner(po_pca %&gt;&gt;% lr_kknn)\n\n查看图学习器的所有超参数：\n\nglr$param_set\n\n&lt;ParamSetCollection&gt;\n                          id    class lower upper nlevels       default\n                      &lt;char&gt;   &lt;char&gt; &lt;num&gt; &lt;num&gt;   &lt;num&gt;        &lt;list&gt;\n 1:               pca.center ParamLgl    NA    NA       2          TRUE\n 2:               pca.scale. ParamLgl    NA    NA       2         FALSE\n 3:                pca.rank. ParamInt     1   Inf     Inf              \n 4:       pca.affect_columns ParamUty    NA    NA     Inf &lt;Selector[1]&gt;\n 5:           classif.kknn.k ParamInt     1   Inf     Inf             7\n 6:    classif.kknn.distance ParamDbl     0   Inf     Inf             2\n 7:      classif.kknn.kernel ParamFct    NA    NA      10       optimal\n 8:       classif.kknn.scale ParamLgl    NA    NA       2          TRUE\n 9:     classif.kknn.ykernel ParamUty    NA    NA     Inf              \n10: classif.kknn.store_model ParamLgl    NA    NA       2         FALSE\n                  value\n                 &lt;list&gt;\n 1:                    \n 2:                    \n 3: &lt;RangeTuneToken[2]&gt;\n 4:                    \n 5: &lt;RangeTuneToken[2]&gt;\n 6:                    \n 7:                    \n 8:                    \n 9:                    \n10:                    \n\n\n查看图学习器的超参数调优范围：\n\nglr$param_set$values\n\n$pca.rank.\nTuning over:\nrange [2, 30]\n\n\n$classif.kknn.k\nTuning over:\nrange [1, 35]\n\n\n创建自动调优图学习器和非调优图学习器：\n\nglr_tu = auto_tuner(\n  tuner = tnr(\"random_search\"), \n  learner = glr,\n  resampling = rsmp(\"cv\", folds = 5), \n  terminator = trm(\"evals\", n_evals = 20))\n\nglr_untu = as_learner(po(\"pca\") %&gt;&gt;% lrn(\"classif.kknn\"))\n\n创建基准测试方案：\n\ndesign = benchmark_grid(tsk(\"sonar\"), c(glr_tu, glr_untu),\n  rsmp(\"cv\", folds = 5))\n\n并行执行基准测试：\n\nlgr::get_logger(\"mlr3\")$set_threshold(\"warn\")\nlgr::get_logger(\"bbotk\")$set_threshold(\"warn\")\n\nfuture::plan(\"multisession\", workers = 8)\nbenchmark(design)$aggregate()[, .(learner_id, classif.ce)]\n\n               learner_id classif.ce\n                   &lt;char&gt;      &lt;num&gt;\n1: pca.classif.kknn.tuned  0.1734030\n2:       pca.classif.kknn  0.2304297\n\nfuture::plan(\"sequential\")\n\n结果表明，采取HPO后，分类错误率降低，泛化性能提高。",
    "crumbs": [
      "进阶知识",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>R语言机器学习建模</span>"
    ]
  },
  {
    "objectID": "CH5.html#常用有监督机器学习算法",
    "href": "CH5.html#常用有监督机器学习算法",
    "title": "5  R语言机器学习建模",
    "section": "5.7 常用有监督机器学习算法",
    "text": "5.7 常用有监督机器学习算法\n传统的有监督机器学习算法包括线性回归、逻辑回归、K最近邻(K-Nearest Neighbor，KNN)、朴素贝叶斯(Naive Bayes，NB)、支持向量机(Support Vector Machine，SVM)、浅层神经网络(Shallow Neural Networks，SNN)、决策树(Decision Tree，DT)以及基于决策树的集成学习(Ensemble Learning)算法，如随机森林(Random Forest，RF)、梯度提升决策树(Gradient Boosting Decision Tree，GBDT)、极致梯度提升机(eXtreme Gradient Boosting Machine，XGBoost)、轻量级梯度提升机(Light Gradient Boosting Machine，lightGBM)等。下面简介KNN、NB、SVM和基于决策树的分类回归树(CART)算法，重点介绍集成学习方法及其R语言实现。\n\n5.7.1 KNN算法\nKNN是一种简单但实用的算法，能用于分类和回归问题，也可用于缺失值填补。KNN算法原理很基于距离测度，将所有训练集数据储存到内存中，对输入的预测样例，根据某种距离测度(常用欧氏距离)从训练集数据中找出该预测样例最近的k个“邻居”，对分类问题，该预测样例的对应类别由k个“邻居”的类别按“少数服从多数”的原则来确定，对回归问题，则该预测样例的输出值是k个“邻居”输出值的平均值或加权平均值。\n\n\n\n\n\n\n\n\n图 5.19: KNN算法原理图\n\n\n\n\n\nKNN基于示例学习，是一个“懒惰”的算法，因为该算法严格意义上没有训练模型的过程，计算都发生在预测阶段。该算法的重要参数是k，其次是距离算法的选择。其中k越小，模型复杂度越高，偏差越小，方差越大；k越大，模型复杂度越低，偏差越大，方差越小。距离可以选择欧氏距离(Euclidean distance)、闵氏距离(Minkowski Distance)、曼哈顿距离(Manhattan distance)、切比雪夫距离(Chebyshev Distance)等。\nKNN算法的优点是原理简单，非常有效，易于理解和实现。KNN算法的缺点是效率低下，大规模数据的预测计算量和存储空间占用都非常大；高度依赖数据，容错性差，如果某些训练数据误差较大，则预测误差很大；存在“维数灾难”问题，随着数据维数的增加，所有距离都集中在很小的范围内，算法就可能失效。\nR中实现KNN算法的包有很多，mlr3支持kknn包实现的KNN算法。在mlr3中创建kknn学习器：\n\nlrn(\"classif.kknn\")  # 创建kknn分类学习器\nlrn(\"regr.kknn\")     # 创建kknn回归学习器\n\n\n\n5.7.2 SVM算法\nSVM是基于统计学习理论的经典机器学习算法，理论体系完备，在上世纪90年代中后期至本世纪初，曾经占据了机器学习的半壁江山。SVM可以解决分类和回归问题。用于分类问题时，其学习策略是分类间隔最大化，从而在经验风险最小化和结构风险最小化之间取得权衡，对于非线性分类问题，SVM通过核函数将特征投影到高维空间，在核空间利用超平面实现分类；构建超平面用到的数据点称为支持向量。对于回归问题，通过引入\\epsilon-不敏感损失函数，尽可能将更多的训练数据样例拟合到间隔(间隔的宽度由\\epsilon决定)中，减少远离间隔的数据样例。\n\n\n\n\n\n\n\n\n图 5.20: SVM算法原理图\n\n\n\n\n\nSVM的优点是原理简单，性能稳定，特别适合于小样本学习问题，理论上能够收敛到全局最小，没有陷入局部最小陷阱的风险。SVM的缺点是大样本学习的时空成本都很大，且性能在新的算法面前没有优势；对缺失数据敏感；对超参数的选择非常敏感。\nmlr3支持e1071、kernlab和LiblineaR三个R包装封装的SVM算法，但在对数据类型的支持上有所差异。\n在mlr3中创建基于e1071包的SVM学习器：\n\nlrn(\"classif.svm\")    # SVM分类学习器\nlrn(\"regr.svm\")       # SVM回归学习器\n\n在mlr3中创建基于kernlab包的SVM学习器：\n\n\nlrn(\"classif.ksvm\")     # KSVM分类学习器\nlrn(\"classif.lssvm\")    # LSSVM(最小二乘SVM)分类学习器\nlrn(\"regr.ksvm\")        # KSVM回归学习器\n\n在mlr3中创建基于LiblineaR包的SVM学习器：\n\nlrn(\"classif.liblinear\")    # 分类学习器\nlrn(\"regr.liblinear\")       # 分类学习器\n\n\n\n5.7.3 CART算法\n分类与回归树算法CART属于决策树(Decision Tree)中的二叉树(Binary Tree)，是一种应用广泛的决策树算法，主要用于各种分类和回归任务，也能用于特征选择。 通过节点分裂来建树是所有决策树算法的关键步骤。对于分类任务，CART算法从根节点开始根据基尼指数(Gini Index)最小化原则选择某个特征向下分裂成两个子节点，如此递归迭代，每个节点不断向下二分，直至触发预先设定的分裂终止条件，最终形成一棵庞大的二叉树。树中不能向下分裂的节点称为叶节点。基尼指数的计算公式如下：\n\n  Gini=\\sum_{k=1}^{K} p_k (1-p_k )=1-\\sum_{k=1}^{K} p_k^2  \n\\tag{5.6}\n上式中p_k为类k(k \\in [1, K])的概率。基尼指数越大说明数据集中包含的类别越多，纯度越低，越小说明数据集中包含的类别越少，纯度越高，这与熵(Entropy)的概念有类似之处。\n对于回归任务，CART算法从根节点开始根据方差(或其他拟合误差的测度)最小化原则选择某个特征进行递归二叉分裂。如果不对分裂进行控制，CART会建立一个庞大的树，对每一个样例都进行精细分类或拟合，这就会造成过拟合问题。因此，决策树算法一般都有一个超参数来设定剪枝(Pruning)操作，以修剪冗余的节点，以避免过拟合现象。\n\n\n\n\n\n\n\n\n图 5.21: CART算法原理图\n\n\n\n\n\nCART的优点是能直接处理连续型变量，能够用于回归任务；可解释性强；对缺失值不敏感等。CART的缺点是容易过拟合(通过剪枝可以一定程度克服)。\nrpart是R中实现CART等经典决策树算法的包。R包C50、RWeka、partykit和dbarts提供了其他的基于决策树的算法。mlr3对以上R包都提供了部分和全部的支持。\n在mlr3中创建基于rpart包的CART学习器：\n\nlrn(\"classif.rpart\")    # 分类树学习器\nlrn(\"regr.rpart\")       # 回归树学习器\n\n\n\n5.7.4 NB算法\nNB算法基于贝叶斯定理，是一种易于实现且非常高效的分类算法，具有坚实的理论基础。NB算法假定类别中的特定特征彼此独立，这一点违背很多常识，但并不影响该算法的分类准确性。\n贝叶斯公式如下所示：\n\n  P(B│A)=\\frac{P(A│B)P(B)}{P(A)}  \n\\tag{5.7}\n该公式涉及到联合概率、条件概率、先验概率、后验概率等知识。将贝叶斯公式换一个表达形式，就是朴素贝叶斯分类算法的原理：\n\n  P(类别│特征)=\\frac{P(特征│类别)P(类别)} {P(特征)}\n\\tag{5.8} 对于一个二分类问题，假设X是特征空间，标签y = +1或-1，对于给定的一个特征样例x_0(x_0 \\in X)，预测其对应的类别，NB算法只需计算两个概率：P(y=+1│X=x_0 )和P(y=-1│X=x_0 )，然后将x_0的类别判定为其中概率最大的那一类。\n\n\n\n\n\n\n\n\n图 5.22: NB分类算法步骤\n\n\n\n\n\nNB算法的优点是具有坚实的理论基础，算法简单，易于实现，计算速度快，存储资源占用少，在分类变量上的表现特别好，在满足特征独立条件下，对小样本的分类性能优于其他分类算法。NB算法的缺点可能是其特征独立的假设在现实世界中很难满足，当样本特征时间存在关联性时，预测效果不佳；对输入数据的表达形式敏感，对数值型变量不友好等。\nR包e1071中的naiveBayes()函数和klaR中的NaiveBayes()函数都封装了NB算法，但mlr3目前只支持e1071包中的算法。\n\nlrn(\"classif.naive_bayes\")    # 创建朴素贝叶斯分类学习器\n\n贝叶斯回归树(BART)模型是贝叶斯理论与回归树的结合，R包dbarts封装了该算法。\n\nlrn(\"regr.dbart\")   # 创建贝叶斯回归学习器\n\n\n\n5.7.5 集成学习方法\n集成学习(Ensemble Learning)是一类旨在整合多个学习器(称为个体学习器或基学习器)来提高预测性能的机器学习建模技巧。如果基学习器类型相同，则称为同质集成，反之则称为异质集成。集成学习方法的类型主要分为Bagging、Boosting和Stacking。\n\n5.7.5.1 Bagging集成学习方法\nBagging方法通过并行方式生成同质的基学习器，每个基学习器的训练数据都是训练集的一个自举抽样子集，即采用不同分布的训练子集来训练基学习器，基学习器之间彼此独立，没有强依赖关系，最终预测结果由所有基学习器共同决定：对于分类，通常采用少数服从多数的投票方法；对于回归，通常采用平均方法或加权平均方法。随机森林算法是bagging方法的典型代表，在为基学习器随机抽取等量自举样本的同时，对特征也进行随机抽样。\n\n\n\n\n\n\n\n\n图 5.23: Bagging集成学习示意图\n\n\n\n\n\n随机森林算法是应用极为广泛、性能优异且稳健的机器学习方法，可以解决回归、分类等诸多类型的任务，同时还能计算特征重要性。学习数据量较小的有监督机器学习建模均可以从随机森林算法开始或以此为基准算法。\nmlr3支持R包randomForest、randomForestSRC和ranger封装的随机森林算法，其中前者不支持缺失值，后者计算速度更快。在mlr3中创建随机森林学习器：\n\n# 基于randomForest包\nlrn(\"classif.randomForest\")\nlrn(\"regr.randomForest\")\n\n# 基于randomForestSRC包\nlrn(\"classif.rfsrc\")\nlrn(\"regr.rfsrc\")\n\n# 基于ranger包\nlrn(\"classif.ranger\")\nlrn(\"regr.ranger\")\n\n\n例 5.6 基于mlr3内置的sonar分类任务，比较单一决策树模型rpart和基于Bagging集成学习方法的随机森林模型ranger的预测性能差异。\n\n\nlibrary(mlr3verse)\nlibrary(mlr3tuningspaces)\n\ntask = tsk(\"sonar\")\n\nset.seed(2023)\nsplit = partition(task)\n\nat1 = auto_tuner(\n  learner = lrn(\"classif.rpart\"),\n  tuner = tnr(\"random_search\"),\n  resampling = rsmp(\"cv\", folds = 5),\n  terminator = trm(\"evals\", n_evals = 20),\n  search_space = lts(\"classif.rpart.rbv2\"),\n  store_models = T)\n\nat2 = auto_tuner(\n  learner = lrn(\"classif.ranger\"),\n  tuner = tnr(\"random_search\"),\n  resampling = rsmp(\"cv\", folds = 5),\n  terminator = trm(\"evals\", n_evals = 20),\n  search_space = lts(\"classif.ranger.rbv2\"),\n  store_models = T)\n\n\nlgr::get_logger(\"mlr3\")$set_threshold(\"warn\")\nlgr::get_logger(\"bbotk\")$set_threshold(\"warn\")\n\nfuture::plan(\"multisession\")\npred1 = at1$train(task,row_ids = split$train)$predict(task,row_ids = split$test)\npred2 = at2$train(task,row_ids = split$train)$predict(task,row_ids = split$test)\nfuture::plan(\"sequential\")\n\n\npred1$score()\n\nclassif.ce \n 0.3043478 \n\npred2$score()\n\nclassif.ce \n  0.173913 \n\n\n结果表明，在训练集和测试集一致的情况下，基于Bagging集成学习方法的ranger模型的分类错误率远小于单一决策树模型rpart。如果需要比较二者的泛化性能，可通过benchmark()函数执行嵌套重抽样策略来估计和比较。\n\n\n5.7.5.2 Boosting集成学习方法\nBoosting方法通过串行迭代的方式生成同质的基学习器，第一个基学习器之后生产的每个基学习器都针对前一个基学习器的预测误差进行学习，从而有效的降低模型的偏差，基学习器之间存在强依赖关系，模型最终预测结果是所有基学习器输出结果的汇总(如求和或加权求和)。目前流行的基于Boosting方法的典型算法有XGBoost、lightGBM和CatBoost算法，此三者都是基于GBDT改进而来，其中lightGBM和CatBoost支持GPU训练，lightGBM训练速度最快、内存消耗最小，CatBoost其次，XGBoost最差。lightGBM和XGBoost能够自动处理缺失值，但CatBoost不支持自动处理缺失值。CatBoost支持类别特征，而前二者需要转换为数值特征或独热编码。\n\n\n\n\n\n\n\n\n图 5.24: Boosting集成学习示意图\n\n\n\n\n\nmlr3支持R包catboost、lightgbm和xgboost分别封装的CatBoost、lightGBM和XGBoost算法，其中catboost包需要从github安装，而lightgbm和xgboost均可从CRAN安装。\n继续对 例 5.6 建立一个xgboost自动调优模型：\n\nat3 = auto_tuner(\n  learner = lrn(\"classif.xgboost\"),\n  tuner = tnr(\"random_search\"),\n  resampling = rsmp(\"cv\", folds = 5),\n  terminator = trm(\"evals\", n_evals = 20),\n  search_space = lts(\"classif.xgboost.rbv2\"))\n\n\nlgr::get_logger(\"mlr3\")$set_threshold(\"warn\")\nlgr::get_logger(\"bbotk\")$set_threshold(\"warn\")\n\nfuture::plan(\"multisession\")\npred3 = at3$train(task,row_ids = split$train)$predict(task,row_ids = split$test)\nfuture::plan(\"sequential\")\n\n\npred3$score()\n\n\n## classif.ce \n##  0.1449275 \n\n结果表明xgboost模型的分类错误率低于随机森林模型。\n\n\n5.7.5.3 Stacking集成学习方法\nStacking方法通过并行方式生成异质的基学习器构成一级学习器，基学习器的输出组合为元学习器(二级学习器)的输入特征，元学习器做出最终的预测结果。通常，不同算法从训练数据中可能学习到不同的特征，从而得出的预测结果存在较大偏差，再经过元学习器的学习就可以得到稳健的预测结果。为了避免过拟合，一级学习器一般选择复杂的算法，而二级学习器一般选择简单的算法。对一级学习器的训练集采用自举抽样子集或分配交叉验证子集可以更好地克服过拟合现象。\n\n\n\n\n\n\n\n\n图 5.25: Stacking集成学习示意图\n\n\n\n\n\nmlr3中快速搭建Stacking集成学习架构的方式是通过ppl()或pipeline_stacking()函数调用mlr_graphs_stacking，也可以利用gunion()与po(\"featureunion\")手动搭建Stacking集成学习架构。\n对 例 5.6 手动建立一个基于Stacking方法的自动调优模型：\n\nlr_kknn = lrn(\"classif.kknn\", predict_type = \"prob\", id = \"kknn\")\nlr_kknn$param_set$set_values(.values = lts(\"classif.kknn.rbv2\")$values)\npo_kknn = po(\"learner_cv\", learner = lr_kknn, \n             resampling.folds = 3, id = \"blr_kknn\")\n\nlr_svm = lrn(\"classif.svm\", predict_type = \"prob\", id = \"svm\", type = \"C-classification\")\nlr_svm$param_set$set_values(.values = lts(\"classif.svm.rbv2\")$values)\npo_svm = po(\"learner_cv\", learner = lr_svm, \n             resampling.folds = 3, id = \"blr_svm\")\n\nlr_rpart = lrn(\"classif.rpart\", predict_type = \"prob\", id = \"rpart\")\nlr_rpart$param_set$set_values(.values = lts(\"classif.rpart.rbv2\")$values)\npo_rpart = po(\"learner_cv\", learner = lr_rpart, \n             resampling.folds = 3, id = \"blr_rpart\")\n\nlr_glm = lrn(\"classif.glmnet\", predict_type = \"prob\", id = \"glmnet\")\nlr_glm$param_set$set_values(.values = lts(\"classif.glmnet.rbv2\")$values)\npo_glm = po(\"learner\", learner = lr_glm, id = \"slr_glm\")\n\ngr_base = gunion(list(po_kknn, po_svm, po_rpart))\ngr_stack = gr_base %&gt;&gt;% po(\"featureunion\") %&gt;&gt;% po_glm \ngr_stack$plot(html = T)\n\n\n\n\n\n\nglr_stack = as_learner(gr_stack)\nat4 = auto_tuner(\n  learner = glr_stack,\n  tuner = tnr(\"random_search\"),\n  resampling = rsmp(\"cv\", folds = 5),\n  terminator = trm(\"evals\", n_evals = 20))\n\n\nlgr::get_logger(\"mlr3\")$set_threshold(\"warn\")\nlgr::get_logger(\"bbotk\")$set_threshold(\"warn\")\n\nfuture::plan(\"multisession\")\npred4 = at4$train(task,row_ids = split$train)$predict(task,row_ids = split$test)\nfuture::plan(\"sequential\")\n\n\npred4$score()\n\nclassif.ce \n0.05288462 \n\n\n手动构建的以kknn、svm和rpart为一级学习器、glmnet为二级学习器的Stacking集成学习模型，在同样的训练集和预测集情况下，分类错误率进一步降低。\n下面以ppl()函数调用mlr_graphs_stacking来构建Stacking模型，并设置将原始数据传给二级学习器：\n\ngrppl_stack = ppl(\"stacking\", \n               base_learners = list(lr_kknn, lr_svm, lr_rpart),\n               super_learner = lr_glm,\n               method = \"cv\",\n               folds = 3,\n               use_features = T)\ngrppl_stack$plot(html = T)\n\n\n\n\n\n\nglrppl_stack = as_learner(grppl_stack)\nat5 = auto_tuner(\n  learner = glrppl_stack,\n  tuner = tnr(\"random_search\"),\n  resampling = rsmp(\"cv\", folds = 5),\n  terminator = trm(\"evals\", n_evals = 20))\n\n\nlgr::get_logger(\"mlr3\")$set_threshold(\"warn\")\nlgr::get_logger(\"bbotk\")$set_threshold(\"warn\")\n\nfuture::plan(\"multisession\")\npred5 = at5$train(task,row_ids = split$train)$predict(task,row_ids = split$test)\nfuture::plan(\"sequential\")\n\n\npred5$score()\n\n classif.ce \n0.009615385 \n\n\n在采用ppl()函数构建的Stacking模型时，设置了参数use_features = T，目的是将原始特征数据传给二级学习器，这与手动搭建的Stacking模型有所区别。结果表明，这种变化带来了更低的分类错误率。\n比较以上建立的单个决策树模型和不同类型的集成学习模型，可以发现集成学习在提高预测性能上具有极大的潜力。在解决各种机器学习任务时，应积极考虑采用集成学习方法。\n\n练习 5.3 从https://archive.ics.uci.edu/dataset/733/water+quality+prediction-1下载水质数据，基于mlr3框架，从RF、XGBoost和lightGBM三个模型筛选最优模型，并对最优模型进行关键超参数的优化，最后给出最优模型的性能评估。在Markdown或Quarto文档中完成整个建模过程，包括建模文档和代码。\n\n\n练习 5.4 基于 练习 5.3 的数据集和模型比选结果，采用图学习器建立最优模型，要考虑采用特征预处理方法以及超参数优化。在Markdown或Quarto文档中完成整个建模过程，包括建模文档和代码。\n\n\n练习 5.5 5-3：基于 练习 5.3 的数据集并参考 例 5.6 ，尝试采用Stacking集成学习方法建立不少于3个不同基模型的集成模型。在Markdown或Quarto文档中完成整个建模过程，包括建模文档和代码。",
    "crumbs": [
      "进阶知识",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>R语言机器学习建模</span>"
    ]
  },
  {
    "objectID": "CH6.html",
    "href": "CH6.html",
    "title": "6  R语言深度学习建模",
    "section": "",
    "text": "6.1 深度学习概述",
    "crumbs": [
      "高阶知识",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>R语言深度学习建模</span>"
    ]
  },
  {
    "objectID": "CH6.html#深度学习概述",
    "href": "CH6.html#深度学习概述",
    "title": "6  R语言深度学习建模",
    "section": "",
    "text": "6.1.1 深度学习概念及其特点\n深度学习是一种基于人工神经网络（ANN）架构对数据进行表征学习（Representation Learning）的算法，是机器学习的一个重要分支。传统机器学习实现了特征到输出的映射，但深度学习则从原始数据自动学习特征表示，构建层次化的概念体系，让计算机从简单概念来学习复杂概念。这不仅仅是简单的特征工程自动化，因为深度学习模型自动提取的特征往往比手动提取的特征表现更好。与传统机器学习相比，深度学习具有如下特点：\n（1）特征工程自动化。深度学习直接利用不同架构的神经网络从原始数据提取深层次的特征表示，不再需要手动特征工程。这个特点对于大数据特别是影像、音频、文本等非结构性数据的特征工程而言，其进步是不言而喻的。\n（2）应用可扩展。深度学习模型能够通过增量学习转换到同一领域的不同应用，而传统机器学习模型则具有很大的局限性。例如，预训练的宠物识别深度神经网络模型可以非常容易地应用于野生动物的识别。\n（3）成本昂贵。深度学习是大数据和高算力驱动发展的，大数据的获取成本很高，高算力的专用硬件（高级GPU）也非常昂贵，而且深度学习模型的训练往往需要很长的时间，LLM的训练时间动辄数月时间，期间的能耗也极为惊人。据研究测算，大语言模型GPT-3（1750亿个参数）一次训练的电力消耗近19万kW·h，同时产生与汽车行驶70万公里相当的二氧化碳排放量。\n抽象和形式化的任务对人类来而言往往具有较高的难度，例如可以用公式表达的数学问题，但计算机很容易解决。那些难以形式化描述但人类可以凭直觉轻松执行的任务，如图像分类、语音识别、语言沟通等，计算机却难以解决，但深度学习在这方面正在赶超人类。\n\n\n6.1.2 人工神经网络基础\n\n6.1.2.1 神经元数学模型和神经元层\n人工神经网络是一种模仿生物神经元（ 图 6.1 左图）运行机制的仿生智能算法，其构成的基础单元是人工神经元（Neuron，也称节点，示意图见 图 6.1 右图）。人工神经元是对生物神经元功能的模仿，其输入相当于生物神经元的树突，输出相当于生物神经元的轴突（轴突的信号可以通过突触或称轴突末梢传递给后续连接的神经元），对输入的处理（包括激活函数，Activation Function）相当于生物神经元的细胞核。\n\n\n\n\n\n\n\n\n图 6.1: 生物神经元和人工神经元模型\n\n\n\n\n\n神经元是神经网络中的基础计算节点，对每一个输入特征都对应一个权重（weights），表示连接强度，控制信息的流动。每个神经元一般还设置一个小的常数项，即偏置（bias），相当于是输入特征恒定为1的权重。偏置的意义在于避免神经元的加权和为0的情况。所有输入特征的加权和与偏置的总和作为激活函数的输入，激活函数的输出即为神经元的输出。正是权重和偏置让神经网络拥有了强大的表达能力和学习能力。\nFrank Rosenblatt发明的基于人工神经元（MP模型）的感知器是世界上第一个人工神经网络，其激活函数是一个阶跃函数，能够实现简单的线性二分类。在二维平面上，感知器是一条直线，不能划分异或区间，因此无法解决异或问题。让神经网络具有强大的功能，就需要采用更多神经元。在复杂神经网络中，神经元按层组织，即神经元层（Layer），层内所有神经元具有相同的激活函数。最典型的神经网络是前馈式全连接网络（ 图 6.2 ），即数据流从输入层（输入层不是由神经元构成，只是第一个隐层的输入）到隐层（可以有多个）再到输出层。神经网络由神经元层逐层搭建而成，通常将隐层数≤2的神经网络归为浅层神经网络，而将隐层数≥3的神经网络归为深度神经网络（ 图 6.2 右图）。\n\n\n\n\n\n\n\n\n图 6.2: 典型前馈式全连接网络结构示意图\n\n\n\n\n\n\n\n6.1.2.2 激活函数\n在多层神经网络中，激活函数非常重要，如果没有激活函数，整个多层网络本质上就是一个单层网络。激活函数将神经元的所有输入加权之和（包括偏置）作为输入，经过激活函数的计算转换为神经元的最终输出。激活函数通常要求可导、单调且其导函数值域在较小区间内，大多数激活函数是非线性的。常用的激活函数有Sigmoid、Tanh、ReLU等类型，这三种激活函数及其导函数的图像如 图 6.3 所示。\n\n\n\n\n\n\n\n\n图 6.3: Sigmoid、Tanh和ReLU激活函数及其导函数的图像\n\n\n\n\n\nSigmoid和Tanh函数的缺陷是存在值域饱和区，从 图 6.3 可见，二者将较大的输入转换为接近1的输出，将较小的输入转换为接近 -1或0的输出，而对0附近狭窄区间的输入变化最敏感。这意味着，当输入值在二者的饱和区时，相应导数接近于0，这对基于梯度下降法进行训练的神经网络而言，导致了梯度消失问题。ReLU函数则能有效缓解梯度消失问题，且计算成本更低。但ReLU函数将小于0的输入转换为0，降低了神经网络恰当拟合数据或训练的能力，为此提出了一种改进的ReLU函数——Leaky ReLU函数，该函数和导函数的图像见 图 6.4 。Leaky ReLU函数对小于0的输入，引入了一个小斜率的线性函数作为激活函数，从而有效克服了ReLU函数的缺陷。ReLU具有稀疏性，只能用于隐层。相比Sigmoid和Tanh激活函数，ReLU能够更积极的打开或关闭神经元。\n\n\n\n\n\n\n\n\n图 6.4: Leaky_ReLU激活函数及其导函数的图像\n\n\n\n\n\n\n\n6.1.2.3 权重更新与梯度下降法\n在前馈神经网络中，数据流从输入层开始逐层处理并传向输出层的过程称为正向传播，然后将网络输出标签与实际标签的损失函数构造为目标函数（一般都是凸函数），反向逐层求出目标函数对各神经元权重的偏导数（链式法则），构成目标函数对权重的梯度，并根据梯度逐层更新权重，这是网络训练过程中最重要的步骤，即反向传播（Back Propagation）。当网络输出标签与实际标签的偏差达到期望值时，结束网络训练。\n神经网络反向传播的过程，是利用梯度更新网络权重的过程，本质上是对目标函数的优化，捕获全局最优（通常都是寻找目标函数的最小值）。在深度神经网络的训练中，最常用的优化算法是梯度下降法( 图 6.5 )。梯度是一个向量，表示函数在某点处的方向导数沿着该方向取得最大值，即函数在该点处沿着该方向（即该梯度的方向）变化率（即该梯度的模）最大。对于一元线性函数，梯度就是斜率，对于多元函数，梯度由各个自变量的偏导数组成。\n\n\n\n\n\n\n\n\n图 6.5: 梯度下降法示意图\n\n\n\n\n\n显然，沿着梯度负方向减小函数值，不断迭代，就能找到或接近目标函数的最小值。距离最优解越远，梯度越大，距离最优解越近，梯度越小。初始位置、下降方向、下降步长是梯度下降法的三要素。在步长的计算中有一个学习率的参数，学习率太小，每次步长更新太小，迭代次数显著增多；学习率太大，则无法保证收敛到最优解。因此，选择合适的学习率极为重要，很多基于梯度下降的优化算法都聚焦于学习率的调整，例如AdaGrad算法、RMSProp算法、Adam算法等，以加快网络训练的收敛速度。每次迭代中输入的样例数量显著影响梯度下降法的收敛：每次迭代输入一个样例的随机梯度下降法（SGD），收敛速度快，但波动性大，不能保证每次迭代都朝正确方向前进；每次迭代输入全部样例的全量随机梯度下降法（Batch SGD）能保证每次迭代都朝正确方向前进，但迭代时间长，内存消耗大；而每次迭代输入多个样例的小批量随机梯度下降法（Minibatch SGD）是一个折中的算法，选择合理的Batch大小，可以让收敛速度比SGD更快、更稳定，在最优解附近跳跃小，甚至能够得到比全量随机梯度下降法更好的解。\n\n\n6.1.2.4 随机失活和批归一化\n深度神经网络容易出现过拟合和训练收敛速度慢等问题，随机失活（Dropout）和批归一化（Batch Normalization）等正则化方法可以克服这类问题。\n随机失活方法( 图 6.6 )是在网络训练阶段的前向传播过程中，以一定概率让某些神经元失活，从而克服过拟合并提高泛化性能。Dropout与传统剪枝（Prunning）方法不同，剪枝方法是按设定规则永远丢弃神经元，而Dropout只是在训练阶段暂时地随机丢弃神经元。\n\n\n\n\n\n\n\n\n图 6.6: Dropout方法示意图\n\n\n\n\n\n此外，随着网络训练中权重的更新，除了第一层的输入数据外，后面每层输入数据的分布都会随之变化，因为在训练的时候，前层训练参数的更新将导致后层输入数据分布的变化，产生所谓的“内部协变量偏移（Internal Covariate Shift）”问题，导致训练的不稳定性，如果数据分布偏移到激活函数的饱和区，网络训练就会非常慢，难以收敛。由于深度神经网络广泛采用小批量随机梯度下降法，批归一化与数据预处理中的归一化相似，但转换公式中增加了尺度参数\\gamma、平移参数\\beta和一个常数项\\varepsilon（微小正数），二者的区别如 图 6.7 所示。\n\n\n\n\n\n\n\n\n图 6.7: 归一化与批归一化示意图\n\n\n\n\n\n\n\n\n6.1.3 经典神经网络模型简介\n神经网络的基本结构根据数据流向可分为前馈神经网络（Feed Forward NN）和反馈神经网络（Feed Back NN）。在前馈神经网络中，数据从输入层开始，每层神经元接收上一层神经元的输出，并输出给下一层的神经元。在反馈神经网络中，除了与前馈神经网络一样的数据流向外，隐层神经元还能接收自己的上一次输出，具有记忆性，在不同时刻具有不同的状态。\n\n6.1.3.1 全连接神经网络FCNN\nFCNN是一种典型的前馈神经网络（ 图 6.2 ），MLP是FCNN的一个代表。在FCNN中，层内神经元互不连接，相邻层的神经元完全互相连接。FCNN的优点是具有很强的灵活性和表达能力，但也存在参数量大和容易过拟合的缺点。\n\n\n6.1.3.2 卷积神经网络CNN\nCNN利用多个卷积层来提取图像的各种局部特征。卷积层利用卷积核（本质上是一种滤波器）执行卷积运算( 图 6.8 )，不同的卷积核能够提取出不同的特征，特别是对象的边缘。CNN中的卷积和数学中的卷积是两种不同的运算。数学卷积的卷积核是预先设定的，运算前先将卷积核绕中心旋转180度，然后与矩阵对应位置相乘并求和；而CNN中的卷积核是随机初始化再经过训练或学习而得到的，运算时卷积核无需旋转，卷积核与图像（矩阵）局部区域对应位置相乘并求和，亦即以卷积核为权重对图像局部区域像素值加权求和，本质上是一种互相关（cross-correlation）运算。卷积运算根据CNN的输入维度，可以是一维，也可以是二维、三维。\n卷积核的大小（kernel size）决定其感受野。例如一个3×3的二维卷积核，一次扫描的感受野是9个像素，而一个5×5的二维卷积核，其感受野是25个像素。\n\n\n\n\n\n\n\n\n图 6.8: CNN卷积运算示意图\n\n\n\n\n\n原始图像经过卷积核运算后，图像边缘被修剪，生成的新图像变小，导致信息部分损失。为了解决这个问题，需要对原始图像边缘进行填充（填充值一般为0）操作( (fig?)=6.9 )，此即所谓的“padding”。图像高和宽两个维度的填充数根据卷积核的高和宽进行调整，以使输出和输入的图像具有相同的高度和宽度。\n\n\n\n\n\n\n\n\n图 6.9: padding操作及卷积运算可视化示意图\n\n\n\n\n\n当原始图像过大，可通过调整卷积核移动的步长（stride）来压缩部分信息。步长包括水平和垂直两个方向， 图 6.10 演示了步长在水平和垂直两个方向都为2的卷积运算示例。\n\n\n\n\n\n\n\n\n图 6.10: 卷积核移动步长为2的卷积运算示意图\n\n\n\n\n\n对卷积核进行扩张（dilation）来增大感受野，可以克服因卷积核移动步长过大而导致的图像失真问题。一个3×3的卷积核，当扩张系数为2时，其感受野会扩大为5×5，扩张后的空洞用0值填充，实际计算量并没有增加，如 图 6.11 所示。\n\n\n\n\n\n\n\n\n图 6.11: 扩张系数为2时的卷积核大小变化示意图\n\n\n\n\n\n输入CNN的图像还有一个通道（channel）的概念。灰度图像只有一个通道，而标准彩色图像有红绿蓝（即RGB）3个通道。同时，还要设定输入图像的批量大小（batch size）。因此，CNN的初始输入是一个四维数组，例如1×3×64×64，表示输入是批量大小为1的3通道64（高）×64（宽）像素的彩色图像。\n卷积运算后的结果经过激活函数的处理，维度保持不变。但一个图像采用多个卷积核执行卷积运算后，维度急剧上升，导致模型参数“爆炸”，因此需要进行降维操作。池化（pooling）运算是一种在CNN中广泛使用的降维操作。在卷积操作后进行池化，不仅能够降低卷积结果的维度，减少参数和计算量，而且能够突出最具代表性的特征，降低像素的重复性，让后续的卷积层操作更有意义。池化本质上是一种聚合运算，包括最大池化（maximum pooling）和平均池化（average pooling），如 图 6.12 所示。\n\n\n\n\n\n\n\n\n图 6.12: 池化运算示意图\n\n\n\n\n\n通常，一个完整的卷积层包括卷积运算和池化运算。CNN一般利用很多个卷积层来提取不同的特征，然后将高维输出展平（flatten）为低维结果，传给后面的全连接层或其他类型的神经元层，进行分类或回归。 图 6.13 是第一个CNN架构——LeNet-5，由Yann LeCun等于1988年发表。LeNet-5包括2个卷积层和3个全连接层，使用平均池化。2012年在ImageNet竞赛中一骑绝尘的AlexNet只是在LeNet-5的基础上堆叠了更多卷积层，由5个卷积层和3个全连接层构成，拥有6千万个参数。\n\n\n\n\n\n\n\n\n图 6.13: LeNet-5架构示意图\n\n\n\n\n\nCNN不仅仅应用于图像分类、目标检测等有影像数据，还可以拓展用于序列数据等。\n\n\n6.1.3.3 循环神经网络RNN及变体\nRNN及其变体的隐层神经元的输出能够传递给自身，从而具备处理序列数据的天赋，这特别适用于语音、视频、自然语言处理、时间序列建模等，因为序列数据的前后元素（深度学习中通常称为token，即构成序列的最小单位，也称词元）之间存在密切联系。 RNN的雏形是1982年提出的单层反馈神经网络 Hopfield Network，是，用来解决组合优化问题。此后的发展中因“梯度消失”和“梯度爆炸”问题，RNN的训练非常困难，应用十分受限。1997年出现了两个RNN的改进版本，一个是Sepp Hochreiter等提出的长短期记忆（LSTM）网络，使用门控单元及记忆机制来解决RNN的训练问题，另一个是Mike Schuster等提出的双向RNN模型（Bidirectional RNN），神经元当前输出同时考虑“上下文”，即同时利用过去和未来的信息。这两种模型拓宽了RNN的应用范围，推动了深度学习在文本、语音、视频、时间序列等法方面的应用。\n传统RNN的结构如 图 6.14 所示，其计算核心是一个采用tanh激活函数的全连接网络，但每一时刻的输入是由上一时刻自身的输出h_{t-1}与当前时刻的输入x_t合并组成。\n\n\n\n\n\n\n\n\n图 6.14: RNN的Cell结构和RNN展开示意图\n\n\n\n\n\nLSTM是一个结构更复杂的RNN变体，其结构与计算如 图 6.15 所示。LSTM计算核心包括5个FCNN，其中3个激活函数为sigmoid，2个为tanh，分别构成三个门控单元：遗忘门、输入门和输出门。此外，LSTM还增加了Cell状态，每一时刻的输入除了上一时刻自身的输出ht-1与当前时刻的输入xt的组合，还有上一时刻的Cell状态Ct-1。\n\n\n\n\n\n\n\n\n图 6.15: LSTM的Cell结构示意图(σ表示Sigmoid激活函数，下同)\n\n\n\n\n\nGRU（门控循环单元）由重置门和更新门组成，是对LSTM的一种计算简化，其结构与计算如 图 6.16 所示。\n\n\n\n\n\n\n\n\n图 6.16: GRU的Cell结构示意图\n\n\n\n\n\n双向RNN模型的隐层采用2个RNN单元（一般采用LSTM），一个正向处理输入，一个反向处理输入，二者的输出拼接后传递给输出层。图 6.17 演示了一个双向LSTM网络的结构示意图。\n\n\n\n\n\n\n\n\n图 6.17: 双向LSTM网络结构示意图\n\n\n\n\n\nRNN和双向RNN网络的隐层都可以堆叠多个RNN单元来构造更深度的网络，但通常建议不要超过3个单元，因为RNN单元在时间维度上是连接的，这意味着每增加一个时序，RNN网络随之增加一个自我拷贝，网络就会变得越来越大，对算力要求急剧增加。\n\n\n6.1.3.4 自注意力神经网络Transformer\n2017年谷歌研究团队在《Attention is all you need》一文中提出了颠覆性的Transformer模型。该模型基于自注意力（Self-attention）机制，实现了捕获长序列中的时间依赖关系与距离无关，克服了RNN和CNN在处理长序列方面的弱点，突破了RNN不能并行计算的限制。基于Transformer的预训练模型（PTM）在语言、语音、视觉和强化学习等各种任务上取得了最高水平的性能，并在大语言模型（如GPT、BERT、LLaMA、ChatGLM等）的发展中发挥了重要作用。\nTransformer模型主要由编码器（Encoder）堆栈和解码器（Decoder）堆栈组成，编码器的输入是原始输入序列通过嵌入层（Enbedding）的输出与序列元素位置编码（Position Encoding）的和，解码器的输入包括与原始输入序列对应的目标输出序列通过嵌入层的输出与序列元素位置编码的和，还包括来自编码器的输出。解码器的输出经过Linear层和Softmax层，输出目标序列。其结构示意图如 图 6.18 所示。\n\n\n\n\n\n\n\n\n图 6.18: Transformer结构示意图\n\n\n\n\n\n（1）嵌入\n在深度学习中，嵌入是用一个低维稠密向量来表示一个对象，使得这个向量能够表达相应对象的某些特征，同时向量之间的距离能反应对象之间的相似性。因此，嵌入的数学本质是以独热编码为输入的单层全连接神经网络的权重，是一种表征学习（representative learning）。嵌入能够根据需要进行降维嵌入或升维嵌入，且信息不丢失。采用相应的嵌入方法，文字、图片、语音、视频、网络图、日期时间等就可以转化为神经网络能识别、能使用的信息。\n\n\n\n\n\n\n\n\n图 6.19: 嵌入原理示意图\n\n\n\n\n\n（2）位置编码\n序列数据中词元的位置是非常重要的信息。CNN和RNN都具有捕获序列中词元位置信息的能力，但都是局部的位置关系，不能获得全局位置信息。Transformer通过位置编码来获得序列中词元的位置信息。位置编码也是一种嵌入方法，是位置信息的表示学习。位置编码可分为绝对位置编码、相对位置编码和旋转位置编码。绝对位置编码是将位置信息转换为向量，在与嵌入向量合并（相加或拼接）。相对位置编码在计算注意力分数时加入可学习的位置参数。旋转位置编码通过在注意力计算中引入一个旋转操作来实现位置编码，即将词元向量在复数空间中进行旋转，旋转的角度由位置决定，从而直接编码了位置信息的方向。与传统的绝对位置编码和相对位置编码相比，旋转位置编码具有更好的外推性，并在大语言模型中得到广泛应用。\n根据位置向量是否随网络训练而改变，可分为固定位置编码和可学习位置编码。固定位置编码可以视为一种数据预处理方法，其中最经典的方法是采用正弦和余弦函数对词元绝对位置进行固定编码，生成与词元嵌入向量大小维度相同的位置编码向量，即对于输入矩阵中第i行、第2j列和2j+1列上的词元，用下列公式分别计算位置编码：\n\n  PE_{pos,2i} = \\text{sin}\\frac{pos}{10000^{2i/d}}\\\\\n  PE_{pos,2i+1} = \\text{cos}\\frac{pos}{10000^{2i/d}}\\\\\n\\tag{6.1}\n上式中PE是位置编码向量，pos是词元在序列中的位置序号，d是PE的维度，与嵌入向量一致；i \\in [0,d/2-1]。\n（3）编码器与自注意力\n编码器采用自注意力（self-attention）机制计算输入序列的注意力分数，编码器的计算结构如 图 6.20 所示，其中最关键的是多头自注意力（MHA）模块。\n\n\n\n\n\n\n\n\n图 6.20: 编码器结构示意图\n\n\n\n\n\n在对人类注意力的研究中发现，影响注意力的因素包括自主性提示和非自主性提示，前者代表人的主观意志，而后者是对象本身的特征（如颜色、大小等）对注意力的影响。Transformer中的缩放点积注意力算法借鉴了人类注意力的研究结果，用Q（Query，查询）代表自主性提示强弱，用K（Key，键）代表非自主性提示的强弱，而V（Vlaue，值）则是与K对应的注意力，而Q的介入会影响与K对应的V。为了获得更多子空间的信息，采用多个并行的自注意力计算模块，这就是所谓的“多头自注意力”，其计算流程如 图 6.21 所示。\n\n\n\n\n\n\n\n\n图 6.21: 多头自注意力模块的计算示意图\n\n\n\n\n\n在深度学习的研究中，已经有多种注意力机制被提出，其中点积自注意力是最初版本的Transformer中所采用的注意力机制。\n（4）解码器与掩码自注意力\n解码器与编码器略有不同。解码器模块包含2个多头自注意力模块，掩码多头自注意力（MMHA）模块和编码-解码多头自注意力（EDMHA）模块，其结构如图 图 6.22 所示。\n\n\n\n\n\n\n\n\n图 6.22: 解码器结构示意图\n\n\n\n\n\n为了避免解码器看到未来的输出，需要将目标序列“右移”（right shift）一位，再进行嵌入、位置编码并相加，作为解码器中MHA模块的输入，如 图 6.23 所示。\n\n\n\n\n\n\n\n\n图 6.23: 目标序列与解码器输入序列的右移关系\n\n\n\n\n\nMMHA模块计算Q、K、V与编码器中MHA模块基本相同，区别是增加了对Q 与K^T 的乘积进行掩码操作，即与掩码算子按位相乘，如 图 6.24 所示。\n\n\n\n\n\n\n\n\n图 6.24: 掩码操作示意图\n\n\n\n\n\n掩码操作完成后，在进行缩放和Softmax运算，然后与V相乘，得到掩码自注意力分数矩阵。将多个并联的掩码自注意力模块输出的注意力分数矩阵进行拼接，然后经过一个线性神经元层，就得到MMHA模块的最终输出。整个计算过程如 图 6.25 所示。\nMMHA模块的输出经过求和与归一化层（LayerNorm，与BatchNorm不同）后，传给模块。完成矩阵输入掩码自注意力模块计算注意力分数，经过求和与归一化后，输入EDMHA模块，以计算Q。同时，编EDMHA模块接收来自编码器的输出，以计算K和V，后续的计算流程与编码器中MHA模块相同。\n\n\n\n\n\n\n\n\n图 6.25: 掩码多头自注意力模块的计算流程示意图\n\n\n\n\n\n深度学习仍在快速发展，创新的深度神经网络模型不断被提出并得到日益完善，极大地推动着人工智能应用到人类生产、生活的各个领域。",
    "crumbs": [
      "高阶知识",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>R语言深度学习建模</span>"
    ]
  },
  {
    "objectID": "CH6.html#基于torch的深度学习建模",
    "href": "CH6.html#基于torch的深度学习建模",
    "title": "6  R语言深度学习建模",
    "section": "6.2 基于torch的深度学习建模",
    "text": "6.2 基于torch的深度学习建模\n\n6.2.1 torch包的简介与安装\n先安装torch包：\n\ninstall.packages(\"torch\")\n\n安装完成后，加载torch包：\n\nlibrary(torch)\n\n第一次加载torch包，会自动执行install_torch()函数，该函数会从网上下载两个zip文件（libtorch和lantern）并安装。如果出现无法下载情况时，可以在将错误提示中的两个zip文件的网址复制到浏览器地址栏或下载软件来下载，然后用Sys.setenv()来分别设置TORCH_URL和LANTERN_URL两个环境变量，最后再运行install_torch()，即可完成torch的安装。\n\n# Sys.setenv(TORCH_URL = \"path/to/libtorch压缩包文件\")\n# 例如\nSys.setenv(TORCH_URL = \"F:/libtorch-win-shared-with-deps-2.0.1+cu117.zip\")\n# Sys.setenv(LANTERN_URL = \"path/to/lantern压缩包文件\")\n# 例如\nSys.setenv(LANTERN_URL = \"F:/lantern-0.12.0+cu117-win64.zip\")\ninstall_torch()\n\n如果计算机上安装了支持CUDA的NVIDIA公司的显卡，且安装了相应的CUDA驱动，将提示安装GPU版本的torch，否则将安装CPU版本的torch。具体安装可参考https://torch.mlverse.org/docs/articles/installation。 安装好torch后，再继续安装torchvision、torchaudio和luz包。torch包采用了基于R6的面向对象编程系统，使用$符号调用对象的方法和属性。\ntorch包是基于Python中的机器学习框架——PyTorch移植而来，不依赖Python环境。大量PyTorch的Python代码可以很容易地改写为R代码在R中独立运行，同时支持GPU加速。torch包是一个完整的深度学习框架，包括张量及各种运算、自动求导、GPU加速支持、学习数据构造和加载以及各种深度神经网络模型搭建、训练、预测和评价。torchvision包提供了与计算机视觉相关的数据集、变换方法和若干预训练的模型（如AlexNet、ResNet、VGG、Inception、MobileNet等）。torchaudio包提供了与语音相关的数据集、处理工具和若干预训练的模型。luz包提供了类似Python中keras工具包的给你，是torch的高级接口，能够显著减少深度学习建模的代码量。\n\n\n6.2.2 创建Dataset和Dataloader\n深度学习建模首先要解决数据的构造和加载问题。在torch中，dataset()函数用于灵活地创建数据对象（Dataset），可以加载本地数据，也可以下载网络数据，还可以对数据进行预处理，重要的是，它能够将数据一项接一项（一项数据通常是列表结构，由一个输入张量和一个目标张量组成，类似于数据框对象中由特征变量和标签变量组成的一行数据）地传递给它的调用者——数据加载器对象（Dataloader），这是由dataloader()函数创建的。除了dataset()外，tensor_dataset()函数可以直接将张量对象快速转换为Dataset，而torchvision包中的image_folder_dataset()可以将分类存储在文件夹中的图片转换为Dataset。\n深度学习中的张量（Tensor）是一种为快速计算而优化的多维数组，不同于数学和物理意义上的张量。torch包提供了完备的张量构造、运算及操作的各种函数。torch创建的张量也是对象，可以通过$来调用其方法和属性。torch_tensor()是最常用的构造张量的函数，torch_ones()、torch_ones_like()、torch_zeros()、torch_rand()、torch_range()等函数用于构造一些特殊的张量。张量运算和操作的函数如torch_matmul()、torch_dot()、torch_cat()等。\n\n6.2.2.1 创建Dataset\n除了直接引用已经创建好的Dataset对象外，dataset()函数还可以从头开始定制Dataset对象。创建过程很简单，只需要实现三个方法：\n（1）initialize()：包括但不限于对数据框对象、文件路径、网络下载链接URL等的引用，以及对数据的各种预处理。\n（2）.getitem(i)：逐项获取数据，传递给数据加载器对象。参数i通常都是用于确定底层数据结构中的起始位置。\n（3）.length()：通常只有一行代码，唯一目的是获取数据集中的数据项数。\n根据需要，还可以包括自定义函数，如用来在initialize()中对数据进行预处理的自定义函数等。\n通常，dataset()的使用模式如下：\n\nds = dataset()(\n  initialize = function(...) {\n    ...\n  },\n  .getitem = function(index) {\n    ...\n  },\n  .length = function() {\n    ...\n  }\n)\n\n要注意区分上面代码中的()和{}。 下面的代码将R包palmerpenguins中的penguins数据框转换为torch的Dataset对象：\n\nlibrary(torch)\nlibrary(palmerpenguins)\nlibrary(dplyr)\npenguins %&gt;% glimpse()\n\nRows: 344\nColumns: 8\n$ species           &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel…\n$ island            &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgerse…\n$ bill_length_mm    &lt;dbl&gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, …\n$ bill_depth_mm     &lt;dbl&gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, …\n$ flipper_length_mm &lt;int&gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186…\n$ body_mass_g       &lt;int&gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, …\n$ sex               &lt;fct&gt; male, female, female, NA, female, male, female, male…\n$ year              &lt;int&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…\n\n\n该数据集中有8个变量，第一列species是分类的标签变量（模型输出），其他列为特征变量（模型输入）。下面通过dataset()函数来选择bill_length_mm、bill_depth_mm、flipper_length_mm和body_mass_g四个特征变量来创建Dataset对象：\n\npgds = dataset(\n  name = \"pgds()\",\n  initialize = function(df) {\n    df = na.omit(df)    # 删除包含缺失值的行\n    self$x = torch_tensor(as.matrix(df[, 3:6]) )    # 选择特征变量并转换为张量\n    self$y = torch_tensor(    # 设定标签变量，先将因子变量转为数值变量，然后转换为张量\n      as.numeric(df$species)\n    )$to(torch_long())    # 指定张量类型\n  },\n  .getitem = function(i) {    # 获取参数i指定的数据项\n    list(x = self$x[i, ], y = self$y[i])\n  },\n  .length = function() {    # 返回数据的项数\n    dim(self$x)[1]\n  }\n)\n\n实例化:\n\nds = pgds(penguins)\nds$.length()    # 等同于length(ds)\n\n[1] 333\n\nds$.getitem(2)     # 等同于ds[2]\n\n$x\ntorch_tensor\n   39.5000\n   17.4000\n  186.0000\n 3800.0000\n[ CPUFloatType{4} ]\n\n$y\ntorch_tensor\n1\n[ CPULongType{} ]\n\n\n查看原始的数据：\n\npenguins[2, c(1,3:6)]\n\n# A tibble: 1 × 5\n  species bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;fct&gt;            &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1 Adelie            39.5          17.4               186        3800\n\n\n比较发现，dataset()对penguins数据框进行了预处理（删除包含NA的行，将因子变量转换为数值变量），然后转换成张量，最后输出的形式为一个列表，包含一组特征张量和一列标签张量。dataset_subset()函数可以根据指定的索引从Dataset对象中获取子集，可用于创建训练集、验证集和测试集。\n下面构造一个人工数据集，用于演示torch深度学习建模的步骤。\n\ndim_in = 4\nn = 5000\n# 生成n × dim_in的随机张量x，作为输入特征\nx = torch_randn(n, dim_in)    \ncoefs = c(0.3, 1.3, -2.2, -0.7)\n# 根据x生成输出标签y\ny = x$matmul(coefs)$unsqueeze(2) + torch_randn(n,1)    \n# 转换为Dataset对象\nds = tensor_dataset(x, y)    \n# 分割为训练集、验证集和测试集\ntr_ids = sample(1:length(ds), size = floor(0.7 * length(ds)))\nte_ids = setdiff(1:length(ds), tr_ids)\nva_ids = tr_ids[sample(1:length(tr_ids), size = floor(0.3*length(tr_ids)))]\ntr_ids = setdiff(tr_ids, va_ids)\n# 利用dataset_subset()函数从Dataset对象中获取子集\ntr_ds = dataset_subset(ds, tr_ids)\nva_ds = dataset_subset(ds, va_ids)\nte_ds = dataset_subset(ds, te_ids)\n\n\n\n6.2.2.2 创建Dataloader\nDataloader负责按规定策略迭代加载Dataset中的数据项，包括按指定批大小（batch_size）、是否随机打乱数据排序再取batch（shuffle）等。dataloader()函数可以根据Dataset轻松创建Dataloader对象。\n\ntr_dl = tr_ds %&gt;% dataloader(batch_size = 50, shuffle = TRUE)\nva_dl = va_ds %&gt;% dataloader(batch_size = 50, shuffle = FALSE)\nte_dl = te_ds %&gt;% dataloader(batch_size = 50, shuffle = FALSE)\n\n查看tr_dl的长度：\n\ntr_dl$.length()    # 等同于length(tr_dl)\n\n[1] 49\n\n\nDataloader的长度是相应Dataset的长度与batch_size的商，不能整除时，结果加1。\n获取第一批数据并查看输入特征与输出标签的维度：\n\nfirst_batch = tr_dl %&gt;%\n  dataloader_make_iter() %&gt;% \n  dataloader_next()\n\ndim(first_batch[[1]])\n\n[1] 50  4\n\ndim(first_batch[[2]])\n\n[1] 50  1\n\n\n\n\n\n6.2.3 搭建模型\ntorch支持定制开发深度学习模型，但这需要专业的知识和复杂的编程。深度学习应用者可以利用torch提供的构建模型的基本单位——以nn_*开头的各种模块（module），如同搭积木一样来搭建模型。这些模块包括各种神经元层（如nn_linear()、nn_lstm()、nn_conv2d()等）、各种激活函数（如nn_relu()、nn_tanh()等20多种）以及其他操作模块（如nn_layer_norm()、nn_batch_norm2d()、nn_dropout()、nn_max_pool2d()等），都是R6对象。此外，还有相应的一系列非对象的nnf_*函数，主要用于定制开发深度学习模型。\n搭建模型主要有两种方式，使用nn_module()和使用nn_sequential()，前者用于构建复杂的模型，后者用于构建相对简单的模型。nn_module()通常包括initialize和forward两个自定义函数部分，前者用于定义模型中的模块及参数（其中可使用容器函数nn_module_dict()和nn_module_list()以及nn_sequential()）,后者用于定义计算流程。nn_sequential()是按计算流程顺序地列出模块，无需initialize和forward自定义函数。这里只介绍nn_module()搭建模型的方法。\n针对前面构造的人工数据集，建立一个全连接神经网络，其中隐层包括2个线性神经元层nn_linear，激活函数采用nn_relu，输出层是1个没有激活函数的nn_linear。为了预防过拟合，在隐层的2个线性层后分别添加随机失活层nn_dropout。\n\ndim_fc1 = 32    # 指定第一个线性层的神经元数\ndim_fc2 = 32    # 指定第二个线性层的神经元数\nnet = nn_module(\n  initialize = function(dim_in, dim_fc1, dim_fc2) {\n    self$fc1 = nn_linear(dim_in, dim_fc1)    # 输入维度和输出维度\n    self$drop1 = nn_dropout(p = 0.7)    # p为失活概率\n    self$fc2 = nn_linear(dim_fc1, dim_fc2)\n    self$drop2 = nn_dropout(p = 0.7)\n    self$output = nn_linear(dim_fc2, 1)\n  },\n  forward = function(x) {\n    x %&gt;% \n    self$fc1() %&gt;%\n    nnf_relu() %&gt;%    #在forward中，激活函数一般用nnf_*\n    self$drop1() %&gt;%\n    self$fc2() %&gt;%\n    nnf_relu() %&gt;%\n    self$drop2() %&gt;%\n    self$output() \n  }\n)\n\n\n\n6.2.4 模型训练\n深度神经网络模型的训练是一个反复迭代的过程，包括以下四个步骤：\n\n网络正向计算输出；\n根据损失函数计算网络输出的损失；\n根据损失求导计算网络权重的梯度；\n优化器根据梯度和相应策略反向传播来更新网络权重（即模型参数）。\n\n因此，模型训练前必须预先设置超参数：损失函数和优化器。torch预置了满足不同类型任务需要的损失函数，如 表 6.1 所示。\n\n\n\n\n表 6.1: torch预置的损失函数\n\n\n\n\n\n\n\n\n\n\n损失函数\n说明\n\n\n\n\nnn_l1_loss()\n计算平均绝对误差\n\n\nnn_mse_loss()\n计算均方误差\n\n\nnn_smooth_l1_loss()\n计算huber损失\n\n\nnn_soft_margin_loss()\n计算二分类逻辑损失\n\n\nnn_nll_loss()\n计算负对数似然损失\n\n\nnn_poisson_nll_loss()\n计算输出服从泊松分布的负对数似然损失\n\n\nnn_kl_div_loss()\n计算KL散度损失\n\n\nnn_cross_entropy_loss()\n计算交叉熵损失\n\n\nnn_bce_with_logits_loss()\n计算结合sigmoid层的二分类交叉熵损失\n\n\nnn_margin_ranking_loss()\n计算边界排序损失\n\n\nnn_hinge_embedding_loss()\n根据向量距离计算损失\n\n\nnn_multi_margin_loss()\n计算多分类合页损失\n\n\nnn_multilabel_margin_loss()\n计算多类多分类合页损失\n\n\nnn_ctc_loss()\n计算连结主义时间损失\n\n\nnn_bce_loss()\n计算二分类交叉熵损失\n\n\nnn_multilabel_soft_margin_loss()\n基于最大熵计算多标签一对多损失\n\n\nnn_cosine_embedding_loss()\n基于余弦距离计算损失\n\n\nnn_triplet_margin_loss()\n计算三元边界损失\n\n\nnn_triplet_margin_with_distance_loss()\n基于距离计算三元边界损失\n\n\n\n\n\n\n\n\n优化器是用来更新神经网络权重（模型参数）以实现损失最小化的算法，其性能直接影响网络训练的收敛速度，表 6.2 列出了torch预置的优化算法。torch还允许用户自定义损失函数和优化算法。\n\n\n\n\n表 6.2: torch预置的优化算法\n\n\n\n\n\n\n优化算法\n说明\n\n\n\n\noptim_adadelta()\n学习率平滑更新的adagrad算法\n\n\noptim_adagrad()\n学习率适自应梯度算法\n\n\noptim_adam()\n自适应动量估计算法\n\n\noptim_adamw()\n引入权重衰减的adam算法\n\n\noptim_asgd()\n平均梯度随机下降算法\n\n\noptim_lbfgs()\n有限内存BFGS算法\n\n\noptim_rmsprop()\n均方根传播算法\n\n\noptim_rprop()\n弹性反向传播算法\n\n\noptim_sgd()\n随机梯度下降算法\n\n\n\n\n\n\n\n\n在网络训练过程中，为了预防过拟合，也如传统机器学习建模一样，通常会采用提前停止技术（Early stopping）。在luz包中提供了很多形如luz_callback_*()的回调（call back）函数，用于动态地改变训练过程，其中luz_callback_early_stopping()用于在满足条件是提前停止训练以避免过拟合。luz_callback_lr_scheduler()是另一个有用的回调，可以在训练过程中动态地调整学习率。\n使用luz来训练网络，可以让用户避免编写繁琐的基于coro包的循环程序，也无需考虑cpu和cuda计算设备的切换，只需利用setup()函数设置训练所需的超参数，如损失函数和优化器等。如果网络是可配置的，可通过set_hparams()函数设置网络配置超参数，如各神经元层的维度。如果需要设置优化器的超参数，可利用set_opt_hparams()函数来设置。最后，luz通过泛型函数fit()传递训练数据和指定迭代轮数（epochs）即开始网络训练。一个迭代轮次是指训练样本中全部数据按Dataloader规定的策略都输入网络参与一次训练，即完成前述的四个计算步骤。干预训练过程的回调也必须在fit()中传入，同时还可以通过参数verbose设置是否将训练过程中的信息输出到控制台。\n\nlibrary(luz)\n\nfitted &lt;- net %&gt;%\n  setup(    # 设置网络训练超参数\n    optimizer = optim_rmsprop,    # 设置优化器\n    loss = nn_mse_loss(),    # 设置损失函数\n    metrics = luz_metric_mae()    # 指定额外的损失测度\n  ) %&gt;%\n  set_hparams(    # 设置网络配置超参数\n    dim_in = dim_in, \n    dim_fc1 = dim_fc1, \n    dim_fc2 = dim_fc2\n  ) %&gt;%\n  set_opt_hparams(lr = 0.001) %&gt;%    # 设置优化器超参数\n  fit(tr_dl,    # 传递训练数据\n    valid_data = va_dl,    # 传递验证数据\n    epochs = 200,    # 设置迭代轮次\n    callbacks = list(    # 设置回调函数\n      luz_callback_early_stopping(patience = 10)),    # 提前停止回调\n    verbose = F    # 禁止信息输出到控制台\n  )\n\nfit()返回的对象可以用luz_save()保存（保存后的模型文件可以通过luz_load()函数加载），可以用泛型函数print()输出信息，可以用泛型函数plot()可视化。\n保存模型：\n\n# 保存训练好的模型\nluz_save(fitted, \"./model/fitted-model.model\")\n\n打印模型信息：\n\n# 查看模型信息\nprint(fitted)    # 也可以直接输入fitted\n\nA `luz_module_fitted`\n── Time ────────────────────────────────────────────────────────────────────────\n• Total time: 29.6s\n• Avg time per training epoch: 427ms\n\n── Results ─────────────────────────────────────────────────────────────────────\nMetrics observed in the last epoch.\n\nℹ Training:\nloss: 2.3379\nmae: 1.1666\n\n── Model ───────────────────────────────────────────────────────────────────────\nAn `nn_module` containing 1,249 parameters.\n\n── Modules ─────────────────────────────────────────────────────────────────────\n• fc1: &lt;nn_linear&gt; #160 parameters\n• drop1: &lt;nn_dropout&gt; #0 parameters\n• fc2: &lt;nn_linear&gt; #1,056 parameters\n• drop2: &lt;nn_dropout&gt; #0 parameters\n• output: &lt;nn_linear&gt; #33 parameters\n\n\n可视化模型训练过程中拟合误差和验证误差：\n\n# 可视化训练过程\nplot(fitted)\n\n\n\n\n\n\n\n图 6.26: 深度学习模型训练过程中拟合误差和验证误差的可视化\n\n\n\n\n\n本例中提前停止回调设置了patience = 10，意味着验证误差连续10次迭代没有下降，则提前停止训练。\n\n\n6.2.5 模型预测与性能评价\n利用训练好的深度学习模型对新的输入特征进行预测，给出输出标签，即推理（inference）。推理阶段不允许更新网络权重，luz包通过泛型函数predict()函数自动处理这个问题，无需用户处理。\n\npred = fitted %&gt;% predict(te_dl) \n\n需要注意，返回的结果是tensor对象，如果要在R中进行处理，需要考虑将tensor从”cuda”设备上转移到”cpu”（可通过函数cuda_is_available()来判断cuda是否可用），然后用as.numeric()函数进行转换：\n\nprediction = as.numeric(pred$to(device = \"cpu\"))  \n\n利用张量对象$to()方法更改device参数值即可改变张量存储的设备。\n下面利用ggplot2来可视化测试集上的预测标签和实际标签：\n\ndf = tibble(\n  truth = as.numeric(te_ds$dataset$tensors[[2]][te_ids]),\n  prediction = as.numeric(pred$to(device = \"cpu\")),\n  id = 1:length(truth)\n) \n\n测试集te_ds中保存了全部数据，而非只是测试集数据，因此需要用索引te_ids来指定测试集数据。本例中pred保存在”cuda”设备上，因此需要转移到”cpu”设备，然后再转换数据类型。\n\nlibrary(ggplot2)\n# 可视化前100个数据点\nggplot(df[1:100,]) +  \n  geom_line(aes(x = id, y = truth, color = \"blue\")) +\n  geom_line(aes(x = id, y = prediction, color = \"red\")) +\n  labs(x = \"\", y = \"values\") +\n  theme_bw() +\n  scale_color_manual(\n    name = NULL,\n    values = c(\"blue\", \"red\"),\n    labels = c(\"truth\", \"prediction\"))\n\n\n\n\n\n\n\n图 6.27: 测试集预测结果的可视化\n\n\n\n\n\nluz提供的evaluate()函数可以定量计算训练好的模型在测试集上的性能指标：\n\nfitted %&gt;% evaluate(te_dl)\n\nA `luz_module_evaluation`\n── Results ─────────────────────────────────────────────────────────────────────\nloss: 1.3815\nmae: 0.9274\n\n\n其中loss是setup()函数中参数loss指定的损失函数计算的，而mae则是setup()函数中参数metrics指定的性能测度函数luz_metric_mae()计算的。\n模型经过严谨的性能评估并确认符合要求后，即可部署应用。关于模型部署的知识，可参考学习网址徐静, 2018. R语言模型部署实战和周震宇, 2018. R 语言实战之模型部署提供的相关内容。",
    "crumbs": [
      "高阶知识",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>R语言深度学习建模</span>"
    ]
  },
  {
    "objectID": "CH6.html#sec-dl-example",
    "href": "CH6.html#sec-dl-example",
    "title": "6  R语言深度学习建模",
    "section": "6.3 深度学习建模案例",
    "text": "6.3 深度学习建模案例\n\n6.3.1 表格数据案例\n本案例数据集来自UCI机器学习数据库(https://archive.ics.uci.edu/dataset/45/heart+disease)，下载heart+disease.zip压缩包文件，解压后，将其中processed.cleveland.data文件改名为heart.csv，保存于项目文件夹的data目录中。该数据集包含303个样例和14个变量，前13个变量为输入特征，最后1个变量为输出标签，即以0到4的整数来区分心脏病，其中0表示没有心脏病。13个输入特征中有两个存在缺失值，缺失数量较少。该数据集属于分类任务，输入特征分为分类变量和数值变量两类。\n本案例模型先采用嵌入层学习分类变量的表示，然后与数值变量一起输入由三个线性层构成的FCNN，前两个线性层的激活函数采用ReLU，最后一个线性层无激活函数。训练时，前两个线性层引入dropout机制。\n先加载需要的工具包：\n\nlibrary(torch)\nlibrary(luz)\nlibrary(purrr)\nlibrary(readr)\nlibrary(dplyr)\n\n读取数据并添加列名：\n\nheart_df = read_csv(\n  \"./data/heart.csv\",\n  col_names = c(\n    \"age\",  # 年龄\n    \"sex\",  # 性别，1为男性，0为女性\n    # 胸痛类型，1为典型心绞痛，2为不典型心绞痛，3为非绞痛疼痛，4为无症状\n    \"pain_type\", \n    \"resting_blood_pressure\",  # 静息血压，mm Hg\n    \"chol\",  # 血清胆固醇，mg/dl\n    \"fasting_blood_sugar\",   # 空腹血糖，&gt;120mg/dl为1，否则为0\n    \"rest_ecg\",   # 静息心电图，0为正常，1为ST-T波异常，2为可能或明确的左心室肥厚\n    \"max_heart_rate\",   # 运动时最大心率\n    \"ex_induced_angina\",  # 运动诱发心绞痛，1为是，0不是\n    \"old_peak\",   # 运动相对于休息引起的ST压低\n    \"slope\",   # 峰值运动ST段的斜率，1为向上倾斜，2为平坦，3为向下倾斜\n    \"ca\",   # 主要血管数目，0~3\n    # 3 = normal; 6 = fixed defect; 7 = reversible defect\n    \"thal\",   # 地中海贫血，0为正常，6为固定缺陷，7为可逆转缺陷\n    # 1-4 = yes; 0 = no\n    \"heart_disease\"   # 心脏病诊断，0不是，1~4为是\n  ),\n  na = \"?\")\n\nRows: 303 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (14): age, sex, pain_type, resting_blood_pressure, chol, fasting_blood_s...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n查询缺失值：\n\nwhich(is.na(heart_df), arr.ind = TRUE)\n\n     row col\n[1,] 167  12\n[2,] 193  12\n[3,] 288  12\n[4,] 303  12\n[5,]  88  13\n[6,] 267  13\n\n\n结果表明第12（ca）和第13列（thal）分别有4个和2个缺失值。考虑缺失值较少，将其作为一个额外是因子水平。\nNA很少，可转化为一个额外的因子水平。因子变量可以采用独热编码转换为数值变量：\n\nunique(heart_df$slope)\n\n[1] 3 2 1\n\nnnf_one_hot(\n  torch_tensor(\n    heart_df$slope,\n    dtype = torch_long()\n  )\n) %&gt;% print(n = 5)\n\ntorch_tensor\n 0  0  1\n 0  1  0\n 0  1  0\n 0  0  1\n 1  0  0\n... [the output was truncated (use n=-1 to disable)]\n[ CPULongType{303,3} ]\n\n\n创建Dataset：\n\n# 创建Dataset\nheart_dataset &lt;- dataset(\n  \n  initialize = function(df) {\n    self$x_cat &lt;- self$get_categorical(df)  # 获取分类变量\n    self$x_num &lt;- self$get_numerical(df)    # 获取数值变量\n    self$y &lt;- self$get_target(df)           # 获取目标变量\n  },\n  \n  .getitem = function(i) {\n    x_cat &lt;- self$x_cat[i, ]\n    x_num &lt;- self$x_num[i, ]\n    y &lt;- self$y[i]\n    list(x = list(x_cat, x_num), y = y)\n  },\n  \n  .length = function() {\n    dim(self$y)[1]\n  },\n  \n  get_target = function(df) {\n    heart_disease &lt;- ifelse(df$heart_disease &gt; 0, 1, 0)\n    heart_disease\n  },\n  \n  get_numerical = function(df) {\n    df %&gt;%\n      select(-(c(heart_disease, pain_type,\n                 rest_ecg, slope, ca, thal))) %&gt;%\n      mutate(across(everything(), \n                    .fns = scale)) %&gt;%\n      as.matrix()\n  },\n  \n  get_categorical = function(df) {\n    df$ca &lt;- ifelse(is.na(df$ca), 999, df$ca)  # 将NA转换为数值999\n    df$thal &lt;- ifelse(is.na(df$thal), 999, df$thal)\n    df %&gt;%\n      select(pain_type, rest_ecg, slope, ca, thal) %&gt;%\n      mutate(across(everything(), \n                    .fns = compose(as.integer, as.factor))) %&gt;%\n      as.matrix()\n  }\n)\n\n实例化Dataset:\n\nds = heart_dataset(heart_df)\n\n创建DataLoader:\n\ntrain_indices &lt;- sample(\n  1:nrow(heart_df), size = floor(0.8 * nrow(heart_df)))\nvalid_indices &lt;- setdiff(\n  1:nrow(heart_df), train_indices)\n\ntrain_ds &lt;- dataset_subset(ds, train_indices)\ntrain_dl &lt;- train_ds %&gt;% \n  dataloader(batch_size = 256, shuffle = TRUE)\n\nvalid_ds &lt;- dataset_subset(ds, valid_indices)\nvalid_dl &lt;- valid_ds %&gt;% \n  dataloader(batch_size = 256, shuffle = FALSE)\n\n构建分类变量的嵌入层：\n\nebd_mod &lt;- nn_module(\n  initialize = function(cardinalities, embedding_dim) {\n    self$embeddings &lt;- nn_module_list(\n      lapply(                            # 对每个分类变量分别进行嵌入表示\n        cardinalities,                   # 分类变量的因子水平数\n        function(x) {\n          nn_embedding(\n            num_embeddings = x, embedding_dim = embedding_dim\n          )\n        }\n      )\n    )\n  },\n  forward = function(x) {\n    embedded &lt;- vector(\n      mode = \"list\",\n      length = length(self$embeddings)\n    )\n    for (i in 1:length(self$embeddings)) {\n      embedded[[i]] &lt;- self$embeddings[[i]](x[, i])\n    }\n    torch_cat(embedded, dim = 2)         # 按第二个维度（列）拼接嵌入向量\n  }\n)\n\n搭建模型：\n\nmodel &lt;- nn_module(\n  initialize = function(cardinalities,\n                        num_numerical,\n                        embedding_dim,\n                        fc1_dim,\n                        fc2_dim) {\n    self$embedder &lt;- ebd_mod(\n      cardinalities,  # 分类变量的因子水平数\n      embedding_dim   # 嵌入向量的维数\n    )\n    self$fc1 &lt;- nn_linear(\n      embedding_dim * length(cardinalities) + num_numerical,  # 输入特征数\n      fc1_dim         # 输出特征数\n    )\n    self$drop1 &lt;- nn_dropout(p = 0.7)\n    self$fc2 &lt;- nn_linear(fc1_dim, fc2_dim)\n    self$drop2 &lt;- nn_dropout(p = 0.7)\n    self$output &lt;- nn_linear(fc2_dim, 1)\n  },\n  \n  forward = function(x) {\n    embedded &lt;- self$embedder(x[[1]])\n    all &lt;- torch_cat(list(embedded, x[[2]]), dim = 2)\n    score &lt;- all %&gt;%\n      self$fc1() %&gt;%\n      nnf_relu() %&gt;%\n      self$drop1() %&gt;%\n      self$fc2() %&gt;%\n      nnf_relu() %&gt;%\n      self$drop2() %&gt;%\n      self$output() %&gt;% \n      nnf_hardsigmoid()\n    score[, 1]\n  }\n)\n\n计算和设定必需的参数值：\n\n# 获得所有分类变量的因子水平数\ncardinalities &lt;- heart_df %&gt;%\n  select(pain_type, rest_ecg, slope, ca, thal) %&gt;%\n  mutate(across(everything(), .fns = as.factor)) %&gt;%\n  summarise(across(everything(), .fns = nlevels))\n\n# ca和thal中因子水平数因NA而增加1\ncardinalities &lt;- cardinalities + c(0, 0, 0, 1, 1) \n\n# 数值变量的数量\nnum_numerical &lt;- ncol(heart_df) - length(cardinalities) - 1\n\nembedding_dim &lt;- 12\n\nfc1_dim &lt;- 64\nfc2_dim &lt;- 64\n\n训练模型：\n\nfitted &lt;- model %&gt;%\n  setup(\n    optimizer = optim_adam,                              # 设置优化器\n    loss = nn_bce_with_logits_loss(),                    # 设置损失函数\n    metrics = luz_metric_binary_accuracy_with_logits()   # 设置额外测度\n  ) %&gt;%\n  set_hparams(                                    # 设置超参数\n    cardinalities = cardinalities,\n    num_numerical = num_numerical,\n    embedding_dim = embedding_dim,\n    fc1_dim = fc1_dim, fc2_dim\n  ) %&gt;%\n  set_opt_hparams(lr = 0.001) %&gt;%                 # 设置优化器超参数\n  fit(train_dl,\n    epochs = 300,\n    valid_data = valid_dl,\n    callbacks = list(\n      luz_callback_early_stopping(patience = 5)   # 设置提前停止\n    ),\n    verbose = FALSE\n  )\n\n对模型训练过程中的拟合性能和验证性能指标进行可视化，结果见 图 6.28。\n\nlibrary(ggplot2)\nfitted %&gt;% plot() + theme_bw()\n\n\n\n\n\n\n\n图 6.28: 训练过程拟合误差与验证误差的可视化\n\n\n\n\n\n由于没有划分预测集，只能在验证集上评估模型性能：\n\n# 用模型对验证集进行预测\npred = fitted %&gt;% predict(valid_dl) %&gt;% nnf_sigmoid() %&gt;% torch_round()\n# 对实际标签和预测标签建立混淆矩阵并计算相关性能指标\ntruth = as.factor(ds$y[valid_ds$indices])\nprediction = as.integer(pred$to(device = \"cpu\")) %&gt;% as.factor()\nmlr3measures::confusion_matrix(truth = truth, \n                               response = prediction, positive = \"1\")\n\n\n##         truth\n## response  1  0\n##        1 23  8\n##        0  2 28\n## acc :  0.8361; ce  :  0.1639; dor :  40.2500; f1  :  0.8214 \n## fdr :  0.2581; fnr :  0.0800; fomr:  0.0667; fpr :  0.2222 \n## mcc :  0.6864; npv :  0.9333; ppv :  0.7419; tnr :  0.7778 \n## tpr :  0.9200 \n\n结果表明，验证集上25个心脏病病例有2个预测错误，36个非心脏病病例有8个预测错误，预测准确率为85.25%，TPR为92.00%，TNR为77.78%，F1-Score约为0.82。\n\n\n6.3.2 时间序列数据案例\n本案例数据集为tsibble工具包预置的澳大利亚维多利亚半小时电力需求数据（vic_elec），时间跨度为2012年1月1日0:00时至2014年12月31日23:30时，每30分钟一个数据点。变量包括Time、Demand、Temperature、Date和Holiday。为了减少冗余信息，按小时合并Demand数据。\n本案例采用LSTM模型，根据过去168个时点的Demand数据（7天×24小时）预测下一个时点Demand数据。\n加载必需的工具包：\n\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(tsibble) \nlibrary(feasts) \nlibrary(tsibbledata) \nlibrary(torch)\nlibrary(luz)\n\n读取数据并选择部分数据进行可视化：\n\nts = vic_elec\n#选择2014年1月和2月的数据进行可视化：\nts %&gt;% filter(year(Time) == 2014 & (month(Time) %in% c(1:2))) %&gt;% \n  autoplot(.vars = Demand) + theme_bw()\n\n\n\n\n\n\n\n图 6.29: 2014年1-2月的Demand数据\n\n\n\n\n\n创建Dataset：\n\ndemand_ds &lt;- dataset(\n  \n  initialize = function(x, n_timesteps, sample_frac = 1) {\n    self$n_timesteps &lt;- n_timesteps\n    self$x &lt;- torch_tensor((x - train_mean) / train_sd)\n    n &lt;- length(self$x) - self$n_timesteps\n    self$starts &lt;- sort(sample.int(\n      n = n,\n      size = n * sample_frac\n    ))\n  },\n  \n  .getitem = function(i) {\n    start &lt;- self$starts[i]\n    end &lt;- start + self$n_timesteps - 1\n\n    list(\n      x = self$x[start:end],\n      y = self$x[end + 1]\n    )\n  },\n  \n  .length = function() {\n    length(self$starts)\n  }\n)\n\n按小时汇总数据（即将半小时数据汇总为小时数据）:\n\ndemand_hourly &lt;- ts %&gt;%\n  index_by(Hour = floor_date(Time, \"hour\")) %&gt;%\n  summarise(Demand = sum(Demand))\n\n分别建立训练集、验证集和测试集：\n\ndemand_train &lt;- demand_hourly %&gt;% \n  filter(year(Hour) == 2012) %&gt;%\n  as_tibble() %&gt;%\n  select(Demand) %&gt;%\n  as.matrix()\n\ndemand_valid &lt;- demand_hourly %&gt;% \n  filter(year(Hour) == 2013) %&gt;%\n  as_tibble() %&gt;%\n  select(Demand) %&gt;%\n  as.matrix()\n\ndemand_test &lt;- demand_hourly %&gt;% \n  filter(year(Hour) == 2014) %&gt;%\n  as_tibble() %&gt;%\n  select(Demand) %&gt;%\n  as.matrix()\n\n计算训练集均值和标准差，用于输入数据的标准化：\n\ntrain_mean &lt;- mean(demand_train)\ntrain_sd &lt;- sd(demand_train)\n\n指定输入时间步：\n\nn_timesteps &lt;- 7 * 24\n\n将训练集、验证集和预测集数据实例化为Dataset:\n\ntrain_ds &lt;- demand_ds(demand_train, n_timesteps)\nvalid_ds &lt;- demand_ds(demand_valid, n_timesteps)\ntest_ds &lt;- demand_ds(demand_test, n_timesteps)\n\n查看输入与输出的维度：\n\ndim(train_ds[1]$x)\n\n[1] 168   1\n\ndim(train_ds[1]$y)\n\n[1] 1\n\n\n创建DataLoader：\n\nbatch_size &lt;- 128\n\ntrain_dl &lt;- train_ds %&gt;%\n  dataloader(batch_size = batch_size, shuffle = TRUE)\nvalid_dl &lt;- valid_ds %&gt;%\n  dataloader(batch_size = batch_size)\ntest_dl &lt;- test_ds %&gt;%\n  dataloader(batch_size = length(test_ds))\n\n查看DataLoader的维度：\n\nb &lt;- train_dl %&gt;%\n  dataloader_make_iter() %&gt;%\n  dataloader_next()\n\ndim(b$x)\n\n[1] 128 168   1\n\ndim(b$y)\n\n[1] 128   1\n\n\n搭建模型：\n\nmodel &lt;- nn_module(\n  initialize = function(input_size,\n                        hidden_size,\n                        dropout = 0.2,\n                        num_layers = 1,\n                        rec_dropout = 0) {\n    self$num_layers &lt;- num_layers\n\n    self$rnn &lt;- nn_lstm(\n      input_size = input_size,\n      hidden_size = hidden_size,\n      num_layers = num_layers,\n      dropout = rec_dropout,\n      batch_first = TRUE\n    )\n\n    self$dropout &lt;- nn_dropout(dropout)\n    self$output &lt;- nn_linear(hidden_size, 1)\n  },\n  \n  forward = function(x) {\n    (x %&gt;%\n      self$rnn())[[1]][, dim(x)[2], ] %&gt;%\n      self$dropout() %&gt;%\n      self$output()\n  }\n)\n\n设定模型超参数：\n\ninput_size &lt;- 1\nhidden_size &lt;- 32\nnum_layers &lt;- 2\nrec_dropout &lt;- 0.2\n\n设定模型超参数：\n\nmodel &lt;- model %&gt;%\n  setup(optimizer = optim_adam, loss = nn_mse_loss()) %&gt;%\n  set_hparams(\n    input_size = input_size,\n    hidden_size = hidden_size,\n    num_layers = num_layers,\n    rec_dropout = rec_dropout\n  )\n\n学习速率是更新神经网络权重中的一个重要超参数，其大小直接影响权重更新，从而影响训练速度。在训练过程中动态地更新学习速率是一个加速训练的重要的技巧，而这可以通过绘制学习速率-损失曲线来确定学习速率的动态调整范围。下面的代码利用luz包中lr_finder()函数来收集学习速率与损失数据并绘制图形.\n\nrates_and_losses &lt;- model %&gt;% \n  lr_finder(train_dl, start_lr = 1e-3, end_lr = 1)\nrates_and_losses %&gt;% plot() + theme_bw()\n\n\n\n\n\n\n\n图 6.30: 损失随学习速率递增的变化曲线（蓝色曲线为指数平滑线）\n\n\n\n\n\n最佳学习速率不是损失最小时所对应的学习速率值，而应取稍小一些的数值，一般可取小一个数量级的值。由 图 6.30 可见，本案例最佳学习速率取0.1比较合适。将最佳学习速率传给学习速率调度器（亦即学习速率在训练中的调整策略），如单周期调度器lr_one_cycle()，该策略将传入的最佳学习速率作为学习速率最大值，在训练中，学习速率从很小的初始值开始，逐渐增大到最大值，然后学习速率将缓慢地减小，直到减小到略小于初始值。\n配置训练超参数，然后执行训练。\n\nfitted &lt;- model %&gt;%\n  fit(train_dl, epochs = 50, valid_data = valid_dl,\n      callbacks = list(\n        luz_callback_early_stopping(patience = 3),\n        luz_callback_lr_scheduler(\n          lr_one_cycle,\n          max_lr = 0.1,\n          epochs = 50,\n          steps_per_epoch = length(train_dl),\n          call_on = \"on_batch_end\")\n      ),\n      verbose = FALSE)\n\n对训练过程中的拟合损失和验证损失进行可视化，结果如 图 6.31 所示。\n\nplot(fitted) + theme_bw()\n\n\n\n\n\n\n\n图 6.31: 训练和验证损失曲线\n\n\n\n\n\n在测试集上评估模型性能：\n\nevaluate(fitted, test_dl)\n\nA `luz_module_evaluation`\n── Results ─────────────────────────────────────────────────────────────────────\nloss: 0.0378\n\n\n为了更好地可视化预测结果，选择2014年12月的数据进行预测，对预测结果进行可视化：\n\ndemand_viz &lt;- demand_hourly %&gt;%\n  filter(year(Hour) == 2014, month(Hour) == 12)\n\ndemand_viz_matrix &lt;- demand_viz %&gt;%\n  as_tibble() %&gt;%\n  select(Demand) %&gt;%\n  as.matrix()\n\nviz_ds &lt;- demand_ds(demand_viz_matrix, n_timesteps)\nviz_dl &lt;- viz_ds %&gt;% dataloader(batch_size = length(viz_ds))\n\npreds &lt;- predict(fitted, viz_dl)\npreds &lt;- preds$to(device = \"cpu\") %&gt;% as.matrix()\npreds &lt;- c(rep(NA, n_timesteps), preds)\n\npred_ts &lt;- demand_viz %&gt;%\n  add_column(forecast = preds * train_sd + train_mean) %&gt;%\n  pivot_longer(-Hour) %&gt;%\n  update_tsibble(key = name)\n\npred_ts %&gt;%\n  autoplot(.vars = value) +\n  scale_colour_manual(values = c(\"lightblue\", \"red\")) +\n  theme_minimal() +\n  theme(legend.position = \"None\")\n\n\n\n\n\n\n\n\n\n图 6.32: 预测结果可视化\n\n\n\n\n\n从 图 6.32 可见，预测结果与实际值吻合较好。\n下面计算均方根误差、平均绝对误差和平均绝对百分比误差：\n\n# 获取实际标签值\ntruth = demand_viz$Demand[(n_timesteps + 1):nrow(demand_viz)]\n# 标签预测值反归一化\nforecast = preds * train_sd + train_mean\nforecast = forecast[(n_timesteps + 1):length(forecast)]\n\n\n# 计算均方根误差\nyardstick::rmse_vec(truth, forecast)\n\n\n## [1] 274.007\n\n\n# 计算平均绝对误差\nyardstick::mae_vec(truth, forecast)\n\n\n## [1] 202.6\n\n\n# 计算平均绝对百分比误差\nyardstick::mape_vec(truth, forecast)\n\n\n## 2.491471\n\n计算实际残差并可视化其频数分布，结果如 图 6.33 所示：\n\nres = tibble(\n  id = 1:length(truth),\n  val = truth - forecast)\n# 散点图\nggplot(res, aes(x = id, y = val)) +\n  geom_point() + \n  geom_hline(yintercept = 0, color = \"red\") +\n  theme_bw()\n# 频数分布图\nggplot(res, aes(x = val, y = after_stat(density))) +\n  geom_histogram(color = \"gray\", fill = \"lightblue\") +   \n  geom_density(alpha = 0.3, color = \"red\", lwd = 1) +  \n  theme_bw()\n\n\n\n\n\n\n\n\n\n\n\n\n(a) 残差散点图\n\n\n\n\n\n\n\n\n\n\n\n(b) 残差频数分布图\n\n\n\n\n\n\n\n图 6.33: 预测残差的可视化\n\n\n\n\n\n6.3.3 图像数据案例\n本案例数据集自https://storage.googleapis.com/torch-datasets/bird-species.zip下载，包含450种鸟类图片，图片均为244×244×3的彩色jpg格式，每张图片仅有一种鸟。该数据集子集已划分，分别保存在train、valid和test目录中，其中train包含70626张图片（不平衡数据集，但每种鸟至少有130张图片），valid和test均包含2250张图片。在这三个目录中，每种鸟的图片分别保存在以种名（即分类标签）命名的子目录中。需要注意的是，尽管该数据集质量很高，但在每种鸟的雌雄比例上很不平衡，雄性约占80%。因此，基于本数据集建立的分类模型对于雌鸟分类可能表现不佳。\n本案例采用ResNet18预训练模型。ResNet（残差网络）由微软研究院的何恺明等人提出，在2015年的ILSVRC（ImageNet Large Scale Visual Recognition Challenge）中取得了冠军。ResNet的主要贡献是发现了深度神经网络的“退化现象（Degradation）”，并提出了“快捷连接（Shortcut connection）”的解决方法，让神经网络的“深度”首次突破100层，最大的神经网络甚至超过1000层。\nResNet18包含18个权重层，其结构如 图 6.34 所示，其中Basic_block的细节见 图 6.35 。\n\n\n\n\n\n\n\n\n图 6.34: ResNet18结构示意图\n\n\n\n\n\n\n\n\n\n\n\n\n\n图 6.35: Basic_block的结构与残差连接\n\n\n\n\n\n\n6.3.3.1 加载工具包\n\n# 利用CPU训练时的内存管理\noptions(torch.threshold_call_gc = 4000)\n# 加载工具包\nlibrary(torch)\nlibrary(torchvision)\nlibrary(torchdatasets)\nlibrary(dplyr)\nlibrary(ggplot2)\n\n本案例可以利用CPU来训练，但耗时较长。如果有配置正确的支持CUDA的GPU计算环境，建议使用GPU进行训练。\n\n# 设置计算设备\n# device = if (cuda_is_available()) torch_device(\"cuda:0\") else \"cpu\"\ndevice = \"cpu\"\n\n\n\n6.3.3.2 图片预处理\n对加载的图片进行张量化、数据增强和标准化处理：\n\n# 对训练集图片的预处理\ntrain_transforms &lt;- function(img) {\n  img %&gt;%\n    # 图片张量化\n    transform_to_tensor() %&gt;%\n    # 转移至指定设备(GPU或CPU)\n    (function(x) x$to(device = device)) %&gt;%\n    # 数据增强：随机剪裁\n    transform_random_resized_crop(size = c(224, 224)) %&gt;%\n    # 数据增强：色彩（亮度、对比度、饱和度）随机改变\n    transform_color_jitter() %&gt;%\n    # 数据增强：随机水平翻转\n    transform_random_horizontal_flip() %&gt;%\n    # 根据resnet要求执行标准化\n    transform_normalize(mean = c(0.485, 0.456, 0.406), \n                        std = c(0.229, 0.224, 0.225))\n}\n\n# 对验证集图片的预处理\nvalid_transforms &lt;- function(img) {\n  img %&gt;%\n    transform_to_tensor() %&gt;%\n    (function(x) x$to(device = device)) %&gt;%\n    transform_resize(256) %&gt;%\n    transform_center_crop(224) %&gt;%\n    transform_normalize(mean = c(0.485, 0.456, 0.406), \n                        std = c(0.229, 0.224, 0.225))\n}\n\n# 对测试集图片的预处理与验证集相同\ntest_transforms &lt;- valid_transforms\n\n\n\n6.3.3.3 下载数据并创建Dataset\n从https://storage.googleapis.com/torch-datasets/bird-species.zip下载鸟类图片数据集，解压到项目根目录下data子目录中，解压后确保数据集目录名为bird_species，其下有train、valid和test三个目录，分别对应训练集、验证集和测试集鸟类图片。为了缩短训练时间，减轻计算硬件压力，本案例数据仅保留了首字母为A的鸟类图片（注意，需要同时删除train、valid和test三个目录中的首字母非A的鸟类图片）。\n该数据集目录结构完全符合torchvision包中image_folder_dataset()的要求，直接利用该函数创建Dataset：\n\ntrain_ds &lt;- image_folder_dataset(\"./data/bird_species/train/\", \n                                 transform = train_transforms)\nvalid_ds &lt;- image_folder_dataset(\"./data/bird_species/valid/\", \n                                 transform = valid_transforms)\ntest_ds &lt;- image_folder_dataset(\"./data/bird_species/test/\", \n                                transform = test_transforms)\n\n查看三个数据子集中图片数量：\n\ntrain_ds$.length()\n\n\n## 7383\n\n\nvalid_ds$.length()\n\n\n## 235\n\n\ntest_ds$.length()\n\n\n## 235\n\n\n\n6.3.3.4 创建Dataloader\n\nbatch_size &lt;- 64\ntrain_dl &lt;- dataloader(train_ds, batch_size = batch_size, shuffle = TRUE)\nvalid_dl &lt;- dataloader(valid_ds, batch_size = batch_size)\ntest_dl &lt;- dataloader(test_ds, batch_size = batch_size)\n\n\n\n6.3.3.5 数据可视化\n显示部分鸟类图片：\n\nclass_names &lt;- test_ds$classes\nbatch &lt;- train_dl$.iter()$.next()\nclasses &lt;- batch[[2]]\nimages &lt;- as_array(batch[[1]]$to(device = \"cpu\")) %&gt;% \n  aperm(perm = c(1, 3, 4, 2))\nmean &lt;- c(0.485, 0.456, 0.406)\nstd &lt;- c(0.229, 0.224, 0.225)\nimages &lt;- std * images + mean\nimages &lt;- images * 255\nimages[images &gt; 255] &lt;- 255\nimages[images &lt; 0] &lt;- 0\n\npar(mfcol = c(2,4), mar = rep(1, 4), cex = 0.5)\n\nimages %&gt;%\n  purrr::array_tree(1) %&gt;%\n  purrr::set_names(class_names[as_array(classes)]) %&gt;%\n  purrr::map(as.raster, max = 255) %&gt;%\n  purrr::iwalk(~{plot(.x); title(.y)})\n\n\n\n\n\n\n\n\n\n图 6.36: 部分鸟类图片\n\n\n\n\n\n\n\n6.3.3.6 搭建模型\n本案例直接借用预训练的resnet18模型，下载链接为https://storage.googleapis.com/torchvision-models/v1/models/resnet18.pth，下载后保存至项目根目录中model子目录中。\n\n# 引用resnet18模型结构\nmodel &lt;- model_resnet18(pretrained = FALSE)\n# 为模型加载预训练的参数\npretrained_state_dict &lt;- load_state_dict(\"./model/resnet18.pth\")\nmodel$load_state_dict(pretrained_state_dict)\n# 锁定模型参数，确保在后面的训练中不被更新\nmodel$parameters %&gt;% purrr::walk(function(param) param$requires_grad_(FALSE))\n# 修改模型最后的输出层（全连接神经元层）参数，确保输出特征与本案例数据相符\nnum_features &lt;- model$fc$in_features\nmodel$fc &lt;- nn_linear(in_features = num_features, \n                      out_features = length(class_names))\n# 将模型载入计算设备\nmodel &lt;- model$to(device = device)\n\n\n\n6.3.3.7 训练模型\n\n# 设置损失函数\ncriterion &lt;- nn_cross_entropy_loss()\n# 设置优化算法\noptimizer &lt;- optim_sgd(model$parameters, lr = 0.003, momentum = 0.9)\n# 设置训练回合\nnum_epochs = 10\n# 设置优化算法中学习率的动态调整策略\nscheduler &lt;- optimizer %&gt;% \n  lr_one_cycle(max_lr = 0.003, epochs = num_epochs, \n               steps_per_epoch = train_dl$.length())\n# 定义训练计算过程（函数）\ntrain_batch &lt;- function(b) {\n\n  optimizer$zero_grad()\n  output &lt;- model(b[[1]])\n  loss &lt;- criterion(output, b[[2]]$to(device = device))\n  loss$backward()\n  optimizer$step()\n  scheduler$step()\n  loss$item()\n\n}\n# 定义验证计算过程（函数）\nvalid_batch &lt;- function(b) {\n  \n  output &lt;- model(b[[1]])\n  loss &lt;- criterion(output, b[[2]]$to(device = device))\n  loss$item()\n  \n}\n# 训练模型\nfor (epoch in 1:num_epochs) {\n\n  model$train()\n  train_losses &lt;- c()\n  # 在训练集上执行计算\n  coro::loop(for (b in train_dl) {\n    loss &lt;- train_batch(b)\n    train_losses &lt;- c(train_losses, loss)\n  })\n\n  model$eval()\n  valid_losses &lt;- c()\n  # 在验证集上执行计算\n  coro::loop(for (b in valid_dl) {\n    loss &lt;- valid_batch(b)\n    valid_losses &lt;- c(valid_losses, loss)\n  })\n  # 输出该回合的训练集平均损失和验证集平均损失\n  cat(sprintf(\"\\nLoss at epoch %d: training: %3f, \n              validation: %3f\\n\", epoch, mean(train_losses), \n              mean(valid_losses)))\n  \n}\n\n\n## Loss at epoch 1: training: 3.354151, \n##               validation: 0.765348\n## \n## Loss at epoch 2: training: 1.403915, \n##               validation: 0.494243\n## \n## Loss at epoch 3: training: 1.179917, \n##               validation: 0.344438\n## \n## Loss at epoch 4: training: 0.983393, \n##               validation: 0.326341\n## \n## Loss at epoch 5: training: 0.833348, \n##               validation: 0.193428\n## \n## Loss at epoch 6: training: 0.714266, \n##               validation: 0.151117\n## \n## Loss at epoch 7: training: 0.618413, \n##               validation: 0.090056\n## \n## Loss at epoch 8: training: 0.512669, \n##               validation: 0.073891\n## \n## Loss at epoch 9: training: 0.397235, \n##               validation: 0.042033\n## \n## Loss at epoch 10: training: 0.379246, \n##               validation: 0.046510\n\n\n\n6.3.3.8 预测并计算预测性能\n\n# 将模型设置为评估状态，即不更新模型参数\nmodel$eval()\n# 定义模型评估计算过程\ntest_batch &lt;- function(b) {\n\n  output &lt;- model(b[[1]])\n  labels &lt;- b[[2]]$to(device = device)\n  loss &lt;- criterion(output, labels)\n  test_losses &lt;&lt;- c(test_losses, loss$item())\n  predicted &lt;- torch_max(output$data(), dim = 2)[[2]]\n  total &lt;&lt;- total + labels$size(1)\n  correct &lt;&lt;- correct + (predicted == labels)$sum()$item()\n\n}\n# 定义计算变量\ntest_losses &lt;- c()\ntotal &lt;- 0\ncorrect &lt;- 0\n# 在训练集上执行计算\ncoro::loop(for (b in test_dl) {\n  test_batch(b)\n})\n\n\n# 计算预测集上平均损失\nmean(test_losses)\n\n\n## [1] 0.07427256\n\n\n# 计算预测准确率\ntest_accuracy &lt;- correct/total\ntest_accuracy\n\n\n## [1] 0.9829787\n\n由结果可见，10个回合的训练，预测集准确率接近98.30%。\n\n\n6.3.3.9 保存模型\n用troch_save()函数保存已经训练好的模型：\n\ntorch_save(model, \"./model/brid_res18_model.rt\")\n\n加载该模型时，直接使用torch_load()函数：\n\nbird_classif_model = torch_load(\"./model/brid_res18_model.rt\")\n\n\n练习 6.1 \n练习 小节 6.3 中的三个例题，确保能够正确无误的运行，并得到较为理想的结果。\n\n\n练习 6.2 \n安装tabnet工具包：install.packages(\"tabnet\")，或remotes::install_github(\"mlverse/tabnet\")；在https://mlverse.github.io/tabnet/网站学习该工具包的使用；安装modeldata工具包：install.packages(\"modeldata\")。然后基于modeldata中的biomass数据集，利用tabnet建立深度学习模型，建模流程参考tabnet网站的案例。",
    "crumbs": [
      "高阶知识",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>R语言深度学习建模</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "参考文献",
    "section": "",
    "text": "[1] 蔡宝森. 环境统计(第2版)[M]. 武汉工业大学出版社, 2004.\n[2] 陈剑虹 杨保华. 环境统计应用(第2版) [M]. 化学工业出版社, 2010.\n[3] 环境统计教材编写委员会. 环境统计分析与应用[M]. 中国环境出版社, 2016.\n[4] 刘强, 裴艳波, 张贝贝. R语言与现代统计方法[M]. 清华大学出版社,2016.\n[5] 吕书龙, 梁飞豹, 等. 应用统计分析与R语言实战[M]. 清华大学出版社,2016.\n[6] 钱松, 曾思育. 环境与生态统计:R语言的应用[M]. 高等教育出版社, 2011.\n[7] 徐继伟, 杨云. 集成学习方法:研究综述[J]. 云南大学学报：自然科学版, 2018, 40(6):11.\n[8] 薛毅. 统计建模与R软件[M]. 清华大学出版社, 2007.\n[9] 薛震, 孙玉林. R语言统计分析与机器学习[M]. 中国水利水电出版社, 2020.\n[10] 周元哲. Python数据分析与机器学习[M]. 机械工业出版社, 2022.\n[11] 庄树林. 环境数据分析[M]. 科学出版社, 2018.\n[12] Baier D , Decker R , Schmidt-Thieme L .Data Analysis and Decision Support (Studies in Classification, Data Analysis, and Knowledge Organization)[J].Springer-Verlag New York, Inc., 2005.\n[13] Bischl, B., Sonabend, R., Kotthoff, L., & Lang, M. (Eds.). Applied Machine Learning Using mlr3 in R[M]. CRC Press, 2024. https://mlr3book.mlr-org.com.\n[14] Boser B E, Guyon I M, and Vapnik V N. A training algorithm for optimal margin classifiers[C]. In Proceedings of the 5th annual workshop on Computational learning theory, 1992: 144-152. ACM Press.\n[15] Buuren S V , Groothuis-Oudshoorn K .MICE: Multivariate Imputation by Chained Equations in R[J]. Journal of statistical software, 2011, 45(3), 1-67.\n[16] Carslaw D C , Ropkins K. openair — An R package for air quality data analysis[J]. Environmental Modelling & Software, 2012, 27:52-61.\n[17] Cho K, Van Merrienboer B, Gulcehre C, et al. Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation[J]. Computer Science, 2014.\n[18] Cortes C and Vapnik V. Support-vector networks[J]. Machine learning, 1995: 20(3):273-297.\n[19] Fernández-López, Javier, Schliep K .rWind: Download, edit and include wind data in ecological and evolutionary analysis[J].Ecography, 2018, 42: 804–810.\n[20] He K, Zhang X, Ren S, et al. Deep Residual Learning for Image Recognition[J]. 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), IEEE, 2016.\n[21] Hochreiter S, Schmidhuber J. Long Short-Term Memory[J]. Neural Computation, MIT Press, 1997, 9(8): 1735–1780.\n[22] Hopfield J J. Neural networks and physical systems with emergent collective computational abilities[J]. Proceedings of the National Academy of Sciences, 1982.\n[23] Fox J and Weisberg S. An R Companion to Applied Regression (3rd ed.) [M]. Sage, Thousand Oaks CA, 2019.\n[24] Kabacoff R I R语言实战(第3版) 编程语言[M]. 王韬 译. 人民邮电出版社, 2023.\n[25] Kowarik A, Templ M. Imputation with the R Package VIM[J]. Journal of Statistical Software, 2016: 74(7), 1-16.  [26] Pebesma E .Simple features for R: Standardized support for spatial vector data[J].The R Journal, 2018, 10(1):439-446.\n[27] Sarkar D. Lattice: Multivariate Data Visualization with R[M]. Springer, New York, 2008.\n[28]Schuster M & Paliwal K K. Bidirectional recurrent neural networks. IEEE Transactions on Signal Processing, 1997, 45(11), 2673–2681.\n[29]Sigrid Keydana. Deep Learning and Scientific Computing with R torch[M]. Chapman & Hall/CRC, 2023. https://skeydan.github.io/Deep-Learning-and-Scientific-Computing-with-R-torch/.\n[30] Tierney N, Cook D. Expanding Tidy Data Principles to Facilitate Missing Data Exploration, Visualization and Assessment of Imputations[J]. Journal of Statistical Software, Foundation for Open Access Statistic, 2023, 105(7), 1-31.\n[31] Venables W N, Ripley B D. Modern Applied Statistics with S[M]. Fourth Edition. Springer, New York, 2002.\n[32] Vapnik V N, Lerner A Y. Recognition of patterns with help of generalized portraits[J]. Avtomat. i Telemekh, 1963, 24(6): 774-780.\n[33] Vaswani A, Shazeer N, Parmar N, et al. Attention Is All You Need[J]. Attention is all you need. Advances in neural information processing systems, 2017, pp. 5998-6008.\n[34] Weihs C, Ligges U, Luebke K, and Raabe N. klaR Analyzing German Business Cycles[C]. In Baier D, Decker R and Schmidt-Thieme L (eds.). Data Analysis and Decision Support, 2005, 335-343, Springer-Verlag, Berlin.\n[35] Wickham H, Averick M, Bryan J, et al. Welcome to the tidyverse[J]. Journal of Open Source Software, 2019, 4(43), 1686.\n[36] Wickham H. ggplot2: Elegant Graphics for Data Analysis[M]. Springer-Verlag New York, 2016.\n[37] Wickham H, Grolemund, G. R数据科学[M]. 人民邮电出版社, 2018.\n[38] LeCun Y, Bottou L, Bengio Y, et al. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 1998, 86(11), 2278–2324.\n[39] Chang W. R Graphics Cookbook (2nd edition) [OL]. https://r-graphics.org/.\n[40] Deep Learning in R Programming[OL]. https://www.geeksforgeeks.org/machine-learning-with-r/.\n\n以下为本书使用的工具包：\n[41] Bion R (2024). ggradar: Create radar charts using ggplot2. R package version 0.2.\n[42] Falbel D, Luraschi J (2023). torch: Tensors and Neural Networks with ‘GPU’ Acceleration. R package version 0.10.0, https://CRAN.R-project.org/package=torch.\n[43] Filzmoser P, Gschwandtner M (2021). mvoutlier: Multivariate Outlier Detection Based on Robust Methods_. R package version 2.1.1, https://CRAN.R-project.org/package=mvoutlier.\n[44] Gordon M, Gragg S, Konings P (2022). htmlTable: Advanced Tables for Markdown/HTML. R package version 2.4.1, https://CRAN.R-project.org/package=htmlTable.\n[45] Hijmans R (2023). raster: Geographic Data Analysis and Modeling. R package version 3.6-20, https://CRAN.R-project.org/package=raster.\n[46] Kassambara A (2023). ggpubr: ‘ggplot2’ Based Publication Ready Plots. R package version 0.6.0, https://CRAN.R-project.org/package=ggpubr.\n[47] Kassambara A, Mundt F (2020). factoextra: Extract and Visualize the Results of Multivariate Data Analyses. R package version 1.0.7, https://CRAN.R-project.org/package=factoextra.\n[48] Kim S (2015). ppcor: Partial and Semi-Partial (Part) Correlation. R package version 1.1, https://CRAN.R-project.org/package=ppcor.\n[49] Kothari A (2022). ggTimeSeries: Time Series Visualisations Using the Grammar of Graphics. R package version 1.0.2, https://CRAN.R-project.org/package=ggTimeSeries.\n[50] Kuhn M, Wickham H, Hvitfeldt E (2023). recipes: Preprocessing and Feature Engineering Steps for Modeling. R package version 1.0.6, https://CRAN.R-project.org/package=recipes.\n[51] Lang D, Chien G (2018). wordcloud2: Create Word Cloud by ‘htmlwidget’. R package version 0.2.1, https://CRAN.R-project.org/package=wordcloud2.\n[52] Lang M, Schratz P (2023). mlr3verse: Easily Install and Load the ‘mlr3’ Package Family. R package version 0.2.8, https://CRAN.R-project.org/package=mlr3verse.\n[53] Machine Learning with R[OL]. https://www.geeksforgeeks.org/machine-learning-with-r/.\n[54] Maechler, M., Rousseeuw, P., Struyf, A., Hubert, M., Hornik, K.(2022). cluster: Cluster Analysis Basics and Extensions. R package version 2.1.4.\n[55] Morgan-Wall T (2024). rayshader: Create Maps and Visualize Data in 2D and 3D. R package version 0.37.3, https://CRAN.R-project.org/package=rayshader.\n[56] Oscar Perpinan Lamigueiro and Robert Hijmans (2023), rasterVis. R package version 0.51.6.\n[57] Pena EA, Slate EH (2019). gvlma: Global Validation of Linear Models Assumptions. R package version 1.0.0.3, https://CRAN.R-project.org/package=gvlma.\n[58] R Core Team (2023). R: A Language and Environment for Statistical Computing. R Foundation for Statistical Computing, Vienna, Austria. https://www.R-project.org/.\n[59] R interface to Keras[OL]. https://keras.rstudio.com/.\n[60] RStudio Team (2020). RStudio: Integrated Development for R. RStudio, PBC, Boston, MA URL http://www.rstudio.com/.\n[61] Schliep K, Hechenbichler K (2016). kknn: Weighted k-Nearest Neighbors. R package version 1.3.1, https://CRAN.R-project.org/package=kknn.\n[62] Tillé Y, Matei A (2023). sampling: Survey Sampling_. R package version 2.10, &lt;https://CRAN.R-project.org/ package=sampling&gt;.\n[63] torch for R[OL]. https://torch.mlverse.org/.\n[64] University, Evanston, Illinois. R package version 2.3.6, https://CRAN.R-project.org/package=psych.\n[65] Wickham H, Pedersen T, Seidel D (2023). scales: Scale Functions for Visualization. R package version 1.3.0, https://CRAN.R-project.org/package=scales.\n[66] William Revelle (2023). psych: Procedures for Psychological, Psychometric, and Personality Research. Northwestern.\n[67] Waring E, Quinn M, McNamara A, Arino de la Rubia E, Zhu H, Ellis S (2022). skimr: Compact and Flexible Summaries of Data. R package version 2.1.5, https://CRAN.R-project.org/package=skimr.\n[68] Wilkins D (2023). treemapify: Draw Treemaps in ‘ggplot2’. R package version 2.5.6, &lt;https://CRAN.R-project.org/ package=treemapify&gt;.",
    "crumbs": [
      "参考文献"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "环境数据分析与机器学习",
    "section": "",
    "text": "前言\n人类进入21世纪以来，数据越来越多、越来越大，也越来越复杂、越来越重要，并被视为非物质形态的关键生产要素，是深度学习(Deep Learning, DL)为代表的人工智能（Artificial Intelligence， AI）技术快速发展的核心驱动力之一。当前，数字经济正在重塑世界经济结构，而数据与智能的融合技术（数智融合）是推动数字经济发展的核心力量。2023年9月7日，习近平总书记在新时代推动东北全面振兴座谈会上的讲话中提出“新质生产力”这一全新概念，而数智融合是形成和发展新质生产力的重要技术，更是大国科技竞争的战略领域。\n在本书编写过程中的2023年3月，美国OpenAI公司基于深度学习技术开发的多模态大语言模型（LLM）ChatGPT推出了 4.0版本，能够与人类进行对话，并解答常识问题和专业问题，能够进行创意写作，理解人类输入的图片，支持多种语言的翻译，并具有学习能力，逐渐适应用户的个性和习惯，提供更加个性化和定制化的服务。2024年2月，OpenAI公司又推出了根据文本生成视频的大模型Sora，能够准确理解物体在物理世界中的存在，并生成具有丰富情感的角色。此外，美国Google、DeepMind、Meta、Anthropic以及我国的华为、腾讯、百度、阿里巴巴、科大讯飞、智谱AI等公司，均在开发与ChatGPT竞争的多模态大语言模型。\n今天，AI已经在工业、农业、服务业等各个领域得到广泛应用，生产和工作更高效，数据分析更准确，决策支持更智能，服务体验更加个性化，艺术设计更具创造力，甚至辅助科学家进行科学研究。AI正在改变人类的教育方式、生活方式、社交方式、医疗方式和工作方式。同时，AI的广泛应用，也带来了各种问题：隐私安全问题，就业机会变化，伦理道德问题等。但AI的发展和应用无可阻挡，各行各业也需要掌握大数据分析方法和机器学习建模技术的人才。新的时代，有新的机会，更需要储备新知识的人才。\n本书基于R语言介绍环境数据分析与机器学习建模，使环境科学与工程等专业本科生和研究生掌握R语言进行数据导入、整理与可视化并进行统计分析和建模，同时熟悉R语言机器学习建模方法和了解R语言深度学习建模方法，拓展学生知识架构，提高数智素养，培养符合数智化时代要求的新工科、新理科人才。\n本书内容分为六章。第一章介绍数据类型、基本统计术语以及R语言和R Studio的安装，第二章介绍R语言编程基础知识，第三章介绍数据导入、整理和可视化，第四章介绍统计分析和建模，第五章介绍机器学习建模，第六章介绍深度学习建模。本书侧重于以案例和代码来介绍R语言在环境数据分析与建模中的应用，对分析方法和算法不进行深入讨论。因此，读者需要自行阅读相关书籍和文献以了解各种分析方法和算法的原理。\n本书为安徽农业大学规划教材，作为环境科学与工程专业《环境数据分析与机器学习》课程的配套教材，亦可供该专业研究生、研究人员和从事环境保护工作的专业人员参考。\n限于编者的学识水平，且缺乏足够经验，加上时间仓促，书中难免存在错误与不足之处，恳请读者批评指正。\n编者\n2024年4月于合肥",
    "crumbs": [
      "前言"
    ]
  }
]